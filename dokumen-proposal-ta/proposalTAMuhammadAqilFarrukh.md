![Icon Description automatically
generated](media/image1.png){width="0.9847222222222223in"
height="0.9847222222222223in"}

------------------------------------------------------------------------

------------------------------------------------------------------------

*Halaman ini sengaja dikosongkan.*

*Halaman ini sengaja dikosongkan*

# LEMBAR PENGESAHAN {#lembar-pengesahan .Heading-0}

**\<JUDUL PROPOSAL TUGAS AKHIR DITULIS SINGKAT,\
JELAS, DAN MENGGAMBARKAN TEMA POKOK PENELITIAN\>**

**PROPOSAL TUGAS AKHIR**

Diajukan untuk memenuhi salah satu syarat

memperoleh gelar Sarjana Komputer pada

Program Studi S-1 Teknik Informatika

Departemen Teknik Informatika

Fakultas Teknologi Elektro dan Informatika Cerdas

Institut Teknologi Sepuluh Nopember

Oleh: **Muhammad Aqil Farrukh**

NRP. 5025221158

Disetujui oleh Tim Penguji Proposal Tugas Akhir:

  --------------------------------------------------------------------------
  1\.   Shintami Chusnul Hidayati, S.Kom., M.Sc.,    Pembimbing
        Ph.D                                         
  ----- -------------------------------------------- -----------------------
  2\.   Dini Adni Navastara, S.Kom., M.Sc.           Ko-pembimbing

  3\.   \<Nama dan gelar penguji\>                   Penguji

  4\.   \<Nama dan gelar penguji\>                   Penguji
  --------------------------------------------------------------------------

**SURABAYA**

**April, 2025**

*Halaman ini sengaja dikosongkan.*

# APPROVAL SHEET {#approval-sheet .Heading-0}

**\<FINAL PROJECT TITLE SHOULD BE WRITTEN IN BRIEF,\
YET CLEAR AND REPRESENTING THE MAIN THEME OF THE RESEARCH\>**

**FINAL PROJECT PROPOSAL**

Submitted to fulfill one of the requirements

for obtaining a Bachelor of Computer Science degree at

Undergraduate Study Program of Informatics

Department of Informatics

Faculty of Intelligent Electrical and Informatics Technology

Institut Teknologi Sepuluh Nopember

By: **Muhammad Aqil Farrukh**

NRP. 5025221158

Approved by Final Project Proposal Examiner Team:

  --------------------------------------------------------------------------
  1\.   Shintami Chusnul Hidayati, S.Kom., M.Sc.,    Advisor
        Ph.D                                         
  ----- -------------------------------------------- -----------------------
  2\.   Dini Adni Navastara, S.Kom., M.Sc.           Co-advisor

  3\.   \<Name and title\>                           Examiner 1

  4\.   \<Name and title\>                           Examiner 2
  --------------------------------------------------------------------------

**SURABAYA**

**April, 2025**

*Halaman ini sengaja dikosongkan.*

# ABSTRAK {#abstrak .Heading-0}

**PENGEMBANGAN SISTEM REALITAS VIRTUAL\
UNTUK SIMULASI INTERAKSI MANUSIA-MESIN**

+----------------------+------------------+--------------------------------------------------------------------+
| **Nama Mahasiswa /   | **:**            | **Muhammad Aqil Farrukh / 5025221158**                             |
| NRP**                |                  |                                                                    |
+======================+============+=====+=======================+============================================+
| **Departemen**       | **:**            | **Teknik Informatika FTEIC - ITS**                                 |
+----------------------+------------------+--------------------------------------------------------------------+
| **Dosen Pembimbing** | **:**            | **Shintami Chusnul Hidayati, S.Kom., M.Sc., Ph.D**                 |
+----------------------+------------+-----+-----------------------+--------------------------------------------+
| **Dosen Ko-pembimbing**           | **:**                       | **Dini Adni Navastara, S.Kom., M.Sc.**     |
+-----------------------------------+-----------------------------+--------------------------------------------+

**Abstrak**

Realitas Virtual (VR) merupakan teknologi yang memungkinkan pengguna
untuk berinteraksi dengan lingkungan virtual yang menyerupai dunia
nyata. Penelitian ini bertujuan untuk mengembangkan sistem VR yang dapat
digunakan untuk simulasi interaksi manusia-mesin, khususnya dalam
konteks pelatihan dan pendidikan. Sistem VR yang dikembangkan
menggunakan perangkat keras dan perangkat lunak terkini untuk
menciptakan pengalaman yang imersif dan realistis. Dalam penelitian ini,
analisis terhadap berbagai aspek interaksi manusia-mesin akan dilakukan,
termasuk respons pengguna terhadap berbagai skenario simulasi, tingkat
kenyamanan, dan efektivitas pelatihan. Metode pengumpulan data meliputi
survei, wawancara, dan pengamatan langsung.

**Kata kunci: *Virtual Reality*, Interaksi Manusia-Mesin, Simulasi,
Pelatihan, Pendidikan.**

*Halaman ini sengaja dikosongkan.*

# *ABSTRACT* {#abstract .Heading-0}

***DEVELOPMENT OF A VIRTUAL REALITY SYSTEM\
FOR HUMAN-MACHINE INTERACTION SIMULATION***

+----------------------+------------------+--------------------------------------------------------------------+
| ***Full Name /       | **:**            | **Muhammad Aqil Farrukh / 5025221158**                             |
| Student ID***        |                  |                                                                    |
+======================+============+=====+=======================+============================================+
| ***Department***     | **:**            | ***Informatics* ELECTICS - ITS**                                   |
+----------------------+------------------+--------------------------------------------------------------------+
| ***Advisor***        | **:**            | **Shintami Chusnul Hidayati, S.Kom., M.Sc., Ph.D**                 |
+----------------------+------------+-----+-----------------------+--------------------------------------------+
| ***Co-advisor***                  | **:**                       | **Dini Adni Navastara, S.Kom., M.Sc.**     |
+-----------------------------------+-----------------------------+--------------------------------------------+

***Abstract***

*Virtual Reality (VR) technology allows users to interact with a virtual
environment that resembles the real world. This research aims to develop
a VR system to simulate human-machine interactions, particularly in the
context of training and education. The developed VR system utilizes the
latest hardware and software to create an immersive and realistic
experience. This study examines various aspects of human-machine
interaction, including user responses to different simulation scenarios,
comfort levels, and the effectiveness of training. Data collection
methods include surveys, interviews, and direct observation.*

***Keywords: Virtual Reality, Human-Machine Interaction, Simulation,
Training, Education.***

*Halaman ini sengaja dikosongkan.*

# DAFTAR ISI {#daftar-isi .Heading-0}

[LEMBAR PENGESAHAN [i](#lembar-pengesahan)](#lembar-pengesahan)

[APPROVAL SHEET [iii](#approval-sheet)](#approval-sheet)

[ABSTRAK [v](#abstrak)](#abstrak)

[*ABSTRACT* [vii](#abstract)](#abstract)

[DAFTAR ISI [ix](#daftar-isi)](#daftar-isi)

[DAFTAR GAMBAR [xi](#daftar-gambar)](#daftar-gambar)

[DAFTAR TABEL [xiii](#daftar-tabel)](#daftar-tabel)

[DAFTAR KODE SUMBER [xv](#daftar-kode-sumber)](#daftar-kode-sumber)

[BAB 1 PENDAHULUAN [1](#pendahuluan)](#pendahuluan)

[1.1 Latar Belakang [1](#latar-belakang)](#latar-belakang)

[1.2 Rumusan Masalah [2](#rumusan-masalah)](#rumusan-masalah)

[1.3 Batasan Masalah [2](#batasan-masalah)](#batasan-masalah)

[1.4 Tujuan [3](#tujuan)](#tujuan)

[1.5 Manfaat [3](#manfaat)](#manfaat)

[1.5.1 Manfaat Teoritis [3](#manfaat-teoritis)](#manfaat-teoritis)

[1.5.2 Manfaat Praktis [3](#manfaat-praktis)](#manfaat-praktis)

[1.5.3 Manfaat Sosial [3](#manfaat-sosial)](#manfaat-sosial)

[BAB 2 TINJAUAN PUSTAKA [5](#tinjauan-pustaka)](#tinjauan-pustaka)

[2.1 Hasil Penelitian Terdahulu
[5](#hasil-penelitian-terdahulu)](#hasil-penelitian-terdahulu)

[2.2 Dasar Teori [5](#dasar-teori)](#dasar-teori)

[BAB 3 METODOLOGI [7](#metodologi)](#metodologi)

[3.1 Metode yang Digunakan
[7](#metode-yang-digunakan)](#metode-yang-digunakan)

[3.2 Bahan dan Peralatan yang Digunakan
[7](#bahan-dan-peralatan-yang-digunakan)](#bahan-dan-peralatan-yang-digunakan)

[3.3 Urutan Pelaksanaan Penelitian
[7](#urutan-pelaksanaan-penelitian)](#urutan-pelaksanaan-penelitian)

[DAFTAR PUSTAKA [9](#daftar-pustaka)](#daftar-pustaka)

[LAMPIRAN [11](#lampiran)](#lampiran)

*Halaman ini sengaja dikosongkan.*

# DAFTAR GAMBAR {#daftar-gambar .Heading-0}

[Gambar 2.1 Kotak berwarna biru [5](#_Toc202858853)](#_Toc202858853)

*Halaman ini sengaja dikosongkan.*

# DAFTAR TABEL {#daftar-tabel .Heading-0}

[Tabel 2.1 Penelitian Terkait [6](#_Toc217337583)](#_Toc217337583)

[Tabel 3.1 Lini Masa Pengerjaan Tugas Akhir
[11](#_Ref193440555)](#_Ref193440555)

*Halaman ini sengaja dikosongkan.*

# DAFTAR KODE SUMBER {#daftar-kode-sumber .Heading-0}

*Halaman ini sengaja dikosongkan.*

#  PENDAHULUAN

## Latar Belakang

Suara manusia tidak hanya menyampaikan isi pesan, tetapi juga
merefleksikan identitas dari penutur. Identitas ini seperti usia, aksen
budaya, hingga kepribadian dari penutur(Lukac, 2024). Sejak 1930-an,
sejumlah penelitian sudah mencoba mengaitkan ciri-ciri vokal dengan
kepribadian individu. Meskipun demikian, temuan di era awal tersebut
cenderung belum konsisten, sehingga keterkaitan antara vokal dengan
kepribadian manusia masih beragam selama beberapa dekade (Lukac, 2024;
Rubio et al., 2024).

Perkembangan teknologi dalam analisis suara dan meluasnya pendekatan
*big data* menciptakan kesempatan untuk menguji sejauh mana fitur-fitur
suara dapat memprediksi kepribadian manusia (Rubio et al., 2024). Selain
berbasis suara, asesmen kepribadian komputasional juga memanfaatkan
beragam sumber data, mulai dari jejak digital media sosial, komunikasi
personal, teks, pergerakan mata, hingga citra wajah (Lukac, 2024). Dalam
studi Rubio et al. (2024), pendekatan berbasis suara didiskusikan untuk
skenario seleksi personel.

Dalam banyak studi, kepribadian dimodelkan menggunakan kerangka Big Five
dan dianalisis hubungannya dengan ciri vokal. Beberapa penelitian dalam
teknologi analisis suara, sudah menggunakan teori umum yang diterima
secara luas seperti *Big Five Personality* dan telah melaporkan temuan
yang lebih konsisten terkait hubungan ciri vokal dan kepribadian.
Seperti fitur-fitur akustik tertentu yang berkorelasi dengan kepribadian
manusia. Misalnya, fitur prosodik seperti nada (*pitch*) bisa menjadi
parameter tingkat *Extraversion* seseorang. Selain itu, tinjauan lain
juga merangkum bahwa kelancaran berbicara, loudness, dan speech rate
kerap berkorelasi dengan Extraversion, serta ada temuan yang mengaitkan
variasi intensitas maupun persepsi pitch dengan penilaian seperti
self-confidence atau submissiveness/dominance (Rubio et al., 2024).

Penelitian yang dilakukan oleh Lukac (2024) menunjukkan bahwa pendekatan
berbasis representasi dapat memprediksi skor Big Five dengan korelasi
sedang, dengan koefisien korelasi antara skor self-report dan skor
prediksi berada pada kisaran r = 0,26--0,39. Studi tersebut
menggabungkan informasi dari aspek akustik dan linguistik, sehingga
masih perlu ditinjau lebih spesifik bagaimana kontribusi sinyal suara
saja (audio-only) ketika modalitas lain tidak digunakan (Lukac, 2024).
Dari sisi data, Rubio et al. (2024) menyebutkan bahwa First Impressions
V2 Corpus terdiri dari 10.000 segmen video berdurasi 15 detik yang
diekstrak dari YouTube dan merepresentasikan situasi wawancara kerja,
sehingga audionya dapat dimanfaatkan untuk eksperimen audio-only,
dataset ini adalah salah satu contoh yang dipakai di riset personality.

Pada banyak penelitian awal, prediksi Big Five dari suara dilakukan
dengan mengekstrak fitur akustik *handcrafted* (misalnya prosodik,
spektral, MFCC, jitter, shimmer) yang kemudian dipadukan dengan
algoritme prediksi (Barchi et al., 2023; Rubio et al., 2024). Hasil dari
pendekatan berbasis fitur ini umumnya masih terbatas, studi yang memakai
set deskriptor akustik yang luas untuk memprediksi self-report hanya
mampu menjelaskan sekitar 16% variansi (Barchi et al., 2023). Selain
itu, performa bisa tampak lebih tinggi jika evaluasi dan pemisahan data
tidak ketat (misalnya potongan dari video yang sama muncul di data latih
dan uji) (Barchi et al., 2023). Perkembangan terkini kemudian bergeser
ke penggunaan model pra-latih berbasis Transformer yang mampu
mempelajari representasi dari sinyal audio secara lebih kaya tanpa
bergantung pada rekayasa fitur manual (Barchi et al., 2023).

Banyak model pra-latih untuk suara dilatih dari rekaman audio tanpa
label, lalu representasinya dipakai lagi untuk berbagai tugas lain. Pada
wav2vec 2.0, sebagian representasi laten dari encoder dilakukan
*masking*, kemudian model dilatih dengan objective kontrastif untuk
menebak representasi yang benar dari sekumpulan kandidat yang mencakup
distractor, model biasanya dilanjutkan dengan fine-tuning memakai data
berlabel untuk tugas target setelah *pretrained*. (Baevski et al.,
2020). HuBERT memakai ide masked prediction juga, tetapi targetnya
berasal dari langkah *offline* *clustering* sehingga model belajar
memprediksi label target pada bagian yang dimasking (Hsu et al., 2021).
WavLM memperluas kerangka ini dengan menambahkan komponen *denoising*
(misalnya input dibuat noisy/overlapped saat pretraining) dan
memperbesar data pra-latih menjadi sekitar 94k jam agar representasinya
lebih kuat dipakai di beragam tugas, termasuk non-ASR (Chen et al.,
2022).

Dari sisi penerapan, estimasi kepribadian berbasis suara sering dibahas
untuk skenario yang melibatkan interaksi lisan seperti wawancara kerja
dan seleksi personel, serta untuk mendukung perancangan antarmuka suara
pada interaksi manusia--komputer (Rubio et al., 2024). Namun, agar
pendekatan audio-only dapat diandalkan, masih perlu kejelasan pada level
teknis: bagaimana pra-pemrosesan audio disusun supaya input konsisten,
backbone Transformer pra-latih mana yang paling cocok, dan apakah lebih
efektif memakai frozen feature extraction atau melakukan fine-tuning
pada tugas Big Five. Karena itu, penelitian ini menyusun pipeline
audio-only dan membandingkan backbone serta strategi pelatihan
berdasarkan metrik evaluasi yang terukur.

## Rumusan Masalah

Penelitian mengenai estimasi kepribadian berbasis suara berkembang pesat
seiring hadirnya model Transformer pra-latih untuk representasi audio.
Namun, masih terdapat kesenjangan dalam menentukan konfigurasi yang
paling efektif, terutama terkait pemilihan backbone pra-latih dan
strategi pelatihan (frozen feature extraction vs fine-tuning) untuk
prediksi kepribadian Big Five pada skenario audio-only. Berdasarkan
latar belakang tersebut, rumusan masalah penelitian ini adalah sebagai
berikut:

1.  Bagaimana merancang dan membangun pipeline estimasi Big Five
    berbasis audio-only menggunakan Transformer pra-latih (ekstraksi
    representasi, perancangan model prediksi, dan evaluasi)?

2.  Bagaimana merancang dan menerapkan tahapan pra-pemrosesan data
    audio-only agar menghasilkan input yang konsisten dan siap digunakan
    dalam *pipeline* estimasi Big Five (misalnya standarisasi format
    audio, pembersihan data, segmentasi, normalisasi, dan penyusunan
    pembagian data)?

3.  Bagaimana performa model Transformer pra-latih dalam memprediksi Big
    Five pada skenario audio-only berdasarkan metrik evaluasi, serta
    bagaimana perbandingan performa antar backbone Transformer pra-latih
    dan antar strategi pelatihan (frozen feature extraction vs
    fine-tuning) untuk menentukan konfigurasi yang paling unggul dan
    stabil?

## Batasan Masalah

Untuk menjaga fokus dan keterkelolaan penelitian, maka batasan masalah
yang ditetapkan adalah sebagai berikut:

1.  Penelitian hanya membahas prediksi kepribadian *Big Five*
    (*Openness, Conscientiousness, Extraversion, Agreeableness,
    Neuroticism*).

2.  Dataset yang digunakan adalah dataset publik berlabel Big Five pada
    skenario audio-only dengan dokumentasi pra-pemrosesan yang jelas;
    apabila dataset berbahasa Indonesia yang setara tidak tersedia,
    digunakan dataset berbahasa Inggris.

3.  Data yang dianalisis hanya berupa suara/audio. Data visual atau
    multimodal lain tidak termasuk dalam lingkup penelitian ini.

4.  Model yang dievaluasi mencakup beberapa Transformer pra-latih untuk
    representasi audio (misalnya Wav2Vec2, HuBERT, WavLM) beserta
    variasi strategi pelatihan (frozen feature extraction vs
    fine-tuning).

5.  Evaluasi dilakukan menggunakan metrik kuantitatif (MAE dan RMSE
    sebagai error, korelasi Pearson (r) sebagai keselarasan prediksi,
    serta R² sebagai proporsi variansi yang dijelaskan) dengan metode
    cross-validation, tanpa melibatkan uji persepsi manusia.

## Tujuan

Tujuan penelitian ini adalah untuk melakukan analisis komparatif
pendekatan machine learning Transformer pra-latih dalam estimasi
kepribadian berbasis suara. Tujuan spesifik dari penelitian ini adalah:

1.  Membangun pipeline end-to-end estimasi Big Five berbasis audio-only
    menggunakan Transformer pra-latih.

2.  Mengevaluasi performa model Transformer pra-latih pada prediksi Big
    Five menggunakan metrik kuantitatif yang ditetapkan.

3.  Membandingkan backbone dan strategi pelatihan (frozen vs
    fine-tuning) untuk memperoleh konfigurasi terbaik dan paling stabil.

## Manfaat

Penelitian ini diharapkan dapat memberikan manfaat baik secara teoritis,
praktis, maupun sosial.

### Manfaat Teoritis

1.  Menambah wawasan dan referensi dalam bidang computational
    personality assessment berbasis suara (audio-only) menggunakan model
    Transformer pra-latih.

2.  Memberikan kontribusi pada literatur terkait pengaruh pemilihan
    backbone pra-latih serta strategi pelatihan (frozen feature
    extraction vs fine-tuning) terhadap performa prediksi Big Five.

3.  Menyediakan hasil evaluasi yang dapat menjadi acuan untuk memahami
    stabilitas performa model pada tiap dimensi Big Five dalam skenario
    evaluasi speaker-independent.

### Manfaat Praktis

1.  Menyediakan rancangan pipeline end-to-end (pra-pemrosesan hingga
    evaluasi) yang dapat direplikasi untuk membangun sistem estimasi
    kepribadian berbasis audio-only.

2.  Memberikan rekomendasi konfigurasi model (pilihan backbone dan
    strategi pelatihan) yang efektif dan realistis untuk diterapkan pada
    kebutuhan dunia nyata, dengan mempertimbangkan performa dan
    keterbatasan komputasi.

3.  Menjadi dasar pengembangan sistem pendukung keputusan pada aplikasi
    seperti rekrutmen, layanan pelanggan, dan interaksi
    manusia--komputer, dengan pendekatan yang terukur dan dapat
    dipertanggungjawabkan.

### Manfaat Sosial

1.  Mendorong pemanfaatan teknologi AI yang lebih personal dan adaptif
    melalui pemahaman karakteristik pengguna berbasis suara.

2.  Membantu meningkatkan objektivitas dalam proses penilaian tertentu
    (misalnya wawancara) dengan tetap menekankan batasan interpretasi
    dan kebutuhan penggunaan secara bertanggung jawab.

3.  Mendorong inovasi riset interdisipliner di Indonesia dalam bidang
    psikologi komputasional dan kecerdasan buatan.

*Halaman ini sengaja dikosongkan.*

#  TINJAUAN PUSTAKA

## Hasil Penelitian Terdahulu

Penyusunan proposal ini melibatkan penelitian - penelitian terdahulu
yang relevan dikumpulkan sebagai dasar perancangan pipeline dan
pembanding hasil pada tugas estimasi kepribadian Big Five berbasis
audio-only*.* Dataset yang sering muncul adalah ChaLearn First
Impressions V2, sementara metodenya beragam dari fitur handcrafted,
embedding wav2vec 2.0, hingga pendekatan multimodal. Ringkasannya ada di
Tabel 2.1, yang juga menjadi sumber informasi untuk merumuskan fokus
penelitian pada pemilihan backbone pra-latih dan strategi frozen vs
fine-tuning.

Salah satu referensi yang diambil adalah penelitian oleh Rubio et al.
(2024) yang menguji kelayakan asesmen kepribadian berbasis suara untuk
konteks seleksi personel dengan merekam 100 partisipan saat wawancara,
lalu membandingkan prediksi dari fitur suara terhadap skor Big Five dari
NEO-FFI (self-report), penilaian orang dekat, dan rating ahli. Dalam
riset ini bahwa sejumlah fitur suara punya daya prediksi dengan kisaran
korelasi sekitar 0,3--0,4, sehingga pendekatan berbasis suara terlihat
menjanjikan, tetapi belum memberi jawaban teknis tentang konfigurasi
audio-only yang paling efektif, misalnya pilihan backbone
self-supervised modern serta perbandingan frozen feature extraction dan
fine-tuning.

Penelitian selanjutnya adalah penelitian yang dilakukan oleh Barchi et
al. (2023) yang menggunakan First Impressions Dataset dari Challenge
ChaLearn LAP 2016, berisi sekitar 10.000 klip berdurasi 15 detik dalam
konteks wawancara kerja, dengan label Big Five yang dinilai oleh
listener (apparent personality). Pada skenario audio-only, mereka
membandingkan fitur eGeMAPS + speech ratio dengan embedding wav2vec 2.0,
lalu melatih Random Forest Regressor dan DNN. Hasil terbaik diperoleh
DNN dengan R²_avg 0,33 pada official split dan 0,28 pada split yang
lebih ketat berdasarkan video identifier. Studi ini memberikan baseline
audio-only sekaligus menekankan pentingnya pemisahan data yang ketat,
tetapi belum membandingkan backbone self-supervised lain seperti HuBERT
atau WavLM.

Selanjutnya ada penelitian yang telah dilakukan oleh Lukac (2024) yang
memprediksi skor Big Five self-report dari rekaman free-form speech yang
diambil dari 2045 responden dengan menggabungkan dua jenis fitur, yaitu
akustik dan linguistik. Fitur akustik diekstrak menggunakan YAMNet yang
merupakan model pra-latih untuk audio event detection (pra-latih di
AudioSet), lalu fitur linguistik dibentuk dari transkrip Whisper yang
diubah menjadi embedding dengan text-embedding-ada-002. Kedua embedding
ini kemudian digabung (1024-d + 1536-d) dan digunakan untuk melatih
model regresi XGBoost (satu model per trait) dengan 5-fold
cross-validation pada data latih, lalu evaluasi utamanya memakai
korelasi antara prediksi dan ground truth di data uji, dengan rentang
korelasi yang dilaporkan 0,26--0,39. Studi ini masih multimodal
(audio+teks) dan belum membahas perbandingan backbone pra-latih maupun
evaluasi kontribusi audio-only.

Penelitian selanjutnya dilakukan oleh Aslan et al. (2021)yang
memprediksi apparent personality (Big Five) dari klip video dengan
menggabungkan beberapa sumber informasi, yaitu kondisi latar/*scene*,
wajah, suara, dan transkrip. Mereka memproses tiap jenis data dengan
model pra-latih yang sesuai (misalnya ResNet untuk gambar/visual, VGGish
untuk audio, dan ELMo untuk teks), lalu menggabungkan hasilnya dengan
mekanisme perhatian agar bagian yang informatif mendapat bobot lebih
besar. Evaluasi dilakukan pada ChaLearn First Impressions V2 yang berisi
sekitar 10.000 video YouTube berdurasi rata-rata ±15 detik, dan
penelitian ini melaporkan performa mean accuracy sekitar 0,918. Studi
ini masih berbasis multimodal sehingga belum menjawab perbandingan
konfigurasi audio-only.

Selanjutnya ada penelitian lain yang dilakukan oleh Zhao et al. (2023)
yang juga menggunakan dataset ChaLearn First Impression V2 (10.000 klip,
±15 detik, \>3.000 video YouTube; split 6.000/2.000/2.000) yang
digunakan untuk prediksi kepribadian berbasis skor Big Five Personality.
Dalam penelitian ini dilakukan ekstraksi fitur dari audio, scene, dan
wajah, kemudian memodelkan dinamika temporal menggunakan Bi-LSTM serta
Transformer, dan menggabungkan beberapa hasil prediksi dengan weighted
decision-level fusion. Evaluasi menggunakan metrik S (semakin tinggi
semakin baik) dan konfigurasi terbaik yang dilaporkan adalah S rata-rata
0,9167 untuk prediksi kepribadian. Dari sisi gap untuk riset audio-only,
studi ini masih bertumpu pada multimodal video, sehingga belum menjawab
konfigurasi teknis yang spesifik untuk audio-only dan perbandingan
backbone.

Kemudian terdapat penelitian oleh Mawalim et al. (2023) yang melakukan
prediksi kepribadian Big Five pada konteks diskusi kelompok menggunakan
dua dataset, yaitu MATRICS (diskusi kelompok berbahasa Jepang dengan 40
partisipan dalam 10 grup) dan ELEA-AV (27 pertemuan, 102 partisipan,
durasi rekaman sekitar 15 menit dengan topik *winter survival*).
Pendekatan yang digunakan menggabungkan beberapa sumber informasi,
seperti suara, bahasa, dan gerak/visual, serta menambahkan fitur yang
mewakili karakteristik pembicara agar perbedaan antar-individu ikut
tertangkap dalam prediksi. Hasil yang didapatkan yaitu penambahan
ciri-ciri pembicara membantu meningkatkan kinerja model pada beberapa
dimensi, dan informasi dari fitur suara berkontribusi konsisten dalam
prediksi dibanding fitur visual yang lebih membantu pada trait tertentu.
Dari sisi gap, penelitian ini masih berfokus pada diskusi kelompok
dengan dataset yang relatif terbatas, dan penulis juga menyebutkan belum
banyak mengeksplor pendekatan yang lebih mutakhir, sehingga ruang
pengembangan masih terbuka.

Ada penelitian lain yang dijadikan referensi yaitu penelitian yang
dilakukan oleh Giritlioğlu et al. (2021) yang membahas prediksi Big Five
dari video dengan memanfaatkan gabungan isyarat wajah, gerak tubuh,
suara, dan transkrip. Penelitian ini juga memperkenalkan dataset SIAP
(*Self-presentation and Induced Behavior Archive for Personality
Analysis*) yaitu rekaman self-presentation dan kondisi perilaku 60
partisipan terhadap pemicu yang diberikan (tiga sudut kamera) dengan
label Big Five berupa penilaian self-assessed dan observed, lalu
evaluasi dilakukan pada SIAP dan juga ChaLearn LAP First Impressions.
Untuk pemodelan, penelitian ini memakai beberapa arsitektur per
modalitas, seperti 3D-ResNext dan CNN-GRU pada informasi wajah, serta
LSTNet untuk voice, transcribed speech, body pose, dan facial action
units. Hasilnya dilaporkan dalam MAE, misalnya voice (LSTNet) memiliki
AVG MAE 0,108 pada FID dan 0,165 pada SIAP-Interview, dan pada skema
fusion performa terbaik mencapai AVG MAE 0,085 pada FID (LSVR) serta
0,153 pada SIAP-Interview (Feature Attention). Penelitian ini
menganalisis kontribusi tiap modalitas dan penggabungan informasi,
kesimpulan yang didapatkan yaitu informasi terkait wajah cenderung
stabil untuk prediksi kepribadian pada data video. Gap yang ada yaitu
masih menggunakan multimodal video, belum membahas perbandingan dan
konfigurasi untuk audio-only.

Selanjutnya ada penelitian yang dilakukan oleh Phan & Rauthmann (2021)
yang membahas *personality computing*, yaitu upaya mengekstrak informasi
kepribadian (misalnya Big Five) dari data yang ditangkap sensor seperti
teks, jejak digital, penggunaan smartphone, perilaku non-verbal, pola
bicara, sampai data dari aktivitas seperti *gameplay*. Artikel ini
menyoroti cara mengevaluasi kinerja prediksi dan isu psikometrik
(reliability dan validity), sekaligus membahas implikasi etis, legal,
dan sosial dari asesmen berbasis sensor. Salah satu catatan pentingnya
adalah kinerja prediksi berbasis *machine learning* terhadap ukuran
kuesioner diperkirakan punya batas atas sekitar r ≈ 0,30--0,50, sehingga
nilai yang lebih tinggi sebaiknya dibaca hati-hati karena berpotensi
terkait *overfitting*. Dari sisi gap untuk riset *audio-only*, artikel
ini lebih berupa peta konsep dan panduan evaluasi umum, belum masuk ke
pembahasan teknis yang spesifik seperti rancangan pipeline audio-only
atau pembandingan backbone pra-latih dan strategi frozen vs fine-tuning.

Penelitian selanjutnya yang dijadikan referensi adalah penelitian oleh
Ghassemi et al. (2024) yang memprediksi Big Five dengan menggabungkan
multimodal (audio, visual, dan transkrip) pada dataset ChaLearn First
Impressions. Penelitian ini menggunakan *dependency-free* split
berdasarkan Youtube channel ID, karena pada split asli banyak channel
yang muncul di train dan juga muncul di validasi maupun test (sekitar
73,2% di validasi dan 84% di test), sehingga hasil bisa terlihat lebih
tinggi. Metode yang dilakukan di penelitian ini menggabungkan beberapa
jenis fitur dan meringkasnya. Model yang digunakan yaitu BERT (teks),
Wav2Vec (audio embedding transfer), OpenFace + FaceNet-style/Inception
(visual), temporal autoencoder, deep ensemble MLP. Hasil yang didapatkan
yaitu gabungan audio dan visual yang memberikan kinerja lebih baik yaitu
R² avg 0,369 dibandingkan audio saja yang memberikan hasil kinerja
(0,280) atau visual saja (0,314), sementara transkrip sendiri relatif
lemah (0,037). Gap penelitian pada studi ini adalah fokusnya lebih pada
dependency-free split dan pipeline multimodal, sementara eksplorasi
backbone audio pra-latih untuk skenario audio-only masih terbatas.

Penelitian terkait terakhir yang diambil sebagai referensi yaitu
penelitian yang dilakukan oleh Aylett et al. (2020) yang membahas
bagaimana system TTS (text-to-speech) dan gaya kualitas suara bisa
mengubah skor Big Five pada suara sintetis. Penelitian ini membuat
stimulus dari tiga korpus teks yang terdiri dari About Myself, Speed
Dating Positive, dan Speed Dating Negative. Kemudian dilakukan rendering
sehingga menjadi beberapa versi menggunakan dua tipe system sistesis
yaitu *parametric synthesis* dan *unit selection synthesis*
(CereVoice2), termasuk variasi kualitas suara neutral, lax, dan tense
pada suara tertentu. Penilaian dilakukan oleh 35 partisipan yang memberi
skor Big Five dan naturalness untuk setiap output. Hasil yang didapatkan
yaitu jenis system dan sumber suara berpengaruh pada naturalness dengan
contoh selisih yang jelas pada *neutral unit selection* (rata-rata
naturalness, dengan contoh selisih yang jelas). Naturalness hanya
menunjukkan hubungan kecil dengan Openness (sekitar r = 0,249),
sementara trait lain tidak tampak kuat terkait naturalness. Dari sisi
persepsi Big Five, perubahan *rendition* (misalnya Text vs Lax vs Tense)
juga memunculkan perbedaan yang signifikan pada beberapa trait, termasuk
Agreeableness (contohnya pada korpus About Myself). Gap yang ada pada
penelitian ini adalah berfokus pada persepsi kepribadian dari suara
sintetis, bukan membangun pipeline prediksi Big Five dari rekaman nyata,
sehingga belum masuk ke pembahasan backbone audio pretrained (SSL
Transformer).

  ---------------------------------------------------------------------------------------
  **Judul**           **Dataset**           **Metode**                **Analisis Gap**
  ------------------- --------------------- ------------------------- -------------------
  Feasibility of Big  100 partisipan,       Ekstraksi fitur suara     Masih berbasis
  Data Analytics to   direkam saat          dari rekaman lalu dibuat  fitur suara, belum
  Assess Personality  wawancara individual; model prediksi            membahas
  Based on Voice      label kepribadian     kepribadian. Hasil        konfigurasi
  Analysis (Rubio et  dari NEO-FFI          prediksi yang dilaporkan  pretrained speech
  al., 2024)          (self-report), plus   berada di kisaran         transformer untuk
                      penilaian "close      0,3--0,4 dan juga terkait audio-only (mis.
                      other" dan rating     dengan rating close other pilihan backbone,
                      ahli berbasis rekaman & ahli                    frozen vs
                                                                      fine-tuning)

  Apparent            First Impressions     Audio-only pada First     Studi ini baru
  personality         Dataset Challenge     Impressions, bandingkan   memberi baseline
  prediction from     ChaLearn LAP 2016     eGeMAPS + speech ratio vs audio-only dan
  speech using expert (wawancara kerja,     embedding wav2vec 2.0,    menekankan
  features and        10.000 klip 15 detik, lalu latih Random Forest  pentingnya split
  wav2vec 2.0 (Barchi ). Dataset ini sudah  Regressor dan DNN.        ketat (by video
  et al., 2023)       dilabeli Big Five     Terbaik DNN dengan R²_avg ID), tetapi belum
                      yang dinilai oleh     0,33 (official split) dan membandingkan
                      listener (apparent    0,28 (split ketat by      backbone SSL lain.
                      personality)          video ID).                

  Speech-based        Free-form speech +    YAMNet (AudioSet) untuk   Masih audio+teks
  personality         self-report Big Five, embedding akustik +       (bukan audio-only),
  prediction using    N = 2045              Whisper →                 backbone akustik
  deep learning with                        text-embedding-ada-002    yang dipakai bukan
  acoustic and                              untuk embedding           SSL Transformer,
  linguistic                                linguistik, digabung      dan belum menguji
  embeddings (Lukac,                        (1024+1536) lalu regresi  perbandingan
  2024)                                     XGBoost; evaluasi         backbone.
                                            korelasi r = 0,26--0,39   

  Multimodal          ChaLearn First        4 modalitas (scene,       Metode utamanya
  assessment of       Impressions V2:       wajah, suara, transkrip). multimodal, bukan
  apparent            10.000 video YouTube, Backbone pra-latih:       audio-only. Belum
  personality using   rata-rata ±15 detik,  ResNet-v2-101 + LSTM      membahas
  feature attention   label apparent Big    (scene & wajah), VGGish + perbandingan
  and error           Five (AMT).           LSTM (audio log-mel),     backbone SSL audio
  consistency                               ELMo + MLP (teks). Fusion modern.
  constraint (Aslan                         pakai feature attention + 
  et al., 2021)                             error consistency. Hasil: 
                                            mean accuracy ≈ 0,918.    

  Integrating audio   ChaLearn First        Fitur audio+visual (scene Fokus multimodal
  and visual          Impression-V2: 10.000 & wajah), temporal        dan bukan
  modalities for      klip (\~15 s) dari    modeling pakai Bi-LSTM    *audio-only,* dan
  multimodal          \>3.000 video         dan Transformer, lalu     dalam penelitian
  personality trait   YouTube; split        fusion berbobot           ini belum ada
  recognition via     6.000/2.000/2.000;    (decision-level).         perbandingan
  hybrid deep         label Big Five (0--1) Terbaik: S_avg 0,9167     backbone.
  learning (Zhao et   & "Interview".        (late fusion).            
  al., 2023)                                                          

  Personality trait   MATRICS (diskusi      Fitur multimodal (audio,  Fokus pada diskusi
  estimation in group kelompok Jepang; 40   bahasa, motion/visual) +  kelompok, korpus
  discussions using   partisipan, 10 grup), speaker embedding         relatif kecil &
  multimodal analysis ELEA-AV (27 meeting,  i-vector/x-vector;        bahasa terbatas.
  and speaker         102 partisipan; ±15   klasifikasi biner Big     Belum mengeksplor
  embedding (Mawalim  menit; winter         Five, evaluasi            pendekatan yang
  et al., 2023)       survival task)        LOPCV/10-fold + ablation, lebih mutakhir.
                                            contoh peningkatan F1     
                                            neuroticism 61→68%        
                                            (ELEA-AV) dan 68→79%      
                                            (MATRICS, skema           
                                            pembanding)               

  Multimodal analysis Dataset SIAP          Memodelkan tiap modalitas Lebih fokus ke
  of personality      (*Self-presentation   dengan arsitektur khusus  dataset & induced
  traits on videos of and Induced Behavior  (misalnya                 behavior, bukan
  self-presentation   Archive for           3D-ResNext/CNN-GRU untuk  evaluasi backbone
  and induced         Personality           wajah dan LSTNet untuk    SSL audio untuk Big
  behavior            Analysis*) yang       voice, transkrip, pose,   Five audio-only,
  (Giritlioğlu et     melibatkan 60 subjek  serta action units), lalu dan bukan
  al., 2021)          direkam pada beberapa melakukan fusion, hasil   membandingkan
                      sesi, dan ChaLearn    terbaik dilaporkan        frozen vs
                      LAP First Impressions mencapai AVG MAE 0,085    fine-tuning.
                                            pada FID (LSVR) dan 0,153 
                                            pada SIAP-Interview       
                                            (Feature Attention).      

  Personality         Tidak menggunakan     Tinjauan literatur        Tidak memberikan
  computing: New      dataset, karena studi tentang konsep            rancangan pipeline
  frontiers in        ini rangkuman sumber  personality computing,    teknis audio-only
  personality         data                  evaluasi performa, isu    Big Five, juga
  assessment (Phan &  "*sensor-assessed*"   psikometrik dan implikasi tidak membahas
  Rauthmann, 2021)    seperti teks, jejak   etis; menekankan adanya   komparasi backbone
                      digital, penggunaan   perkiraan ceiling         SSL audio dan
                      *smartphone*,         convergent validity       strategi pelatihan.
                      perilaku non-verbal,  sekitar r ≈ 0,30--0,50    
                      pola berbicara, dan   dan risiko overfitting    
                      *game-play.*          jika terlalu tinggi.      

  Unsupervised        ChaLearn First        Multimodal (audio,        Fokus utama pada
  Multimodal Learning Impressions (10.000   visual, transkrip) dengan dependency-free
  for Dependency-Free klip \~15 detik, dari agregasi temporal.        split dan pipeline
  Personality         YouTube); memakai     Model/komponen: BERT      multimodal;
  Recognition         dependency-free split (teks), Wav2Vec (audio    eksplorasi backbone
  (Ghassemi et al.,   berbasis YouTube      embedding transfer),      audio pra-latih
  2024)               channel ID karena     OpenFace +                untuk skenario
                      overlap channel pada  FaceNet-style/Inception   audio-only masih
                      split asli tinggi     (visual), temporal        terbatas (tidak
                      (±73,2% val; ±84%     autoencoder, dan prediksi membandingkan
                      test).                akhir dengan deep         banyak backbone).
                                            ensemble MLP**.** Hasil:  
                                            audio+visual R² avg 0,369 
                                            (audio 0,280, visual      
                                            0,314, transkrip 0,037).  

  Speech synthesis    Tiga korpus teks:     Membandingkan parametric  Fokus pada
  for the generation  About Myself, Speed   synthesis vs unit         membentuk/menilai
  of artificial       Dating Positive,      selection synthesis       persepsi
  personality (Aylett Speed Dating          (CereVoice2), plus        kepribadian pada
  et al., 2020)       Negative; stimulus    variasi voice quality     suara sintetis,
                      berupa ucapan         (neutral/lax/tense).      bukan prediksi Big
                      sintetis yang dinilai Hasil contoh: naturalness Five audio-only
                      oleh 35 partisipan    neutral unit selection    dari data wawancara
                      (skor Big Five +      sekitar 3,1 vs 3,9        nyata; tidak
                      naturalness).         (tergantung voice), dan   membahas
                                            naturalness hanya         perbandingan
                                            berkorelasi kecil dengan  backbone SSL audio.
                                            Openness (r≈0,249).       
  ---------------------------------------------------------------------------------------

  : []{#_Toc217337583 .anchor}Tabel 2.1 Penelitian Terkait

## Dasar Teori

Berdasarkan penelitian sebelumnya (Soldati et al., 2007), eksplorasi di
VR mampu menghasilkan data yang beragam.

*Halaman ini sengaja dikosongkan.*

#  METODOLOGI

## Metode yang Digunakan

## Bahan dan Peralatan yang Digunakan 

## Urutan Pelaksanaan Penelitian

Penelitian Tugas Akhir ini akan dilaksanakan selama enam bulan dari
Maret sampai dengan September 2024. Lini masa pengerjaan Tugas Akhir
bisa dilihat pada [Tabel ‎3.1](#_Ref193440555). Judul tabel perlu ditulis
dalam format *Title Case*, yang berarti setiap kata diawali huruf
kapital, kecuali untuk kata depan seperti 'di', 'ke', 'dari', 'yang',
'untuk', 'kepada', dan sebagainya.

  ---------------------------------------------------------------------------
  No   Aktivitas              MAR   APR   MEI   JUN         JUL   AGU   SEP
  ---- ---------------------- ----- ----- ----- ----- ----- ----- ----- -----
  1    Studi literatur                                                  

  2    Empati konteks                                                   
       permasalahan                                                     

  3    Definisi spesifikasi                                             
       kebutuhan                                                        

  4    Ideasi solusi                                                    

  5    Pembuatan purwarupa                                              
       solusi                                                           

  6    Evaluasi                                                         

  7    Penulisan laporan                                                
       Tugas Akhir                                                      
  ---------------------------------------------------------------------------

  : []{#_Ref193440555 .anchor}Tabel 3.1 Lini Masa Pengerjaan Tugas Akhir

*Halaman ini sengaja dikosongkan.*

# DAFTAR PUSTAKA {#daftar-pustaka .Heading-0}

Aslan, S., Güdükbay, U., & Dibeklioğlu, H. (2021). Multimodal assessment
of apparent personality using feature attention and error consistency
constraint. *Image and Vision Computing*, *110*.
https://doi.org/10.1016/j.imavis.2021.104163

Aylett, M. P., Vinciarelli, A., & Wester, M. (2020). Speech synthesis
for the generation of artificial personality. *IEEE Transactions on
Affective Computing*, *11*(2), 361--372.
https://doi.org/10.1109/TAFFC.2017.2763134

Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). *wav2vec 2.0: A
Framework for Self-Supervised Learning of Speech Representations*.
http://arxiv.org/abs/2006.11477

Barchi, R., Pepino, L., Gauder, L., Estienne, L., Meza, M., Riera, P., &
Ferrer, L. (2023). *Apparent personality prediction from speech using
expert features and wav2vec 2.0*. 21--25.
https://doi.org/10.21437/smm.2023-5

Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda,
N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian,
Y., Wu, J., Zeng, M., Yu, X., & Wei, F. (2022). *WavLM: Large-Scale
Self-Supervised Pre-Training for Full Stack Speech Processing*.
https://doi.org/10.1109/JSTSP.2022.3188113

Ghassemi, S., Zhang, T., Van Breda, W., Koutsoumpis, A., Oostrom, J. K.,
Holtrop, D., & De Vries, R. E. (2024). Unsupervised Multimodal Learning
for Dependency-Free Personality Recognition. *IEEE Transactions on
Affective Computing*, *15*(3), 1053--1066.
https://doi.org/10.1109/TAFFC.2023.3318367

Giritlioğlu, D., Mandira, B., Yilmaz, S. F., Ertenli, C. U., Akgür, B.
F., Kınıklıoğlu, M., Kurt, A. G., Mutlu, E., Gürel, Ş. C., &
Dibeklioğlu, H. (2021). Multimodal analysis of personality traits on
videos of self-presentation and induced behavior. *Journal on Multimodal
User Interfaces*, *15*(4), 337--358.
https://doi.org/10.1007/s12193-020-00347-7

Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R.,
& Mohamed, A. (2021). *HuBERT: Self-Supervised Speech Representation
Learning by Masked Prediction of Hidden Units*.
http://arxiv.org/abs/2106.07447

Lukac, M. (2024). Speech-based personality prediction using deep
learning with acoustic and linguistic embeddings. *Scientific Reports*,
*14*(1). https://doi.org/10.1038/s41598-024-81047-0

Mawalim, C. O., Okada, S., Nakano, Y. I., & Unoki, M. (2023).
Personality trait estimation in group discussions using multimodal
analysis and speaker embedding. *Journal on Multimodal User Interfaces*,
*17*(2), 47--63. https://doi.org/10.1007/s12193-023-00401-0

Phan, L. V., & Rauthmann, J. F. (2021). Personality computing: New
frontiers in personality assessment. *Social and Personality Psychology
Compass*, *15*(7). https://doi.org/10.1111/spc3.12624

Rubio, V. J., Aguado, D., Toledano, D. T., & Fernández-Gallego, M. P.
(2024). Feasibility of Big Data Analytics to Assess Personality Based on
Voice Analysis. *Sensors*, *24*(22). https://doi.org/10.3390/s24227151

Soldati, M., Doulis, M., & Csillaghy, A. (2007). SphereViz - Data
Exploration in a Virtual Reality Environment. *2007 11th International
Conference Information Visualization (IV '07)*, 680--683.
https://doi.org/10.1109/IV.2007.105

Zhao, X., Liao, Y., Tang, Z., Xu, Y., Tao, X., Wang, D., Wang, G., & Lu,
H. (2023). Integrating audio and visual modalities for multimodal
personality trait recognition via hybrid deep learning. *Frontiers in
Neuroscience*, *16*. https://doi.org/10.3389/fnins.2022.1107284

 

*Halaman ini sengaja dikosongkan.*

# LAMPIRAN {#lampiran .Heading-0}
