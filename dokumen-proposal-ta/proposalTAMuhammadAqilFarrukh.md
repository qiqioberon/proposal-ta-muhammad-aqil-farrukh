![Icon Description automatically
generated](media/image1.png){width="0.9847222222222223in"
height="0.9847222222222223in"}

> **PROPOSAL TUGAS AKHIR -- EF234702**
>
> **Analisis Komparatif Pendekatan Machine Learning dan Transformer
> Pra-latih untuk Prediksi Kepribadian dari Data Suara**
>
> **Muhammad Aqil Farrukh**
>
> NRP 5025221158
>
> Dosen Pembimbing
>
> Shintami Chusnul Hidayati, S.Kom., M.Sc., Ph.D
>
> NIP 1987202012004
>
> Dosen Ko-pembimbing
>
> Dini Adni Navastara, S.Kom., M.Sc.
>
> NIP 198510172015042001
>
> **Program Studi S-1 Teknik Informatika**
>
> Departemen Teknik Informatika
>
> Fakultas Teknologi Elektro dan Informatika Cerdas
>
> Institut Teknologi Sepuluh Nopember
>
> Surabaya
>
> 2025

*Halaman ini sengaja dikosongkan.*

*Halaman ini sengaja dikosongkan*

# LEMBAR PENGESAHAN {#lembar-pengesahan .Heading-0}

**\<JUDUL PROPOSAL TUGAS AKHIR DITULIS SINGKAT,\
JELAS, DAN MENGGAMBARKAN TEMA POKOK PENELITIAN\>**

**PROPOSAL TUGAS AKHIR**

Diajukan untuk memenuhi salah satu syarat

memperoleh gelar Sarjana Komputer pada

Program Studi S-1 Teknik Informatika

Departemen Teknik Informatika

Fakultas Teknologi Elektro dan Informatika Cerdas

Institut Teknologi Sepuluh Nopember

Oleh: **Muhammad Aqil Farrukh**

NRP. 5025221158

Disetujui oleh Tim Penguji Proposal Tugas Akhir:

  --------------------------------------------------------------------------
  1\.   Shintami Chusnul Hidayati, S.Kom., M.Sc.,    Pembimbing
        Ph.D                                         
  ----- -------------------------------------------- -----------------------
  2\.   Dini Adni Navastara, S.Kom., M.Sc.           Ko-pembimbing

  3\.   \<Nama dan gelar penguji\>                   Penguji

  4\.   \<Nama dan gelar penguji\>                   Penguji
  --------------------------------------------------------------------------

**SURABAYA**

**April, 2025**

*Halaman ini sengaja dikosongkan.*

# APPROVAL SHEET {#approval-sheet .Heading-0}

**\<FINAL PROJECT TITLE SHOULD BE WRITTEN IN BRIEF,\
YET CLEAR AND REPRESENTING THE MAIN THEME OF THE RESEARCH\>**

**FINAL PROJECT PROPOSAL**

Submitted to fulfill one of the requirements

for obtaining a Bachelor of Computer Science degree at

Undergraduate Study Program of Informatics

Department of Informatics

Faculty of Intelligent Electrical and Informatics Technology

Institut Teknologi Sepuluh Nopember

By: **Muhammad Aqil Farrukh**

NRP. 5025221158

Approved by Final Project Proposal Examiner Team:

  --------------------------------------------------------------------------
  1\.   Shintami Chusnul Hidayati, S.Kom., M.Sc.,    Advisor
        Ph.D                                         
  ----- -------------------------------------------- -----------------------
  2\.   Dini Adni Navastara, S.Kom., M.Sc.           Co-advisor

  3\.   \<Name and title\>                           Examiner 1

  4\.   \<Name and title\>                           Examiner 2
  --------------------------------------------------------------------------

**SURABAYA**

**April, 2025**

*Halaman ini sengaja dikosongkan.*

# ABSTRAK {#abstrak .Heading-0}

**PENGEMBANGAN SISTEM REALITAS VIRTUAL\
UNTUK SIMULASI INTERAKSI MANUSIA-MESIN**

+----------------------+------------------+--------------------------------------------------------------------+
| **Nama Mahasiswa /   | **:**            | **Muhammad Aqil Farrukh / 5025221158**                             |
| NRP**                |                  |                                                                    |
+======================+============+=====+=======================+============================================+
| **Departemen**       | **:**            | **Teknik Informatika FTEIC - ITS**                                 |
+----------------------+------------------+--------------------------------------------------------------------+
| **Dosen Pembimbing** | **:**            | **Shintami Chusnul Hidayati, S.Kom., M.Sc., Ph.D**                 |
+----------------------+------------+-----+-----------------------+--------------------------------------------+
| **Dosen Ko-pembimbing**           | **:**                       | **Dini Adni Navastara, S.Kom., M.Sc.**     |
+-----------------------------------+-----------------------------+--------------------------------------------+

**Abstrak**

Realitas Virtual (VR) merupakan teknologi yang memungkinkan pengguna
untuk berinteraksi dengan lingkungan virtual yang menyerupai dunia
nyata. Penelitian ini bertujuan untuk mengembangkan sistem VR yang dapat
digunakan untuk simulasi interaksi manusia-mesin, khususnya dalam
konteks pelatihan dan pendidikan. Sistem VR yang dikembangkan
menggunakan perangkat keras dan perangkat lunak terkini untuk
menciptakan pengalaman yang imersif dan realistis. Dalam penelitian ini,
analisis terhadap berbagai aspek interaksi manusia-mesin akan dilakukan,
termasuk respons pengguna terhadap berbagai skenario simulasi, tingkat
kenyamanan, dan efektivitas pelatihan. Metode pengumpulan data meliputi
survei, wawancara, dan pengamatan langsung.

**Kata kunci: *Virtual Reality*, Interaksi Manusia-Mesin, Simulasi,
Pelatihan, Pendidikan.**

*Halaman ini sengaja dikosongkan.*

# *ABSTRACT* {#abstract .Heading-0}

***DEVELOPMENT OF A VIRTUAL REALITY SYSTEM\
FOR HUMAN-MACHINE INTERACTION SIMULATION***

+----------------------+------------------+--------------------------------------------------------------------+
| ***Full Name /       | **:**            | **Muhammad Aqil Farrukh / 5025221158**                             |
| Student ID***        |                  |                                                                    |
+======================+============+=====+=======================+============================================+
| ***Department***     | **:**            | ***Informatics* ELECTICS - ITS**                                   |
+----------------------+------------------+--------------------------------------------------------------------+
| ***Advisor***        | **:**            | **Shintami Chusnul Hidayati, S.Kom., M.Sc., Ph.D**                 |
+----------------------+------------+-----+-----------------------+--------------------------------------------+
| ***Co-advisor***                  | **:**                       | **Dini Adni Navastara, S.Kom., M.Sc.**     |
+-----------------------------------+-----------------------------+--------------------------------------------+

***Abstract***

*Virtual Reality (VR) technology allows users to interact with a virtual
environment that resembles the real world. This research aims to develop
a VR system to simulate human-machine interactions, particularly in the
context of training and education. The developed VR system utilizes the
latest hardware and software to create an immersive and realistic
experience. This study examines various aspects of human-machine
interaction, including user responses to different simulation scenarios,
comfort levels, and the effectiveness of training. Data collection
methods include surveys, interviews, and direct observation.*

***Keywords: Virtual Reality, Human-Machine Interaction, Simulation,
Training, Education.***

*Halaman ini sengaja dikosongkan.*

# DAFTAR ISI {#daftar-isi .Heading-0}

[LEMBAR PENGESAHAN [i](#lembar-pengesahan)](#lembar-pengesahan)

[APPROVAL SHEET [iii](#approval-sheet)](#approval-sheet)

[ABSTRAK [v](#abstrak)](#abstrak)

[*ABSTRACT* [vii](#abstract)](#abstract)

[DAFTAR ISI [ix](#daftar-isi)](#daftar-isi)

[DAFTAR GAMBAR [xi](#daftar-gambar)](#daftar-gambar)

[DAFTAR TABEL [xiii](#daftar-tabel)](#daftar-tabel)

[DAFTAR KODE SUMBER [xv](#daftar-kode-sumber)](#daftar-kode-sumber)

[BAB 1 PENDAHULUAN [1](#pendahuluan)](#pendahuluan)

[1.1 Latar Belakang [1](#latar-belakang)](#latar-belakang)

[1.2 Rumusan Masalah [2](#rumusan-masalah)](#rumusan-masalah)

[1.3 Batasan Masalah [2](#batasan-masalah)](#batasan-masalah)

[1.4 Tujuan [3](#tujuan)](#tujuan)

[1.5 Manfaat [3](#manfaat)](#manfaat)

[1.5.1 Manfaat Teoritis [3](#manfaat-teoritis)](#manfaat-teoritis)

[1.5.2 Manfaat Praktis [3](#manfaat-praktis)](#manfaat-praktis)

[1.5.3 Manfaat Sosial [3](#manfaat-sosial)](#manfaat-sosial)

[BAB 2 TINJAUAN PUSTAKA [5](#tinjauan-pustaka)](#tinjauan-pustaka)

[2.1 Hasil Penelitian Terdahulu
[5](#hasil-penelitian-terdahulu)](#hasil-penelitian-terdahulu)

[2.2 Dasar Teori [5](#dasar-teori)](#dasar-teori)

[BAB 3 METODOLOGI [7](#metodologi)](#metodologi)

[3.1 Metode yang Digunakan
[7](#metode-yang-digunakan)](#metode-yang-digunakan)

[3.2 Bahan dan Peralatan yang Digunakan
[7](#bahan-dan-peralatan-yang-digunakan)](#bahan-dan-peralatan-yang-digunakan)

[3.3 Urutan Pelaksanaan Penelitian
[7](#urutan-pelaksanaan-penelitian)](#urutan-pelaksanaan-penelitian)

[DAFTAR PUSTAKA [9](#daftar-pustaka)](#daftar-pustaka)

[LAMPIRAN [11](#lampiran)](#lampiran)

*Halaman ini sengaja dikosongkan.*

# DAFTAR GAMBAR {#daftar-gambar .Heading-0}

[Gambar 2.1 Kotak berwarna biru [5](#_Toc202858853)](#_Toc202858853)

*Halaman ini sengaja dikosongkan.*

# DAFTAR TABEL {#daftar-tabel .Heading-0}

[Tabel 2.1 Penelitian Terkait [9](#_Ref217659410)](#_Ref217659410)

[Tabel 3.1 Lini Masa Pengerjaan Tugas Akhir
[17](#_Ref193440555)](#_Ref193440555)

*Halaman ini sengaja dikosongkan.*

# DAFTAR KODE SUMBER {#daftar-kode-sumber .Heading-0}

*Halaman ini sengaja dikosongkan.*

#  PENDAHULUAN

## Latar Belakang

Suara manusia tidak hanya menyampaikan isi pesan, tetapi juga
merefleksikan identitas dari penutur. Identitas ini seperti usia, aksen
budaya, hingga kepribadian dari penutur (Lukac, 2024). Sejak 1930-an,
sejumlah penelitian sudah mencoba mengaitkan ciri-ciri vokal dengan
kepribadian individu. Meskipun demikian, temuan di era awal tersebut
cenderung belum konsisten, sehingga keterkaitan antara vokal dengan
kepribadian manusia masih beragam selama beberapa dekade (Lukac, 2024;
Rubio dkk., 2024).

Perkembangan teknologi dalam analisis suara dan meluasnya pendekatan
*big data* menciptakan kesempatan untuk menguji sejauh mana fitur-fitur
suara dapat memprediksi kepribadian manusia (Rubio dkk., 2024). Selain
berbasis suara, asesmen kepribadian komputasional juga memanfaatkan
beragam sumber data, mulai dari jejak digital media sosial, komunikasi
personal, teks, pergerakan mata, hingga citra wajah (Lukac, 2024). Dalam
studi Rubio dkk. (2024), pendekatan berbasis suara didiskusikan untuk
skenario seleksi personel.

Dalam banyak studi, kepribadian dimodelkan menggunakan kerangka Big Five
dan dianalisis hubungannya dengan ciri vokal. Beberapa penelitian dalam
teknologi analisis suara, sudah menggunakan teori umum yang diterima
secara luas seperti *Big Five Personality* dan telah melaporkan temuan
yang lebih konsisten terkait hubungan ciri vokal dan kepribadian.
Seperti fitur-fitur akustik tertentu yang berkorelasi dengan kepribadian
manusia. Misalnya, fitur prosodik seperti nada (*pitch*) bisa menjadi
parameter tingkat *Extraversion* seseorang. Selain itu, tinjauan lain
juga merangkum bahwa kelancaran berbicara, loudness, dan speech rate
kerap berkorelasi dengan *Extraversion*, serta ada temuan yang
mengaitkan variasi intensitas maupun persepsi pitch dengan penilaian
seperti *self-confidence* atau *submissiveness/dominance* (Rubio dkk.,
2024).

Penelitian yang dilakukan oleh Lukac (2024) menunjukkan bahwa pendekatan
berbasis representasi dapat memprediksi skor Big Five dengan korelasi
sedang, dengan koefisien korelasi antara skor self-report dan skor
prediksi berada pada kisaran r = 0,26--0,39. Studi tersebut
menggabungkan informasi dari aspek akustik dan linguistik, sehingga
masih perlu ditinjau lebih spesifik bagaimana kontribusi sinyal suara
saja (*audio-only*) ketika modalitas lain tidak digunakan (Lukac, 2024).
Dari sisi data, Rubio dkk. (2024) menyebutkan bahwa First Impressions V2
Corpus [(sitasi paper penerbit dataset)]{.mark} adalah salah satu contoh
yang dipakai di riset personality yang diekstrak dari YouTube, sehingga
audionya dapat dimanfaatkan untuk eksperimen *audio-only*.

Pada banyak penelitian awal, prediksi Big Five dari suara dilakukan
dengan mengekstrak fitur akustik *handcrafted* (misalnya prosodik,
spektral, MFCC, jitter, shimmer) yang kemudian dipadukan dengan
algoritme prediksi (Barchi dkk., 2023; Rubio dkk., 2024). Hasil dari
pendekatan berbasis fitur ini umumnya masih terbatas, studi yang memakai
set deskriptor akustik yang luas untuk memprediksi *self-report* hanya
mampu menjelaskan sekitar 16% variansi (Barchi dkk., 2023). Selain itu,
kinerja bisa tampak lebih tinggi jika evaluasi dan pemisahan data tidak
ketat (misalnya potongan dari video yang sama muncul di data latih dan
uji) (Barchi dkk., 2023). Perkembangan terkini kemudian bergeser ke
penggunaan model pra-latih berbasis Transformer yang mampu mempelajari
representasi dari sinyal audio secara lebih kaya tanpa bergantung pada
rekayasa fitur manual (Barchi dkk., 2023).

Baevski dkk. (2020) memperkenalkan wav2vec 2.0, yang merupakan model
*self-supervised* untuk *speech* yang menggunakan Transformer sebagai
*context network*, pada model ini sebagian representasi laten dari
encoder di-*masking,* lalu model dilatih dengan *objective* kontrastif
untuk memilih representasi yang benar dari sekumpulan kandidat yang
mencakup *distractor*. HuBERT memakai ide masked prediction juga, tetapi
targetnya berasal dari langkah *offline* *clustering* sehingga model
belajar memprediksi label target pada bagian yang dimasking (Hsu dkk.,
2021). WavLM memperluas kerangka ini dengan menambahkan komponen
*denoising* (misalnya input dibuat *noisy/overlapped* saat pretraining)
dan memperbesar data pra-latih menjadi sekitar 94k jam agar
representasinya lebih kuat dipakai di beragam tugas, termasuk non-ASR
(Chen dkk., 2022).

Dari sisi penerapan, estimasi kepribadian berbasis suara sering dibahas
untuk skenario yang melibatkan interaksi lisan seperti wawancara kerja
dan seleksi personel, serta untuk mendukung perancangan antarmuka suara
pada interaksi manusia dan komputer (Rubio dkk., 2024). Namun, agar
pendekatan audio-only dapat diandalkan, masih perlu kejelasan pada level
teknis: bagaimana pra-pemrosesan audio disusun supaya input konsisten,
backbone Transformer pra-latih mana yang paling cocok, dan apakah lebih
efektif memakai frozen feature extraction atau melakukan fine-tuning
pada tugas Big Five. Karena itu, penelitian ini menyusun pipeline
audio-only dan membandingkan backbone serta strategi pelatihan
berdasarkan metrik evaluasi yang terukur.

## Rumusan Masalah

Penelitian mengenai estimasi kepribadian berbasis suara berkembang pesat
seiring hadirnya model Transformer pra-latih untuk representasi audio.
Namun, masih terdapat kesenjangan dalam menentukan konfigurasi yang
paling efektif, terutama terkait pemilihan backbone pra-latih dan
strategi pelatihan (frozen feature extraction vs fine-tuning) untuk
prediksi kepribadian Big Five pada skenario *audio-only*. Berdasarkan
latar belakang tersebut, rumusan masalah penelitian ini adalah sebagai
berikut:

1.  Bagaimana merancang dan menerapkan tahapan pra-pemrosesan data
    *audio-only* agar menghasilkan input yang konsisten dan siap
    digunakan dalam *pipeline* estimasi Big Five?

2.  Bagaimana performa model Transformer pra-latih dalam memprediksi Big
    Five pada skenario audio-only berdasarkan metrik evaluasi, serta
    bagaimana perbandingan performa antar backbone Transformer pra-latih
    dan antar strategi pelatihan (frozen feature extraction vs
    fine-tuning) untuk menentukan konfigurasi yang paling unggul dan
    stabil?

3.  [Bagaimana]{.mark}

## Batasan Masalah

Untuk menjaga fokus dan keterkelolaan penelitian, maka batasan masalah
yang ditetapkan adalah sebagai berikut:

1.  Penelitian hanya membahas prediksi kepribadian *Big Five*
    (*Openness, Conscientiousness, Extraversion, Agreeableness,
    Neuroticism*).

2.  Dataset yang digunakan adalah dataset publik berlabel Big Five pada
    skenario audio-only dengan dokumentasi pra-pemrosesan yang jelas,
    apabila dataset berbahasa Indonesia yang setara tidak tersedia,
    digunakan dataset berbahasa Inggris.

3.  Data yang dianalisis hanya berupa suara/audio. Data visual atau
    multimodal lain tidak termasuk dalam lingkup penelitian ini.

## Tujuan

Tujuan penelitian ini adalah untuk melakukan analisis komparatif
pendekatan machine learning Transformer pra-latih dalam estimasi
kepribadian berbasis suara. Tujuan spesifik dari penelitian ini adalah:

1.  Aduh nanti aja dulu, nunggu rumusan masalah

## Manfaat

Penelitian ini diharapkan dapat memberikan manfaat baik secara teoritis,
praktis, maupun sosial.

### Manfaat Teoritis

1.  Menambah wawasan dan referensi dalam bidang computational
    personality assessment berbasis suara (audio-only) menggunakan model
    Transformer pra-latih.

2.  Memberikan kontribusi pada literatur terkait pengaruh pemilihan
    backbone pra-latih serta strategi pelatihan (frozen feature
    extraction vs fine-tuning) terhadap performa prediksi Big Five.

3.  Menyediakan hasil evaluasi yang dapat menjadi acuan untuk memahami
    stabilitas performa model pada tiap dimensi Big Five dalam skenario
    evaluasi speaker-independent.

### Manfaat Praktis

1.  Menyediakan rancangan pipeline end-to-end (pra-pemrosesan hingga
    evaluasi) yang dapat direplikasi untuk membangun sistem estimasi
    kepribadian berbasis audio-only.

2.  Memberikan rekomendasi konfigurasi model (pilihan backbone dan
    strategi pelatihan) yang efektif dan realistis untuk diterapkan pada
    kebutuhan dunia nyata, dengan mempertimbangkan performa dan
    keterbatasan komputasi.

3.  Menjadi dasar pengembangan sistem pendukung keputusan pada aplikasi
    seperti rekrutmen, layanan pelanggan, dan interaksi
    manusia--komputer, dengan pendekatan yang terukur dan dapat
    dipertanggungjawabkan.

### Manfaat Sosial

1.  Mendorong pemanfaatan teknologi AI yang lebih personal dan adaptif
    melalui pemahaman karakteristik pengguna berbasis suara.

2.  Membantu meningkatkan objektivitas dalam proses penilaian tertentu
    (misalnya wawancara) dengan tetap menekankan batasan interpretasi
    dan kebutuhan penggunaan secara bertanggung jawab.

3.  Mendorong inovasi riset interdisipliner di Indonesia dalam bidang
    psikologi komputasional dan kecerdasan buatan.

*Halaman ini sengaja dikosongkan.*

#  TINJAUAN PUSTAKA

## Hasil Penelitian Terdahulu

Penyusunan proposal ini melibatkan penelitian - penelitian terdahulu
yang relevan dikumpulkan sebagai dasar perancangan pipeline dan
pembanding hasil pada tugas estimasi kepribadian Big Five berbasis
*audio-only.* Studi-studi tersebut digunakan untuk memahami
karakteristik tugas dan dataset, memilih pendekatan ekstraksi fitur dan
pemodelan yang sesuai, serta mengidentifikasi praktik evaluasi yang
valid agar hasil yang diperoleh tidak bias. Selain itu, penelitian
terdahulu juga dimanfaatkan sebagai pembanding untuk menempatkan capaian
penelitian ini dalam konteks literatur yang ada, baik dari sisi metrik
kinerja maupun asumsi dan keterbatasan yang dilaporkan. Ringkasan
penelitian terdahulu yang menjadi rujukan utama dalam proposal ini
disajikan pada Tabel 2.1.

Salah satu referensi yang diambil adalah penelitian oleh Rubio dkk.
(2024) yang melakukan estimasi kepribadian dengan merekam 100 partisipan
saat wawancara, lalu membandingkan prediksi berbasis suara terhadap skor
Big Five dari NEO-FFI (*NEO Five-Factor Inventory*) yang mana merupakan
sebuah kuesioner yang bersifat *self-report*, penilaian orang dekat, dan
rating ahli. Pada sisi pemrosesan, Rubio menggunakan pendekatan
*audio-only* yang masih klasik, yaitu mengambil segmen audio dari
wawancara lalu mengekstrak fitur menggunakan OpenSMILE dengan set
AVEC2011 sebanyak sekitar 1941 fitur (misalnya MFCC, jitter, shimmer,
dan fitur akustik lainnya). Untuk tahap prediksi, skor tiap trait Big
Five yang awalnya kontinu diubah menjadi tiga kelas (low/medium/high)
menggunakan batas persentil P25 dan P75 pada data training, sehingga 25%
terbawah menjadi low, 50% di tengah menjadi medium, dan 25% teratas
menjadi high. Selanjutnya dalam studi dilakukan Random Forest untuk
mengklasifikasikan level tersebut, dengan input top-5 fitur yang
korelasinya paling tinggi terhadap trait yang diprediksi. Hasilnya
menunjukkan adanya sinyal prediktif pada fitur suara dengan korelasi
yang cenderung moderat (skor pearson r sekitar 0,3--0,4), sehingga suara
dapat memberi "petunjuk" terhadap trait meskipun belum deterministik,
dan kemudian dilakukan prediksi model klasifikasi (low/medium/high) dan
didapatkan kinerja yang memiliki akurasi keseluruhan 43% hingga 60%
tergantung trait dan sumber label. Temuan ini mendukung bahwa pendekatan
audio-only memang menjanjikan, tetapi penelitian ini belum menjawab
konfigurasi teknis audio-only yang lebih modern, misalnya pemilihan
backbone self-supervised serta perbandingan skenario frozen feature
extraction dan fine-tuning.

Selain Rubio dkk. (2024) yang masih memakai pipeline audio-only klasik
berbasis fitur akustik, penelitian selanjutnya yang dilakukan oleh
Barchi dkk. (2023) bergerak ke pendekatan self-supervised speech
representation untuk tugas apparent personality prediction. Studi ini
menggunakan First Impressions Dataset dari Challenge ChaLearn LAP 2016,
berisi sekitar 10.000 klip berdurasi 15 detik dari YouTube vlog dengan
konteks "first impression / interview", dan label Big Five yang bukan
self-report, melainkan rating dari listener (Amazon Mechanical Turk)
sehingga termasuk apparent personality. Sebelum dilakukan pelatihan
model, dilakukan pengecekan kondisi audio dan menemukan banyak sampel
mengandung background music yang dapat mengganggu prediksi berbasis
speech. Karena itu eksperimen utama difokuskan pada subset no-music agar
sinyal ucapan lebih bersih. Pada skenario audio-only, mereka
membandingkan dua jalur fitur: jalur "klasik" berupa eGeMAPS dari
openSMILE ditambah speech ratio (rasio durasi bicara) yang dihitung
menggunakan Silero VAD, serta jalur modern berupa embedding wav2vec 2.0
base (mengambil representasi dari seluruh layer, bukan hanya layer
terakhir). Untuk pemodelan, mereka melatih Random Forest Regressor
dengan embedding yang diringkas lalu direduksi dimensinya menggunakan
PCA, serta DNN yang memanfaatkan embedding wav2vec 2.0 sebagai urutan
(sequence) sebelum dipooling untuk memprediksi lima skor Big Five. Hasil
terbaik diperoleh DNN dengan R²_avg 0,33 pada official split, namun
turun menjadi 0,28 pada split yang lebih ketat berbasis video
identifier, karena official split dinilai dapat terlalu optimistis
ketika potongan dari video yang sama di train dan test, penelitian ini
juga menjaga komposisi fold tetap seimbang melalui stratifikasi
(misalnya gender, ethnicity, dan rata-rata rating). Studi ini melengkapi
Rubio dengan menunjukkan bahwa representasi pretrained seperti wav2vec
2.0 memberikan baseline *audio-only* yang lebih kuat, sekaligus
menegaskan pentingnya pembagian data yang ketat dan kualitas kondisi
audio. Namun penelitian ini masih terbatas pada wav2vec 2.0 dan belum
membandingkan backbone self-supervised lain seperti HuBERT atau WavLM,
maupun mengeksplor perbandingan frozen feature extraction dan
fine-tuning.

Penelitian selanjuutnya yang dilakukan oleh Lukac (2024) ini masuk ke
arah speech-based personality prediction yang lebih "dua sisi". Mereka
mengumpulkan 2045 responden yang mengisi kuesioner Big Five self-report
(IPIP 50 item dengan 10 item per trait), lalu masing-masing diminta
merekam free-form speech (intro diri, bebas tanpa batasan konten). Dari
rekaman ini, studi ini melakukan ekstraksi dua embedding: (1) fitur
akustik memakai YAMNet (pretrained AudioSet untuk audio event
detection), lalu outputnya diringkas dengan mean pooling menjadi vektor
1024 dimensi per rekaman, dan (2) fitur linguistik dengan men-transkrip
audio menggunakan Whisper, lalu teksnya diubah menjadi embedding dengan
text-embedding-ada-002 menjadi vektor 1536 dimensi. Kedua embedding
tersebut kemudian digabung (concat) menjadi 2560 dimensi, dan dipakai
untuk melatih model regresi XGBoost (dibuat satu model per trait) dengan
skema train 80% dan test 20% tanpa validation, hyperparameter dituning
menggunakan 5-fold cross-validation di data latih. Evaluasi utamanya
bukan akurasi kelas seperti penelitian sebelumnya, melainkan korelasi
antara skor prediksi dan skor self-report pada data uji, dengan rentang
yang dilaporkan sekitar 0,26--0,39 (misalnya neuroticism dan
agreeableness cenderung lebih tinggi dibanding extraversion). Studi ini
menarik karena menunjukkan performa korelasi yang "moderat" mirip pola
temuan sebelumnya (bahwa sinyal dari suara memang ada), tetapi
pendekatannya tidak *audio-only* murni karena memasukkan jalur teks
dengan Whisper, sehingga belum menjawab konfigurasi audio-only yang
paling efektif dan juga perbandingan model backbone yang hanya berfokus
untuk audio saja.

Penelitian selanjutnya dilakukan oleh Aslan dkk. (2021) yang memprediksi
apparent personality (Big Five) dari klip video dengan menggabungkan 4
fitur, yaitu kondisi latar/*scene*, wajah, suara, dan transkrip. Studi
ini juga menggunakan Chalearn First Impression V2 dengan split 6000 data
untuk train, 2000 data untuk validation, dan 2000 untuk test. Fitur yang
diambil diproses menggunakan backbone pra-latih yang sesuai dengan fitur
masing-masing. Fitur latar/*scene* dan wajah menggunakan ResNet-v2-101 +
LSTM (diambil 1 fps dan di resize 224x224 dimana wajah akan di-*crop*
terlebih dahulu dengan MTCNN). Fitur audio pada langkah awal akan
diproses menjadi log-mel spectogram (25 ms window, 10 ms step, 64 mel
bins) lalu dipotong menjadi urutan frame 960 ms ukuran 96x64 (*time = 96
x mel bins = 64)*, kemudian masuk VGGish (pretrained YouTube) untuk
dapat embedding 128 dimensi per frame dan dilanjut LSTM agar model bisa
lihat pola suara yang berurutan. Fitur teks transkrip di-embed
menggunakan ELMo (1024 dimensi *mean pooled*) lalu diakhiri dengan MLP
untuk *convert* hasil ELMo yang masih umum menjadi 5 skor *Big Five*.
Penelitian memiliki ciri khas dengan menggabungkan *two-stage training*,
langkah pertama yaitu tiap subnetwork dilatih sendiri untuk prediksi
skor *Big Five*, sehingga masing-masing mengeluarkan output FC dengan 5
neuron yaitu 5 skor Big Five dan juga skor rata-rata MAE untuk setiap
model yang sudah matang. Untuk langkah kedua, akan diambil model
pretrained hasil langkah pertama lalu akan di-*freeze* bobotnya. Setelah
itu akan dihapus FC terakhir (*regression layer)* dan gabung (*concat*)
dari semua representasi vector sebelum FC terakhir dari tiap model,
kemudian masuk *feature attention* (fitur yang lebih informatif dapat
bobot lebih besar) yang mana akan menghasilkan daftar bobot fitur dan
diakhiri dengan masuk regressor (FC terakhir) untuk output prediksi Big
Five. Untuk MAE sendiri pada tahap pertama dilakukan *error consistency
constraint* supada model bisa memahami trait lebih merata dan
menghasilkan model pra-latih dengan akurasi yang baik. Evaluasi yang
dilakukan pada studi ini menggunakan *challenge accuracy* = 1 -- MAE.
Hasil dari model tahap pertama yang terbaik adalah model fitur wajah,
dilanjut fitur latar, suara, dan terakhir teks. Untuk kinerja multimodal
hasil dari tahap kedua didapatkan kinerja terbaik dengan *mean accuracy*
0,918 setelah diaktifkan *feature attention* dan *error consistency
contraint.* Meskipun penelitian ini menunjukkan bahwa multimodal bisa
menghasilkan hasil yang baik, tapi belum menunjukkan fitur audio dengan
perbandingan backbone modern.

Selanjutnya ada penelitian lain yang dilakukan oleh Zhao dkk. (2023)
yang juga menggunakan dataset ChaLearn First Impression V2 (10.000 klip,
±15 detik, \>3.000 video YouTube dengan split 6.000 train, 2.000 val,
dan 2.000 test, dataset ini digunakan untuk prediksi kepribadian
berbasis skor Big Five Personality. Dalam penelitian ini dilakukan
ekstraksi fitur dari audio, scene, dan wajah, kemudian memodelkan
dinamika temporal menggunakan Bi-LSTM serta Transformer, dan
menggabungkan beberapa hasil prediksi dengan weighted decision-level
fusion. Evaluasi menggunakan metrik S (semakin tinggi semakin baik) dan
konfigurasi terbaik yang dilaporkan adalah S rata-rata 0,9167 untuk
prediksi kepribadian. Dari sisi gap untuk riset audio-only, studi ini
masih bertumpu pada multimodal video, sehingga belum menjawab
konfigurasi teknis yang spesifik untuk audio-only dan perbandingan
backbone.

Kemudian terdapat penelitian oleh Mawalim dkk. (2023) yang melakukan
prediksi kepribadian Big Five pada konteks diskusi kelompok menggunakan
dua dataset, yaitu MATRICS (diskusi kelompok berbahasa Jepang dengan 40
partisipan dalam 10 grup) dan ELEA-AV (27 pertemuan, 102 partisipan,
durasi rekaman sekitar 15 menit dengan topik *winter survival*).
Pendekatan yang digunakan menggabungkan beberapa sumber informasi,
seperti suara, bahasa, dan gerak/visual, serta menambahkan fitur yang
mewakili karakteristik pembicara agar perbedaan antar-individu ikut
tertangkap dalam prediksi. Hasil yang didapatkan yaitu penambahan
ciri-ciri pembicara membantu meningkatkan kinerja model pada beberapa
dimensi, dan informasi dari fitur suara berkontribusi konsisten dalam
prediksi dibanding fitur visual yang lebih membantu pada trait tertentu.
Dari sisi gap, penelitian ini masih berfokus pada diskusi kelompok
dengan dataset yang relatif terbatas, dan penulis juga menyebutkan belum
banyak mengeksplor pendekatan yang lebih mutakhir, sehingga ruang
pengembangan masih terbuka.

Ada penelitian lain yang dijadikan referensi yaitu penelitian yang
dilakukan oleh Giritlioğlu dkk. (2021) yang membahas prediksi Big Five
dari video dengan memanfaatkan gabungan isyarat wajah, gerak tubuh,
suara, dan transkrip. Penelitian ini juga memperkenalkan dataset SIAP
(*Self-presentation and Induced Behavior Archive for Personality
Analysis*) yaitu rekaman self-presentation dan kondisi perilaku 60
partisipan terhadap pemicu yang diberikan (tiga sudut kamera) dengan
label Big Five berupa penilaian self-assessed dan observed, lalu
evaluasi dilakukan pada SIAP dan juga ChaLearn LAP First Impressions.
Untuk pemodelan, penelitian ini memakai beberapa arsitektur per
modalitas, seperti 3D-ResNext dan CNN-GRU pada informasi wajah, serta
LSTNet untuk voice, transcribed speech, body pose, dan facial action
units. Hasilnya dilaporkan dalam MAE, misalnya voice (LSTNet) memiliki
AVG MAE 0,108 pada FID dan 0,165 pada SIAP-Interview, dan pada skema
fusion performa terbaik mencapai AVG MAE 0,085 pada FID (LSVR) serta
0,153 pada SIAP-Interview (Feature Attention). Penelitian ini
menganalisis kontribusi tiap modalitas dan penggabungan informasi,
kesimpulan yang didapatkan yaitu informasi terkait wajah cenderung
stabil untuk prediksi kepribadian pada data video. Gap yang ada yaitu
masih menggunakan multimodal video, belum membahas perbandingan dan
konfigurasi untuk audio-only.

Selanjutnya ada penelitian yang dilakukan oleh Phan & Rauthmann (2021)
yang membahas *personality computing*, yaitu upaya mengekstrak informasi
kepribadian (misalnya Big Five) dari data yang ditangkap sensor seperti
teks, jejak digital, penggunaan smartphone, perilaku non-verbal, pola
bicara, sampai data dari aktivitas seperti *gameplay*. Artikel ini
menyoroti cara mengevaluasi kinerja prediksi dan isu psikometrik
(reliability dan validity), sekaligus membahas implikasi etis, legal,
dan sosial dari asesmen berbasis sensor. Salah satu catatan pentingnya
adalah kinerja prediksi berbasis *machine learning* terhadap ukuran
kuesioner diperkirakan punya batas atas sekitar r ≈ 0,30--0,50, sehingga
nilai yang lebih tinggi sebaiknya dibaca hati-hati karena berpotensi
terkait *overfitting*. Dari sisi gap untuk riset *audio-only*, artikel
ini lebih berupa peta konsep dan panduan evaluasi umum, belum masuk ke
pembahasan teknis yang spesifik seperti rancangan pipeline audio-only
atau pembandingan backbone pra-latih dan strategi frozen vs fine-tuning.

Penelitian selanjutnya yang dijadikan referensi adalah penelitian oleh
Ghassemi dkk. (2024) yang memprediksi Big Five dengan menggabungkan
multimodal (audio, visual, dan transkrip) pada dataset ChaLearn First
Impressions. Penelitian ini menggunakan *dependency-free* split
berdasarkan Youtube channel ID, karena pada split asli banyak channel
yang muncul di train dan juga muncul di validasi maupun test (sekitar
73,2% di validasi dan 84% di test), sehingga hasil bisa terlihat lebih
tinggi. Metode yang dilakukan di penelitian ini menggabungkan beberapa
jenis fitur dan meringkasnya. Model yang digunakan yaitu BERT (teks),
Wav2Vec (audio embedding transfer), OpenFace + FaceNet-style/Inception
(visual), temporal autoencoder, deep ensemble MLP. Hasil yang didapatkan
yaitu gabungan audio dan visual yang memberikan kinerja lebih baik yaitu
R² avg 0,369 dibandingkan audio saja yang memberikan hasil kinerja
(0,280) atau visual saja (0,314), sementara transkrip sendiri relatif
lemah (0,037). Gap penelitian pada studi ini adalah fokusnya lebih pada
dependency-free split dan pipeline multimodal, sementara eksplorasi
backbone audio pra-latih untuk skenario audio-only masih terbatas.

Penelitian terkait terakhir yang diambil sebagai referensi yaitu
penelitian yang dilakukan oleh Aylett dkk. (2017) yang membahas
bagaimana system TTS (text-to-speech) dan gaya kualitas suara bisa
mengubah skor Big Five pada suara sintetis. Penelitian ini membuat
stimulus dari tiga korpus teks yang terdiri dari About Myself, Speed
Dating Positive, dan Speed Dating Negative. Kemudian dilakukan rendering
sehingga menjadi beberapa versi menggunakan dua tipe system sistesis
yaitu *parametric synthesis* dan *unit selection synthesis*
(CereVoice2), termasuk variasi kualitas suara neutral, lax, dan tense
pada suara tertentu. Penilaian dilakukan oleh 35 partisipan yang memberi
skor Big Five dan naturalness untuk setiap output. Hasil yang didapatkan
yaitu jenis system dan sumber suara berpengaruh pada naturalness dengan
contoh selisih yang jelas pada *neutral unit selection* (rata-rata
naturalness, dengan contoh selisih yang jelas). Naturalness hanya
menunjukkan hubungan kecil dengan Openness (sekitar r = 0,249),
sementara trait lain tidak tampak kuat terkait naturalness. Dari sisi
persepsi Big Five, perubahan *rendition* (misalnya Text vs Lax vs Tense)
juga memunculkan perbedaan yang signifikan pada beberapa trait, termasuk
Agreeableness (contohnya pada korpus About Myself). Gap yang ada pada
penelitian ini adalah berfokus pada persepsi kepribadian dari suara
sintetis, bukan membangun pipeline prediksi Big Five dari rekaman nyata,
sehingga belum masuk ke pembahasan backbone audio pretrained (SSL
Transformer).

+---------------------+--------------------------+---------------------+
| **Judul & Dataset** | **Metode & Hasil**       | **Analisis Gap**    |
+=====================+==========================+=====================+
| **Judul :**         | Pipeline audio-only      | Masih berbasis      |
|                     | klasik:                  | handcrafted         |
| Feasibility of Big  |                          | acoustic features + |
| Data Analytics to   | 1\) Ekstraksi fitur      | RandomForest, belum |
| Assess Personality  | pakai OpenSMILE (set     | membahas            |
| Based on Voice      | AVEC 2011, \~1941 fitur: | konfigurasi         |
| Analysis (Rubio     | MFCC, F0/voicing,        | pretrained speech   |
| dkk., 2024).        | jitter--shimmer, HNR,    | transformer untuk   |
|                     | dsb).                    | audio-only.         |
| **Dataset :**       |                          |                     |
|                     | 2\) Analisis korelasi    | Belum ada studi     |
| 100 partisipan,     | fitur ↔ trait.           | perbandingan        |
| direkam saat        |                          | backbone untuk      |
| wawancara           | 3\) Untuk prediksi, skor | audio-only          |
| individual, label   | trait diubah jadi 3      |                     |
| kepribadian dari    | kelas (low/medium/high)  |                     |
| NEO-FFI             | dengan batas P25 & P75   |                     |
| (self-report), plus | (berdasarkan data        |                     |
| penilaian "close    | training), lalu dilatih  |                     |
| other" dan rating   | Random Forest memakai    |                     |
| ahli berbasis       | top-5 fitur paling       |                     |
| rekaman             | berkorelasi untuk tiap   |                     |
|                     | trait.                   |                     |
|                     |                          |                     |
|                     | Hasil yang didapatkan:   |                     |
|                     |                          |                     |
|                     | 1\) Korelasi fitur-suara |                     |
|                     | vs skor trait di kisaran |                     |
|                     | r \~0,3--0,4             |                     |
|                     |                          |                     |
|                     | 2\) Akurasi klasifikasi  |                     |
|                     | 3 kelas sekitar 43%--60% |                     |
+---------------------+--------------------------+---------------------+
| Judul:              | Audio-only pada First    | Studi ini baru      |
|                     | Impressions, bandingkan  | memberi baseline    |
| Apparent            | eGeMAPS + speech ratio   | audio-only dan      |
| personality         | vs embedding wav2vec     | menekankan          |
| prediction from     | 2.0, lalu latih Random   | pentingnya split    |
| speech using expert | Forest Regressor dan     | ketat (by video     |
| features and        | DNN. Terbaik DNN dengan  | ID), tetapi belum   |
| wav2vec 2.0 (Barchi | R²_avg 0,33 (official    | membandingkan       |
| dkk., 2023)         | split) dan 0,28 (split   | backbone SSL lain.  |
|                     | ketat by video ID).      |                     |
| Dataset:            |                          |                     |
|                     |                          |                     |
| First Impressions   |                          |                     |
| Dataset Challenge   |                          |                     |
| ChaLearn LAP 2016   |                          |                     |
| (wawancara kerja,   |                          |                     |
| 10.000 klip 15      |                          |                     |
| detik, ). Dataset   |                          |                     |
| ini sudah dilabeli  |                          |                     |
| Big Five yang       |                          |                     |
| dinilai oleh        |                          |                     |
| listener (apparent  |                          |                     |
| personality)        |                          |                     |
+---------------------+--------------------------+---------------------+
| Judul:              | YAMNet (AudioSet) untuk  | Masih audio+teks    |
|                     | embedding akustik +      | (bukan audio-only), |
| Speech-based        | Whisper →                | backbone akustik    |
| personality         | text-embedding-ada-002   | yang dipakai bukan  |
| prediction using    | untuk embedding          | SSL Transformer,    |
| deep learning with  | linguistik, digabung     | dan belum menguji   |
| acoustic and        | (1024+1536) lalu regresi | perbandingan        |
| linguistic          | XGBoost dengan evaluasi  | backbone.           |
| embeddings (Lukac,  | korelasi r = 0,26--0,39  |                     |
| 2024)               |                          |                     |
|                     |                          |                     |
| Dataset:            |                          |                     |
|                     |                          |                     |
| Free-form speech +  |                          |                     |
| self-report Big     |                          |                     |
| Five, N = 2045      |                          |                     |
+---------------------+--------------------------+---------------------+
| Judul:              | 4 modalitas (scene,      | Metode utamanya     |
|                     | wajah, suara,            | multimodal, bukan   |
| Multimodal          | transkrip). Backbone     | audio-only. Belum   |
| assessment of       | pra-latih:               | membahas            |
| apparent            | ResNet-v2-101 + LSTM     | perbandingan        |
| personality using   | (scene & wajah),         | backbone SSL audio  |
| feature attention   | VGGish + LSTM (audio     | modern.             |
| and error           | log-mel), ELMo + MLP     |                     |
| consistency         | (teks). Fusion pakai     |                     |
| constraint (Aslan   | feature attention +      |                     |
| dkk., 2021)         | error consistency.       |                     |
|                     | Hasil: mean accuracy ≈   |                     |
| Dataset:            | 0,918.                   |                     |
|                     |                          |                     |
| ChaLearn First      |                          |                     |
| Impressions V2:     |                          |                     |
| 10.000 video        |                          |                     |
| YouTube, rata-rata  |                          |                     |
| ±15 detik, label    |                          |                     |
| apparent Big Five   |                          |                     |
| (AMT).              |                          |                     |
+---------------------+--------------------------+---------------------+
| Judul:              | Fitur audio+visual       | Fokus multimodal    |
|                     | (scene & wajah),         | dan bukan           |
| Integrating audio   | temporal modeling pakai  | *audio-only,* dan   |
| and visual          | Bi-LSTM dan Transformer, | dalam penelitian    |
| modalities for      | lalu fusion berbobot     | ini belum ada       |
| multimodal          | (decision-level).        | perbandingan        |
| personality trait   | Terbaik: S_avg 0,9167    | backbone.           |
| recognition via     | (late fusion).           |                     |
| hybrid deep         |                          |                     |
| learning (Zhao      |                          |                     |
| dkk., 2023)         |                          |                     |
|                     |                          |                     |
| Dataset:            |                          |                     |
|                     |                          |                     |
| ChaLearn First      |                          |                     |
| Impression-V2:      |                          |                     |
| 10.000 klip (\~15   |                          |                     |
| s) dari \>3.000     |                          |                     |
| video YouTube       |                          |                     |
| dengan split        |                          |                     |
| 6.000/2.000/2.000   |                          |                     |
| dan telah dilabeli  |                          |                     |
| Big Five (0--1) &   |                          |                     |
| "Interview".        |                          |                     |
+---------------------+--------------------------+---------------------+
| Judul:              | Fitur multimodal (audio, | Fokus pada diskusi  |
|                     | bahasa, motion/visual) + | kelompok, korpus    |
| Personality trait   | speaker embedding        | relatif kecil &     |
| estimation in group | i-vector/x-vector.       | bahasa terbatas.    |
| discussions using   | Menggunakan klasifikasi  | Belum mengeksplor   |
| multimodal analysis | biner Big Five, evaluasi | pendekatan yang     |
| and speaker         | LOPCV/10-fold +          | lebih mutakhir.     |
| embedding (Mawalim  | ablation, contoh         |                     |
| dkk., 2023)         | peningkatan F1           |                     |
|                     | neuroticism 61→68%       |                     |
| Dataset:            | (ELEA-AV) dan 68→79%     |                     |
|                     | (MATRICS, skema          |                     |
| MATRICS (diskusi    | pembanding)              |                     |
| kelompok Jepang, 40 |                          |                     |
| partisipan, 10      |                          |                     |
| grup), ELEA-AV (27  |                          |                     |
| meeting, 102        |                          |                     |
| partisipan dengan   |                          |                     |
| durasi ±15 menit    |                          |                     |
| berisi topik        |                          |                     |
| tentang *winter     |                          |                     |
| survival task*      |                          |                     |
+---------------------+--------------------------+---------------------+
| Judul:              | Memodelkan tiap          | Lebih fokus ke      |
|                     | modalitas dengan         | dataset & induced   |
| Multimodal analysis | arsitektur khusus        | behavior, bukan     |
| of personality      | (misalnya                | evaluasi backbone   |
| traits on videos of | 3D-ResNext/CNN-GRU untuk | SSL audio untuk Big |
| self-presentation   | wajah dan LSTNet untuk   | Five audio-only,    |
| and induced         | voice, transkrip, pose,  | dan bukan           |
| behavior            | serta action units),     | membandingkan       |
| (Giritlioğlu dkk.,  | lalu melakukan fusion,   | frozen vs           |
| 2021)               | hasil terbaik dilaporkan | fine-tuning.        |
|                     | mencapai AVG MAE 0,085   |                     |
| Dataset:            | pada FID (LSVR) dan      |                     |
|                     | 0,153 pada               |                     |
| Dataset SIAP        | SIAP-Interview (Feature  |                     |
| (*Self-presentation | Attention).              |                     |
| and Induced         |                          |                     |
| Behavior Archive    |                          |                     |
| for Personality     |                          |                     |
| Analysis*) yang     |                          |                     |
| melibatkan 60       |                          |                     |
| subjek direkam pada |                          |                     |
| beberapa sesi, dan  |                          |                     |
| ChaLearn LAP First  |                          |                     |
| Impressions         |                          |                     |
+---------------------+--------------------------+---------------------+
| Personality         | Tinjauan literatur       | Tidak memberikan    |
| computing: New      | tentang konsep           | rancangan pipeline  |
| frontiers in        | personality computing,   | teknis audio-only   |
| personality         | evaluasi performa, isu   | Big Five, juga      |
| assessment (Phan &  | psikometrik dan          | tidak membahas      |
| Rauthmann, 2021)    | implikasi etis dengan    | komparasi backbone  |
|                     | menekankan adanya        | SSL audio dan       |
| Dataset:            | perkiraan ceiling        | strategi pelatihan. |
|                     | convergent validity      |                     |
| Tidak menggunakan   | sekitar r ≈ 0,30--0,50   |                     |
| dataset, karena     | dan risiko overfitting   |                     |
| studi ini rangkuman | jika terlalu tinggi.     |                     |
| sumber data         |                          |                     |
| "*sensor-assessed*" |                          |                     |
| seperti teks, jejak |                          |                     |
| digital, penggunaan |                          |                     |
| *smartphone*,       |                          |                     |
| perilaku            |                          |                     |
| non-verbal, pola    |                          |                     |
| berbicara, dan      |                          |                     |
| *game-play*         |                          |                     |
+---------------------+--------------------------+---------------------+
| Unsupervised        | Multimodal (audio,       | Fokus utama pada    |
| Multimodal Learning | visual, transkrip)       | dependency-free     |
| for Dependency-Free | dengan agregasi          | split dan pipeline  |
| Personality         | temporal.                | multimodal,         |
| Recognition         | Model/komponen: BERT     | eksplorasi backbone |
| (Ghassemi dkk.,     | (teks), Wav2Vec (audio   | audio pra-latih     |
| 2024)               | embedding transfer),     | untuk skenario      |
|                     | OpenFace +               | audio-only masih    |
| Dataset:\           | FaceNet-style/Inception  | terbatas (tidak     |
| ChaLearn First      | (visual), temporal       | membandingkan       |
| Impressions (10.000 | autoencoder, dan         | banyak backbone).   |
| klip \~15 detik,    | prediksi akhir dengan    |                     |
| dari YouTube),      | deep ensemble MLP**.**   |                     |
| memakai             | Hasil: audio+visual R²   |                     |
| dependency-free     | avg 0,369 (audio 0,280,  |                     |
| split berbasis      | visual 0,314, transkrip  |                     |
| YouTube channel ID  | 0,037).                  |                     |
| karena overlap      |                          |                     |
| channel pada split  |                          |                     |
| asli tinggi (±73,2% |                          |                     |
| val, ±84% test).    |                          |                     |
+---------------------+--------------------------+---------------------+
| Speech synthesis    | Membandingkan parametric | Fokus pada          |
| for the generation  | synthesis vs unit        | membentuk/menilai   |
| of artificial       | selection synthesis      | persepsi            |
| personality (Aylett | (CereVoice2), plus       | kepribadian pada    |
| dkk., 2017)         | variasi voice quality    | suara sintetis,     |
|                     | (neutral/lax/tense).     | bukan prediksi Big  |
| Dataset:            | Hasil contoh:            | Five audio-only     |
|                     | naturalness neutral unit | dari data wawancara |
| Tiga korpus teks:   | selection sekitar 3,1 vs | nyata; tidak        |
| About Myself, Speed | 3,9 (tergantung voice),  | membahas            |
| Dating Positive,    | dan naturalness hanya    | perbandingan        |
| Speed Dating        | berkorelasi kecil dengan | backbone SSL audio. |
| Negative. Stimulus  | Openness (r≈0,249).      |                     |
| berupa ucapan       |                          |                     |
| sintetis yang       |                          |                     |
| dinilai oleh 35     |                          |                     |
| partisipan (skor    |                          |                     |
| Big Five +          |                          |                     |
| naturalness).       |                          |                     |
+---------------------+--------------------------+---------------------+

: []{#_Ref217659410 .anchor}Tabel 2.1 Penelitian Terkait

## Dasar Teori

Subbab ini memaparkan landasan teori yang digunakan sebagai dasar dalam
penelitian estimasi kepribadian Big Five berbasis audio-only. Landasan
teori mencakup konsep kepribadian Big Five dan jenis label yang
digunakan pada dataset, karakteristik sinyal suara dan fitur yang umum
dipakai dalam analisis suara, prinsip dasar pemodelan berbasis
representasi dengan Transformer pra-latih untuk audio, serta strategi
pelatihan (frozen feature extraction dan fine-tuning) dan metrik
evaluasi yang digunakan untuk menilai kinerja model pada skenario
audio-only.

### Kepribadian Big Five

Kepribadian dapat dipahami sebagai kumpulan perbedaan individu yang
sering disebut sebagai trait psikologis, yang menjelaskan pola proses
mental dan perilaku yang cenderung bertahan dari waktu ke waktu (Phan &
Rauthmann, 2021). Karena *trait* tidak dapat diamati secara langsung,
keberadaannya umumnya disimpulkan dari pola respons atau reaksi yang
berulang (Phan & Rauthmann, 2021). Dalam penelitian ini, kerangka yang
digunakan adalah Big Five atau Five-Factor Model (FFM), yaitu
pengelompokan trait ke dalam lima dimensi dasar (McCrae & John, 1992).
Lima dimensi tersebut adalah Extraversion, Agreeableness,
Conscientiousness, Neuroticism, dan Openness to Experience (McCrae &
John, 1992). Struktur lima dimensi ini juga sering muncul dari kata-kata
sifat yang biasa dipakai untuk menggambarkan kepribadian, dan pola yang
serupa dilaporkan tetap terlihat pada beberapa bahasa yang berbeda
(Goldberg, 1990). Big Five banyak digunakan sebagai taksonomi *trait*
yang umum dipakai dalam berbagai pengukuran kepribadian (Phan &
Rauthmann, 2021). Ini dikarenakan trait yang diukur cenderung relatif
stabil dari waktu ke waktu, dapat digunakan lintas budaya, dan
dilaporkan berkaitan dengan berbagai hasil penting dalam kehidupan
(Mawalim dkk., 2023). FFM ini didukung oleh banyak penelitian, baik yang
memakai kata-kata sifat dalam bahasa sehari-hari maupun kuesioner
kepribadian, dan model ini dilaporkan tetap relevan meski dinilai oleh
orang yang berbeda atau berasal dari budaya yang berbeda (McCrae & John,
1992).

Untuk memberi gambaran ringkas, kelima dimensi Big Five sering
dijelaskan melalui contoh ciri berikut (Phan & Rauthmann, 2021).

1.  Openness: rasa ingin tahu, imajinatif, peka estetika.

2.  Conscientiousness: terorganisasi, produktif, bertanggung jawab.

3.  Extraversion: supel, tegas, energik.

4.  Agreeableness: berbelas kasih, menghargai orang lain, mudah percaya.

5.  Neuroticism / Negative Affectivity: mudah cemas, murung, emosinya
    mudah berubah.

Dalam praktiknya, Big Five biasanya diukur menggunakan kuesioner yang
menghasilkan skor untuk kelima dimensinya. Salah satu alat ukur yang
cukup sering digunakan adalah Big Five Inventory (BFI), yang tersedia
dalam versi lengkap maupun versi singkat. Sebagai contoh, BFI-44 terdiri
dari 44 butir pertanyaan, sedangkan BFI-10 hanya memuat 10 butir
sehingga lebih cocok dipakai ketika waktu pengisian terbatas dan
dibutuhkan pengukuran yang ringkas (Rammstedt & John, 2007). Meski versi
singkat memudahkan saat waktu pengisian terbatas, beberapa kajian
menilai skala yang terlalu singkat berisiko kurang menangkap ragam
aspek/nuansa kepribadian dibandingkan versi yang lebih panjang (Mõttus
dkk., 2019).

### Jenis label: *self-report* vs *apparent personality*

Dalam penelitian prediksi kepribadian, label (ground truth) umumnya
berasal dari dua sumber yang berbeda, yaitu *self-report* dan *apparent
personality (Lukac, 2024)*. *Self-report* diperoleh saat seseorang
menilai dirinya sendiri melalui kuesioner Big Five (Lukac, 2024). Dengan
cara ini, skor yang dikumpulkan merepresentasikan bagaimana individu
melihat dirinya sendiri, sehingga label sangat bergantung pada jawaban
pribadi responden (Lukac, 2024). Pada studi Lukac (2024), peserta
mengisi kuesioner Big Five (self-reported) lalu memberikan sampel suara,
dan model kemudian dilatih untuk memprediksi skor yang dilaporkan oleh
peserta tersebut. Namun, *self-report* tetap memiliki keterbatasan,
misalnya jawaban dapat dipengaruhi kecenderungan ingin terlihat baik
atau bias lain dalam pelaporan diri (Lukac, 2024).

Sedangkan *apparent personality* (sering juga dipahami sebagai
*perceived personality*) diperoleh dari penilaian orang lain berdasarkan
perilaku yang terlihat/terdengar, bukan dari pengakuan diri sendiri dari
subjek penelitian (L´opez dkk., 2016). Pada konteks dataset First
Impressions (ChaLearn LAP 2016), target Big Five dinilai menggunakan
*human judgment* dari banyak penilai (*crowd workers*) terhadap klip
video subjek yang berbicara di depan kamera, sehingga label yang
dihasilkan merepresentasikan kesan yang terbentuk pada pengamat
(*apparent*) (L´opez dkk., 2016). Selain itu, studi Aylett dkk. (2017)
menunjukkan bahwa penilaian Big Five juga dapat dilakukan oleh pendengar
ketika diberikan stimulus ujaran (termasuk ujaran sintetis), dan
persepsi sifat dapat berubah seiring perbedaan kualitas suara maupun isi
teks yang diucapkan.

Secara ringkas, perbedaannya dapat dipahami sebagai berikut:

1.  Self-report: skor "kepribadian menurut diri sendiri" (identitas
    diri), biasanya dari kuesioner (Lukac, 2024).

2.  Apparent personality: skor "kepribadian menurut orang lain"
    (kesan/penilaian pengamat) berdasarkan cuplikan perilaku, misalnya
    klip wawancara (Aylett dkk., 2017; L´opez dkk., 2016).

Karena penelitian ini memakai dataset berlabel *apparent*, maka hasil
prediksi yang dibahas nantinya lebih tepat dipahami sebagai kemampuan
model meniru penilaian pengamat terhadap kepribadian, bukan semata-mata
"kepribadian yang dilaporkan oleh subjek sendiri".

### Dataset & karakter tugas (audio-only personality estimation)

Penelitian ini akan menggunakan dataset First Impressions V2 dari
rangkaian ChaLearn LAP 2016 *First Impressions Challenge*. Dataset ini
berisi 10.000 segmen video pendek yang dikumpulkan dari konten YouTube,
dengan durasi 15 detik per segmen (L´opez dkk., 2016). Pada dataset ini,
target yang diprediksi berupa lima skor Big Five (dan pada challenge
juga ada skor "interview"), yang disajikan sebagai nilai kontinu pada
rentang 0 sampai 1(L´opez dkk., 2016). Pembagian data (split) yang umum
dipakai mengikuti pembagian dari sumber dataset, yaitu 6.000 data untuk
train, 2.000 untuk validation, dan 2.000 untuk test (L´opez dkk., 2016).
Walaupun dataset ini pada dasarnya bersifat multimodal (misalnya
tersedia fitur visual dan audio), penelitian ini berfokus pada
audio-only, sehingga input yang dipakai adalah fitur suara yang
diekstrak dari dataset tersebut.

Estimasi kepribadian menggunakan data suara dapat dipahami sebagai upaya
memetakan cuplikan suara singkat menjadi lima skor Big Five. Namun, pada
dataset seperti First Impressions V2, perhatian khusus perlu diberikan
pada cara evaluasi dan split. Ghassemi dkk. (2024) menyoroti bahwa split
bawaan dataset berpotensi menyisakan ketergantungan subjek
(subject-dependency) karena sebagian besar video pada validation dan
test masih berbagi channel YouTube yang sama dengan data train. Kondisi
ini dikhawatirkan membuat performa tampak lebih tinggi karena model
"terbantu" oleh kemiripan identitas yang terselip antar split, bukan
murni kemampuan generalisasi (Ghassemi et al., 2021). Karena itu, split
berbasis identitas yang lebih ketat (misalnya berdasarkan channel atau
identitas pembicara) sering disarankan agar risiko leakage bisa ditekan
(Ghassemi et al., 2021).

Dari sisi bidangnya, prediksi kepribadian dari suara biasanya dibahas
dalam ranah paralinguistik, yaitu pemanfaatan isyarat nonverbal pada
ucapan (misalnya nada, intensitas, kelancaran, dan ritme) untuk
mengaitkan pola vokal dengan trait tertentu (Rubio et al., 2024). Pada
beberapa challenge terdahulu, pendekatan berbasis fitur akustik juga
banyak digunakan, termasuk penggunaan tool seperti OpenSMILE untuk
mengekstraksi fitur suara sebagai baseline atau pembanding (Rubio et
al., 2024). Gambaran ini membantu menjelaskan mengapa pendekatan
audio-only masih relevan, sekaligus menguatkan bahwa evaluasi yang
hati-hati (terutama pada split) menjadi bagian penting dari karakter
tugas ini.

### Pra-pemrosesan audio untuk pipeline

### Representasi fitur suara

### Self-supervised pretraining & pretrained speech transformers

### Strategi pelatihan: frozen feature extraction vs fine-tuning

### Pemodelan prediksi Big Five (regresi)

### Metrik evaluasi & skema validasi

### Strategi split untuk mencegah leakage

*Halaman ini sengaja dikosongkan.*

#  METODOLOGI

## Metode yang Digunakan

## Bahan dan Peralatan yang Digunakan 

## Urutan Pelaksanaan Penelitian

Penelitian Tugas Akhir ini akan dilaksanakan selama enam bulan dari
Maret sampai dengan September 2024. Lini masa pengerjaan Tugas Akhir
bisa dilihat pada Tabel ‎3.1. Judul tabel perlu ditulis dalam format
*Title Case*, yang berarti setiap kata diawali huruf kapital, kecuali
untuk kata depan seperti 'di', 'ke', 'dari', 'yang', 'untuk', 'kepada',
dan sebagainya.

  ---------------------------------------------------------------------------
  No   Aktivitas              MAR   APR   MEI   JUN         JUL   AGU   SEP
  ---- ---------------------- ----- ----- ----- ----- ----- ----- ----- -----
  1    Studi literatur                                                  

  2    Empati konteks                                                   
       permasalahan                                                     

  3    Definisi spesifikasi                                             
       kebutuhan                                                        

  4    Ideasi solusi                                                    

  5    Pembuatan purwarupa                                              
       solusi                                                           

  6    Evaluasi                                                         

  7    Penulisan laporan                                                
       Tugas Akhir                                                      
  ---------------------------------------------------------------------------

  : []{#_Ref193440555 .anchor}Tabel 3.1 Lini Masa Pengerjaan Tugas Akhir

*Halaman ini sengaja dikosongkan.*

# DAFTAR PUSTAKA {#daftar-pustaka .Heading-0}

Aslan, S., Güdükbay, U., & Dibeklioğlu, H. (2021). Multimodal assessment
of apparent personality using feature attention and error consistency
constraint. *Image and Vision Computing*, *110*.
https://doi.org/10.1016/j.imavis.2021.104163

Aylett, M. P., Vinciarelli, A., & Wester, M. (2017). Speech synthesis
for the generation of artificial personality. *IEEE Transactions on
Affective Computing*, *11*(2), 361--372.
https://doi.org/10.1109/TAFFC.2017.2763134

Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). *wav2vec 2.0: A
Framework for Self-Supervised Learning of Speech Representations*.
http://arxiv.org/abs/2006.11477

Barchi, R., Pepino, L., Gauder, L., Estienne, L., Meza, M., Riera, P., &
Ferrer, L. (2023). *Apparent personality prediction from speech using
expert features and wav2vec 2.0*. 21--25.
https://doi.org/10.21437/smm.2023-5

Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda,
N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian,
Y., Wu, J., Zeng, M., Yu, X., & Wei, F. (2022). *WavLM: Large-Scale
Self-Supervised Pre-Training for Full Stack Speech Processing*.
https://doi.org/10.1109/JSTSP.2022.3188113

Ghassemi, S., Zhang, T., Van Breda, W., Koutsoumpis, A., Oostrom, J. K.,
Holtrop, D., & De Vries, R. E. (2024). Unsupervised Multimodal Learning
for Dependency-Free Personality Recognition. *IEEE Transactions on
Affective Computing*, *15*(3), 1053--1066.
https://doi.org/10.1109/TAFFC.2023.3318367

Giritlioğlu, D., Mandira, B., Yilmaz, S. F., Ertenli, C. U., Akgür, B.
F., Kınıklıoğlu, M., Kurt, A. G., Mutlu, E., Gürel, Ş. C., &
Dibeklioğlu, H. (2021). Multimodal analysis of personality traits on
videos of self-presentation and induced behavior. *Journal on Multimodal
User Interfaces*, *15*(4), 337--358.
https://doi.org/10.1007/s12193-020-00347-7

Goldberg, L. R. (1990). *An Alternative "Description of Personality":
The Big-Five Factor Structure*.

Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R.,
& Mohamed, A. (2021). *HuBERT: Self-Supervised Speech Representation
Learning by Masked Prediction of Hidden Units*.
http://arxiv.org/abs/2106.07447

L´opez, V. P., Chen, B., Oliu, M., Corneanu, C., Clapes, A., Guyon, I.,
Baro, X., Escalante, H. J., & Escalera, S. (2016). *ChaLearn LAP 2016:
First Round Challenge on First Impressions - Dataset and Results* (G.
Hua & H. Jégou, Ed.; Vol. 9915). Springer International Publishing.
https://doi.org/10.1007/978-3-319-49409-8

Lukac, M. (2024). Speech-based personality prediction using deep
learning with acoustic and linguistic embeddings. *Scientific Reports*,
*14*(1). https://doi.org/10.1038/s41598-024-81047-0

Mawalim, C. O., Okada, S., Nakano, Y. I., & Unoki, M. (2023).
Personality trait estimation in group discussions using multimodal
analysis and speaker embedding. *Journal on Multimodal User Interfaces*,
*17*(2), 47--63. https://doi.org/10.1007/s12193-023-00401-0

McCrae, R. R., & John, O. P. (1992). *An Introduction to the Five-Factor
Model and Its Applications*.
https://digitalcommons.unl.edu/publichealthresources

Mõttus, R., Sinick, J., Terracciano, A., Hřebíčková, M., Kandler, C.,
Ando, J., Mortensen, E. L., Colodro-Conde, L., & Jang, K. L. (2019).
Personality characteristics below facets: A replication and
meta-analysis of cross-rater agreement, rank-order stability,
heritability, and utility of personality nuances. *Journal of
Personality and Social Psychology*, *117*(4), e35--e50.
https://doi.org/10.1037/pspp0000202

Phan, L. V., & Rauthmann, J. F. (2021). Personality computing: New
frontiers in personality assessment. *Social and Personality Psychology
Compass*, *15*(7). https://doi.org/10.1111/spc3.12624

Rammstedt, B., & John, O. P. (2007). Measuring personality in one minute
or less: A 10-item short version of the Big Five Inventory in English
and German. *Journal of Research in Personality*, *41*(1), 203--212.
https://doi.org/10.1016/j.jrp.2006.02.001

Rubio, V. J., Aguado, D., Toledano, D. T., & Fernández-Gallego, M. P.
(2024). Feasibility of Big Data Analytics to Assess Personality Based on
Voice Analysis. *Sensors*, *24*(22). https://doi.org/10.3390/s24227151

Zhao, X., Liao, Y., Tang, Z., Xu, Y., Tao, X., Wang, D., Wang, G., & Lu,
H. (2023). Integrating audio and visual modalities for multimodal
personality trait recognition via hybrid deep learning. *Frontiers in
Neuroscience*, *16*. https://doi.org/10.3389/fnins.2022.1107284

 

*Halaman ini sengaja dikosongkan.*

# LAMPIRAN {#lampiran .Heading-0}
