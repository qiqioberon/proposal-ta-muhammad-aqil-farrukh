{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb022f5a",
   "metadata": {},
   "source": [
    "# Pembuka\n",
    "Notebook ini mendokumentasikan eksperimen training V1: memeriksa dataset & fold, bake-off backbone frozen, ekstraksi embedding penuh, evaluasi klasifikasi tertile, training final, hingga demo inferensi. Jalankan sel berurutan; setiap penjelasan sel merinci tujuan dan artefak yang dihasilkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: ['extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness']\n",
      "Group column: person_id | Fold column: fold\n",
      "X: 25 features | Rows: 5425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"/kaggle/input/traningv1\")\n",
    "df = pd.read_csv(BASE/\"fi_v2_meta.with_feats.clean.folds.csv\")\n",
    "\n",
    "# --- Samakan nama kolom person ---\n",
    "if \"group_id\" in df.columns and \"person_id\" not in df.columns:\n",
    "    df = df.rename(columns={\"group_id\": \"person_id\"})\n",
    "\n",
    "# --- Pastikan kolom fold ada; kalau tidak, join dari person_folds.csv ---\n",
    "if \"fold\" not in df.columns:\n",
    "    folds = pd.read_csv(BASE/\"person_folds.csv\")\n",
    "    if \"group_id\" in folds.columns and \"person_id\" not in folds.columns:\n",
    "        folds = folds.rename(columns={\"group_id\": \"person_id\"})\n",
    "    df = df.merge(folds[[\"person_id\",\"fold\"]], on=\"person_id\", how=\"left\")\n",
    "    assert df[\"fold\"].notna().all(), \"Ada person tanpa fold — cek person_folds.csv\"\n",
    "\n",
    "# --- Deteksi target & fitur ---\n",
    "target_cols = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "meta_like = {\"person_id\",\"fold\",\"raw_id\",\"relpath\",\"wav_path\",\"duration_sec\"}\n",
    "num_cols = [c for c in df.select_dtypes(\"number\").columns if c not in meta_like|set(target_cols)]\n",
    "\n",
    "print(\"Targets:\", target_cols)\n",
    "print(\"Group column: person_id | Fold column: fold\")\n",
    "print(\"X:\", len(num_cols), \"features | Rows:\", len(df))\n",
    "\n",
    "# --- Cek kebocoran: tiap person harus 1 fold saja ---\n",
    "assert (df.groupby(\"person_id\")[\"fold\"].nunique() == 1).all(), \"LEAKAGE detected!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81718a5",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 1)\n",
    "- Memuat dataset fitur bersih, menstandarkan kolom person_id/fold, dan menentukan target serta fitur numerik.\n",
    "- Validasi: cek tidak ada leakage (tiap person hanya satu fold) dan menampilkan jumlah fitur/baris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ranges:\n",
      "                        min       max      mean\n",
      "extraversion       0.018692  0.925234  0.483551\n",
      "agreeableness      0.000000  1.000000  0.555661\n",
      "conscientiousness  0.000000  0.970874  0.531477\n",
      "neuroticism        0.020833  0.979167  0.529956\n",
      "openness           0.000000  1.000000  0.573542\n",
      "\n",
      "Distribusi per fold:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>persons</th>\n",
       "      <th>rows_per_person</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1093</td>\n",
       "      <td>1093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1017</td>\n",
       "      <td>1017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1121</td>\n",
       "      <td>1121</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1106</td>\n",
       "      <td>1106</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rows  persons  rows_per_person\n",
       "fold                                \n",
       "0     1093     1093              1.0\n",
       "1     1017     1017              1.0\n",
       "2     1121     1121              1.0\n",
       "3     1106     1106              1.0\n",
       "4     1088     1088              1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "# Asumsi df, target_cols, num_cols sudah ada dari cell sebelumnya\n",
    "print(\"Label ranges:\")\n",
    "print(df[target_cols].agg(['min','max','mean']).T)\n",
    "\n",
    "print(\"\\nDistribusi per fold:\")\n",
    "byf = df.groupby('fold').agg(rows=('fold','size'),\n",
    "                             persons=('person_id','nunique'))\n",
    "byf['rows_per_person'] = (byf['rows']/byf['persons']).round(2)\n",
    "display(byf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc6e127",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 2)\n",
    "- Menyajikan rentang label dan distribusi ukuran per fold/person untuk memastikan skala 0-1 serta keseimbangan data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label 0–1 semua (bagus; konsisten). Rata-rata ~0.48–0.57 wajar.\n",
    "\n",
    "Fold balance: 1017–1121 per fold (imbalance < ~5%) → oke.\n",
    "\n",
    "rows_per_person = 1.0 ⇒ satu sampel per orang; pakai person_id tetap aman (efeknya sama seperti KFold, tapi future-proof kalau nanti ada multi-clip per orang)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cek apakah distribusi label per fold mirip global**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max deviasi mean per trait (per fold vs global):\n",
      "extraversion         0.002532\n",
      "agreeableness        0.004962\n",
      "conscientiousness    0.008739\n",
      "neuroticism          0.007431\n",
      "openness             0.003056\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>openness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.485160</td>\n",
       "      <td>0.555372</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.536339</td>\n",
       "      <td>0.576599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.486082</td>\n",
       "      <td>0.560623</td>\n",
       "      <td>0.530410</td>\n",
       "      <td>0.530502</td>\n",
       "      <td>0.574588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.481513</td>\n",
       "      <td>0.552048</td>\n",
       "      <td>0.531131</td>\n",
       "      <td>0.522525</td>\n",
       "      <td>0.571791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.482990</td>\n",
       "      <td>0.556425</td>\n",
       "      <td>0.528889</td>\n",
       "      <td>0.532164</td>\n",
       "      <td>0.573739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.482236</td>\n",
       "      <td>0.554258</td>\n",
       "      <td>0.526681</td>\n",
       "      <td>0.528445</td>\n",
       "      <td>0.571099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      extraversion  agreeableness  conscientiousness  neuroticism  openness\n",
       "fold                                                                       \n",
       "0         0.485160       0.555372           0.540216     0.536339  0.576599\n",
       "1         0.486082       0.560623           0.530410     0.530502  0.574588\n",
       "2         0.481513       0.552048           0.531131     0.522525  0.571791\n",
       "3         0.482990       0.556425           0.528889     0.532164  0.573739\n",
       "4         0.482236       0.554258           0.526681     0.528445  0.571099"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold_stats = df.groupby('fold')[target_cols].mean()\n",
    "global_mean = df[target_cols].mean()\n",
    "print(\"Max deviasi mean per trait (per fold vs global):\")\n",
    "print((fold_stats - global_mean).abs().max())\n",
    "fold_stats  # lihat tabel mean per fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a32eb",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 3)\n",
    "- Mengukur deviasi rata-rata label per fold dibanding rata-rata global untuk melihat keseimbangan target antar fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skenario 1 — Transformer Pra-latih (**Frozen**)\n",
    "\n",
    "---\n",
    "\n",
    "## Ringkasan Singkat\n",
    "\n",
    "Di skenario ini kita **tidak melatih ulang** backbone transformer (Wav2Vec2/HuBERT/WavLM), melainkan **membekukannya (frozen)**, lalu **mengekstrak embedding** audio dan melatih **head ringan** (Ridge/MLP kecil) untuk memetakan embedding → **skor Big Five (0–1)**.\n",
    "**Step-0** bertujuan memilih **backbone terbaik** secara cepat dan objektif sebelum masuk eksperimen penuh.\n",
    "\n",
    "---\n",
    "\n",
    "## Tujuan\n",
    "\n",
    "* Memilih **1 backbone pra-latih** terbaik (basis “base”) untuk skenario frozen.\n",
    "* Menetapkan **konfigurasi awal**: 16 kHz mono, **mean pooling**, **head Ridge**, **fold per-speaker**.\n",
    "* Mendapatkan **patokan performa** (MAE & lift vs **null baseline**) + **waktu ekstraksi**.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset & Protokol Validasi\n",
    "\n",
    "* **Data**: subset audio-only First Impressions V2 (baris = 1 klip / 1 orang).\n",
    "* **Split**: 5-fold **speaker-independent** (kolom `fold`, group = `person_id`).\n",
    "* **Evaluasi**: gunakan fold yang **sudah ada**; **fit** scaler/model hanya di **train fold** → evaluasi di **val fold** (anti-leak).\n",
    "\n",
    "---\n",
    "\n",
    "## Apa itu “Frozen” (cerita singkat)\n",
    "\n",
    "Bayangkan backbone sebagai **“guru telinga”** yang sudah belajar dari jutaan jam suara.\n",
    "Di mode **frozen**, kita **tidak mengajari ulang gurunya**—kita cuma minta dia **mendengarkan** dan mengeluarkan **catatan (embedding)**. Lalu **asisten kecil** (Ridge/MLP) menerjemahkan catatan itu menjadi skor Big Five.\n",
    "Secara teknis: `model.eval()`, `torch.no_grad()`, semua bobot **tanpa grad** → **tidak ada backprop** ke backbone.\n",
    "\n",
    "---\n",
    "\n",
    "## Backbone Kandidat (base, ringan, stabil)\n",
    "\n",
    "* `microsoft/wavlm-base-plus`\n",
    "* `facebook/hubert-base-ls960`\n",
    "* `facebook/wav2vec2-base-960h`\n",
    "\n",
    "> Cukup 2–3 kandidat di Step-0. Pemenang dibawa ke eksperimen penuh.\n",
    "\n",
    "---\n",
    "\n",
    "## Konfigurasi Awal (konstan untuk semua kandidat)\n",
    "\n",
    "* **Audio**: 16 kHz, mono (resample dulu agar seragam).\n",
    "* **Pooling**: **mean pooling** di atas `last_hidden_state` (nanti bisa tambah **stats pooling** = mean ⨁ std).\n",
    "* **Head**: **Ridge** (per-trait).\n",
    "* **Prediksi**: **clip [0,1]** (bounded target).\n",
    "* **Fold**: pakai **predefined** 5-fold (speaker-independent).\n",
    "\n",
    "---\n",
    "\n",
    "## Metrik & Baseline\n",
    "\n",
    "* **Primary**: **MAE** per-trait + **rata-rata MAE** 5 trait.\n",
    "* **Tambahan**: RMSE, R² (opsional di Step-0).\n",
    "* **Null baseline**: menebak **mean** label di train fold → **MAE_null** (patokan minimum).\n",
    "\n",
    "---\n",
    "\n",
    "## Acceptance Criteria (Keputusan Step-0)\n",
    "\n",
    "* Pemenang = backbone dengan **MAE rata-rata terendah** dan **lift** (MAE_null − MAE_model) **positif** di semua trait.\n",
    "* Jika beda tipis (≤ ~0.002–0.003 MAE), utamakan **yang lebih cepat** (detik/klip) dan **memori lebih hemat**.\n",
    "* (Opsional) Uji cepat **stats pooling** pada 2 kandidat teratas: pilih jika memberi **+1–3%** relatif.\n",
    "\n",
    "---\n",
    "\n",
    "## Alur Step-0 (Bake-off)\n",
    "\n",
    "1. **Subset pilot (±20%)**: sampling proporsional per-fold untuk percepat uji.\n",
    "2. **Ekstraksi embedding** (frozen) untuk tiap backbone → **cache Parquet**\n",
    "   `ssl_emb_<backbone>_mean_pilot.parquet` (kolom: `person_id, fold, clip_id, …, emb_0..emb_D-1`).\n",
    "3. **Train cepat**: Ridge per-trait dengan split yang sama → hitung **MAE** & **MAE_null**.\n",
    "4. **Catat waktu ekstraksi** (detik/klip) → efisiensi.\n",
    "5. **Bandingkan & putuskan pemenang** → lanjut ke ekstraksi **full dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "## Struktur Output yang Disarankan\n",
    "\n",
    "```\n",
    "/emb/\n",
    "  ssl_emb_<backbone>_mean_pilot.parquet\n",
    "/results/\n",
    "  results_frozen_<backbone>_mean_pilot.csv\n",
    "  leaderboard_step0.csv                # rangkuman per backbone\n",
    "```\n",
    "\n",
    "**Kolom hasil yang ideal (per backbone):**\n",
    "`backbone, emb_dim, sec_per_clip, mae_avg, lift_abs, lift_rel_%`\n",
    "\n",
    "---\n",
    "\n",
    "## Reproducibility Checklist\n",
    "\n",
    "* Set **seed** (numpy/torch) + **log versi paket** (transformers/torch/sklearn).\n",
    "* Gunakan **config** per run (JSON kecil: backbone, pooling, head, grid, seed, fold).\n",
    "* **Cache embedding sekali**, **reuse** untuk semua percobaan head.\n",
    "\n",
    "---\n",
    "\n",
    "## Risiko & Mitigasi\n",
    "\n",
    "* **OOM** saat ekstraksi → kecilkan batch / pakai **window 5s stride 2–3s**, lalu **rata-rata** embedding window.\n",
    "* **Prediksi keluar [0,1]** → selalu **clip** (atau pakai transform logit lewat `TransformedTargetRegressor`).\n",
    "* **Beda performa tipis** → cobain **stats pooling** dan pilih yang **lebih efisien**.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps (setelah Step-0)\n",
    "\n",
    "1. Ekstraksi **full dataset** dengan backbone pemenang (**mean pooling**).\n",
    "2. Coba **stats pooling** di pemenang untuk cek gains.\n",
    "3. Train **Ridge** & **MLP kecil** (multi-output) → pilih setup terbaik untuk **S2 (fine-tune)** atau **fusion**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python : 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "torch  : 2.6.0+cu124\n",
      "transformers: 4.53.3\n",
      "huggingface_hub: 1.0.0.rc2\n",
      "httpx : 0.28.1\n",
      "tokenizers: 0.21.2\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, pkgutil\n",
    "def _v(mod): \n",
    "    try:\n",
    "        m=__import__(mod); return getattr(m,\"__version__\", \"unknown\")\n",
    "    except: \n",
    "        return \"not-installed\"\n",
    "\n",
    "print(\"python :\", sys.version)\n",
    "print(\"torch  :\", torch.__version__)\n",
    "print(\"transformers:\", _v(\"transformers\"))\n",
    "print(\"huggingface_hub:\", _v(\"huggingface_hub\"))\n",
    "print(\"httpx :\", _v(\"httpx\"))\n",
    "print(\"tokenizers:\", _v(\"tokenizers\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b44df7",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 4)\n",
    "- Audit versi Python/Torch/Transformers/HF Hub agar environment sesuai untuk inferensi backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c8e9315a8748d9a92303c517650227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF OK → /root/.cache/huggingface/hub/models--microsoft--wavlm-base-plus/snapshots/4c66d4806a428f2e922ccfa1a962776e232d487b/config.json\n",
      "Audio sample: /kaggle/input/traningv1/wav/J4GQm9j0JZ0.003.wav | exists: True\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# 1) koneksi & izin akses model (download file kecil saja)\n",
    "repo = \"microsoft/wavlm-base-plus\"\n",
    "cfg_path = hf_hub_download(repo_id=repo, filename=\"config.json\")\n",
    "print(\"HF OK →\", cfg_path)\n",
    "\n",
    "# 2) ambil 1 path audio dari CSV kamu\n",
    "base = \"/kaggle/input/traningv1\"\n",
    "df0 = pd.read_csv(f\"{base}/fi_v2_meta.with_feats.clean.folds.csv\")\n",
    "audio_col = \"wav_path\" if \"wav_path\" in df0.columns else \"relpath\"\n",
    "one_path = os.path.join(base, df0[audio_col].iloc[0].lstrip(\"./\"))\n",
    "print(\"Audio sample:\", one_path, \"| exists:\", os.path.exists(one_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f9bed",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 5)\n",
    "- Tes koneksi HuggingFace Hub lewat unduh config.json dan verifikasi ada contoh path audio lokal yang valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f2e7fc268a47f48aeab15306997f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2723ea2a74c42c9a64b81da46f1228f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8772a65c09f248fba1ea1d58fe8d5e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879822450a6f4d798076f721f2430723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local snapshot dir: /kaggle/working/hf_cache/models--microsoft--wavlm-base-plus/snapshots/4c66d4806a428f2e922ccfa1a962776e232d487b\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# taruh cache di working untuk menghindari konflik\n",
    "HF_CACHE = \"/kaggle/working/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE\n",
    "os.environ[\"HF_HUB_CACHE\"] = HF_CACHE\n",
    "pathlib.Path(HF_CACHE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BACKBONE_ID = \"microsoft/wavlm-base-plus\"   # bisa ganti: \"facebook/wav2vec2-base-960h\" / \"facebook/hubert-base-ls960\"\n",
    "\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=BACKBONE_ID,\n",
    "    cache_dir=HF_CACHE,\n",
    "    allow_patterns=[\n",
    "        \"config.json\",\n",
    "        \"preprocessor_config.json\", \"feature_extractor.json\",\n",
    "        \"pytorch_model*.bin\", \"model.safetensors\", \"*.json\"\n",
    "    ],\n",
    ")\n",
    "print(\"Local snapshot dir:\", local_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928493b1",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 6)\n",
    "- Mengunduh snapshot backbone pilihan ke cache lokal (/kaggle/working/hf_cache) agar ekstraksi embedding offline dan reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-11-12 05:56:12.430220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762926972.627476      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762926972.678893      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (768,) | first 5 dims: [-0.0448 -0.0012 -0.0368  0.0174  0.0229]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd, numpy as np, librosa, torch\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "# ambil 1 file audio dari CSV kamu\n",
    "BASE = \"/kaggle/input/traningv1\"\n",
    "df0  = pd.read_csv(f\"{BASE}/fi_v2_meta.with_feats.clean.folds.csv\")\n",
    "audio_col = \"wav_path\" if \"wav_path\" in df0.columns else \"relpath\"\n",
    "one_path  = os.path.join(BASE, df0[audio_col].iloc[0].lstrip(\"./\"))\n",
    "\n",
    "SR = 16000\n",
    "y, _ = librosa.load(one_path, sr=SR, mono=True)\n",
    "\n",
    "# >>> perhatikan: load dari local_dir + local_files_only=True <<<\n",
    "fe    = AutoFeatureExtractor.from_pretrained(local_dir, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(local_dir, local_files_only=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inp = fe(y, sampling_rate=SR, return_tensors=\"pt\")\n",
    "    inp = {k: v.to(device) for k, v in inp.items()}\n",
    "    hs  = model(**inp).last_hidden_state     # [1, T, D]\n",
    "    emb = hs.mean(dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "print(\"Embedding shape:\", emb.shape, \"| first 5 dims:\", np.round(emb[:5], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc907f",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 7)\n",
    "- Memuat satu WAV, menyiapkan extractor/model lokal, lalu menghitung embedding mean sebagai sanity check; menampilkan bentuk vektor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bf3dd2c7f146c1b582d6e0e8b33789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0112f8d2c41440fa947ebd7cd5a8be51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561697a44e0a4c72b94e0a6cb38e1cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da576051f1348ae88e328daf37c4146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d44b2d0e7844d79b1ccc0f43d1e25fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204b8200e23445428981263ce0b803a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430eb612aead439fbd4e7e77e96fff86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5ad538679a4f9399e4ed49336dac4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "feature_extractor_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5ba5ced1cb4a90972f841933bb23e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102d7ff159474fe0b955b286c4071d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def0c4743e3b4e84ac8e7da9dbab5727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55d96e19bc9449db57cda80c80c8b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5befa8a1e32e4e3dae2f3bb2a80f3112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e33d79e3b94471a6270d7425ca71d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'wavlm-base-plus': '/kaggle/working/hf_cache/models--microsoft--wavlm-base-plus/snapshots/4c66d4806a428f2e922ccfa1a962776e232d487b',\n",
       " 'hubert-base': '/kaggle/working/hf_cache/models--facebook--hubert-base-ls960/snapshots/dba3bb02fda4248b6e082697eee756de8fe8aa8a',\n",
       " 'wav2vec2-base': '/kaggle/working/hf_cache/models--facebook--wav2vec2-base-960h/snapshots/22aad52d435eb6dbaf354bdad9b0da84ce7d6156'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os, pathlib\n",
    "\n",
    "HF_CACHE = \"/kaggle/working/hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE\n",
    "os.environ[\"HF_HUB_CACHE\"] = HF_CACHE\n",
    "pathlib.Path(HF_CACHE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BACKBONES = {\n",
    "    \"wavlm-base-plus\": \"microsoft/wavlm-base-plus\",\n",
    "    \"hubert-base\": \"facebook/hubert-base-ls960\",\n",
    "    \"wav2vec2-base\": \"facebook/wav2vec2-base-960h\",\n",
    "}\n",
    "LOCAL_DIRS = {}\n",
    "for name, repo in BACKBONES.items():\n",
    "    LOCAL_DIRS[name] = snapshot_download(\n",
    "        repo_id=repo, cache_dir=HF_CACHE,\n",
    "        allow_patterns=[\"config.json\",\"preprocessor_config.json\",\"feature_extractor.json\",\n",
    "                        \"pytorch_model*.bin\",\"model.safetensors\",\"*.json\"]\n",
    "    )\n",
    "LOCAL_DIRS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111baee",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 8)\n",
    "- Mengunduh tiga backbone kandidat (wavlm, hubert, wav2vec2) ke cache dan menyimpan path di LOCAL_DIRS untuk uji banding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39/919925803.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(frac=0.2, random_state=42))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1085, 39),\n",
       " fold\n",
       " 0    219\n",
       " 1    203\n",
       " 2    224\n",
       " 3    221\n",
       " 4    218\n",
       " Name: count, dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "BASE = \"/kaggle/input/traningv1\"\n",
    "df = pd.read_csv(f\"{BASE}/fi_v2_meta.with_feats.clean.folds.csv\")\n",
    "\n",
    "audio_col = \"wav_path\" if \"wav_path\" in df.columns else \"relpath\"\n",
    "df[\"audio_path\"] = df[audio_col].apply(lambda p: os.path.join(BASE, p.lstrip(\"./\")))\n",
    "\n",
    "target_cols = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "\n",
    "df_pilot = (df.groupby(\"fold\", group_keys=False)\n",
    "              .apply(lambda g: g.sample(frac=0.2, random_state=42))\n",
    "              .reset_index(drop=True))\n",
    "df_pilot.shape, df_pilot[\"fold\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b546b1",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 9)\n",
    "- Menambahkan kolom audio_path absolut dan membuat subset pilot 20% per fold yang akan dipakai bake-off cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, time, pandas as pd\n",
    "import librosa\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "SR = 16000\n",
    "\n",
    "def load_16k(path):  # simple loader\n",
    "    y, _ = librosa.load(path, sr=SR, mono=True)\n",
    "    return y\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embed_mean_local_batch(paths, local_dir, batch_size=4):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    fe = AutoFeatureExtractor.from_pretrained(local_dir, local_files_only=True)\n",
    "    model = AutoModel.from_pretrained(local_dir, local_files_only=True).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embs, t0 = [], time.time()\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        waves = [load_16k(p) for p in paths[i:i+batch_size]]\n",
    "        inputs = fe(waves, sampling_rate=SR, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        hs = model(**inputs).last_hidden_state   # [B, T, D]\n",
    "        embs.append(hs.mean(dim=1).cpu().numpy())  # mean over time\n",
    "    embs = np.vstack(embs)\n",
    "    spc = (time.time() - t0) / max(len(paths), 1)\n",
    "    return embs, spc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f18735",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 10)\n",
    "- Mendefinisikan fungsi ekstraksi embedding batch (mean pooling) yang mengembalikan waktu rata-rata per klip sebagai indikator efisiensi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39/1766703514.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(frac=0.2, random_state=42))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1085, 41),\n",
       " fold\n",
       " 0    219\n",
       " 1    203\n",
       " 2    224\n",
       " 3    221\n",
       " 4    218\n",
       " Name: count, dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- NORMALISASI ID (buat person_id) ---\n",
    "import os, pandas as pd, numpy as np\n",
    "\n",
    "BASE = \"/kaggle/input/traningv1\"\n",
    "df = pd.read_csv(f\"{BASE}/fi_v2_meta.with_feats.clean.folds.csv\")\n",
    "\n",
    "audio_col = \"wav_path\" if \"wav_path\" in df.columns else \"relpath\"\n",
    "df[\"audio_path\"] = df[audio_col].apply(lambda p: os.path.join(BASE, p.lstrip(\"./\")))\n",
    "\n",
    "# if person_id missing → derive from group_id/raw_id/filename\n",
    "if \"person_id\" not in df.columns:\n",
    "    if \"group_id\" in df.columns:\n",
    "        df[\"person_id\"] = df[\"group_id\"]\n",
    "    elif \"raw_id\" in df.columns:\n",
    "        # contoh raw_id:  J4QGm9j0JZ0.003.mp4  → ambil sebelum segmen\n",
    "        df[\"person_id\"] = df[\"raw_id\"].str.split(\".\").str[0]\n",
    "    else:\n",
    "        df[\"person_id\"] = df[\"audio_path\"].apply(lambda p: os.path.basename(p).split(\".\")[0])\n",
    "\n",
    "# (opsional) clip_id untuk audit\n",
    "df[\"clip_id\"] = df[audio_col].apply(lambda p: os.path.basename(p))\n",
    "\n",
    "target_cols = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "assert all(c in df.columns for c in target_cols), \"Cek lagi target_cols di CSV.\"\n",
    "\n",
    "# --- PILOT 20% PER FOLD ---\n",
    "df_pilot = (df.groupby(\"fold\", group_keys=False)\n",
    "              .apply(lambda g: g.sample(frac=0.2, random_state=42))\n",
    "              .reset_index(drop=True))\n",
    "df_pilot.shape, df_pilot[\"fold\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969cddd",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 11)\n",
    "- Menormalkan person_id/clip_id jika belum ada dan menyiapkan ulang subset pilot dengan kolom audit agar konsisten di semua percobaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at /kaggle/working/hf_cache/models--facebook--wav2vec2-base-960h/snapshots/22aad52d435eb6dbaf354bdad9b0da84ce7d6156 and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>sec_per_clip</th>\n",
       "      <th>parquet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wavlm-base-plus</td>\n",
       "      <td>768</td>\n",
       "      <td>0.112399</td>\n",
       "      <td>/kaggle/working/emb/ssl_emb_wavlm-base-plus_me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hubert-base</td>\n",
       "      <td>768</td>\n",
       "      <td>0.094765</td>\n",
       "      <td>/kaggle/working/emb/ssl_emb_hubert-base_mean_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wav2vec2-base</td>\n",
       "      <td>768</td>\n",
       "      <td>0.095511</td>\n",
       "      <td>/kaggle/working/emb/ssl_emb_wav2vec2-base_mean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          backbone  emb_dim  sec_per_clip  \\\n",
       "0  wavlm-base-plus      768      0.112399   \n",
       "1      hubert-base      768      0.094765   \n",
       "2    wav2vec2-base      768      0.095511   \n",
       "\n",
       "                                             parquet  \n",
       "0  /kaggle/working/emb/ssl_emb_wavlm-base-plus_me...  \n",
       "1  /kaggle/working/emb/ssl_emb_hubert-base_mean_p...  \n",
       "2  /kaggle/working/emb/ssl_emb_wav2vec2-base_mean...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"/kaggle/working/emb\", exist_ok=True)\n",
    "\n",
    "pilot_meta = []\n",
    "for name, local_dir in LOCAL_DIRS.items():\n",
    "    embs, sec_per_clip = extract_embed_mean_local_batch(\n",
    "        df_pilot[\"audio_path\"].tolist(), local_dir, batch_size=4\n",
    "    )\n",
    "    D = embs.shape[1]\n",
    "    out_path = f\"/kaggle/working/emb/ssl_emb_{name}_mean_pilot.parquet\"\n",
    "\n",
    "    cols = [f\"emb_{i}\" for i in range(D)]\n",
    "    emb_df = pd.concat([\n",
    "        df_pilot[[\"person_id\",\"fold\",\"clip_id\",\"audio_path\"] + target_cols].reset_index(drop=True),\n",
    "        pd.DataFrame(embs, columns=cols)\n",
    "    ], axis=1)\n",
    "    emb_df.to_parquet(out_path, index=False)\n",
    "\n",
    "    pilot_meta.append({\"backbone\": name, \"emb_dim\": D, \"sec_per_clip\": sec_per_clip, \"parquet\": out_path})\n",
    "\n",
    "pd.DataFrame(pilot_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38e056",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 12)\n",
    "- Mengekstrak embedding pilot untuk setiap backbone, menyimpannya ke Parquet, dan mencatat dimensi embedding serta detik/klip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SCORING BACKBONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.23398e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56477e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.43854e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.7543e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.44159e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.23398e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56477e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.43854e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.7543e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.44159e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.23398e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56477e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.43854e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.7543e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.44159e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.23398e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56477e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.43854e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.7543e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.44159e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.23398e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.56477e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.43854e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.7543e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=5.44159e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>sec_per_clip</th>\n",
       "      <th>mae_avg</th>\n",
       "      <th>lift_abs</th>\n",
       "      <th>lift_rel_%</th>\n",
       "      <th>pertrait_csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hubert-base</td>\n",
       "      <td>768</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>3.77</td>\n",
       "      <td>/kaggle/working/results_step0_hubert-base_pert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wavlm-base-plus</td>\n",
       "      <td>768</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>3.35</td>\n",
       "      <td>/kaggle/working/results_step0_wavlm-base-plus_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wav2vec2-base</td>\n",
       "      <td>768</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>/kaggle/working/results_step0_wav2vec2-base_pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          backbone  emb_dim  sec_per_clip  mae_avg  lift_abs  lift_rel_%  \\\n",
       "0      hubert-base      768        0.0948   0.1125    0.0047        3.77   \n",
       "1  wavlm-base-plus      768        0.1124   0.1130    0.0042        3.35   \n",
       "2    wav2vec2-base      768        0.0955   0.1183   -0.0011       -1.12   \n",
       "\n",
       "                                        pertrait_csv  \n",
       "0  /kaggle/working/results_step0_hubert-base_pert...  \n",
       "1  /kaggle/working/results_step0_wavlm-base-plus_...  \n",
       "2  /kaggle/working/results_step0_wav2vec2-base_pe...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# pastikan sudah ada\n",
    "# pilot_meta -> list of {\"backbone\",\"emb_dim\",\"sec_per_clip\",\"parquet\"}\n",
    "# target_cols -> [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "\n",
    "def eval_ridge_cv(parquet_path, target_cols, fold_col=\"fold\"):\n",
    "    dat = pd.read_parquet(parquet_path)\n",
    "    feat_cols = [c for c in dat.columns if c.startswith(\"emb_\")]\n",
    "    folds = dat[fold_col].values\n",
    "    uniq = sorted(np.unique(folds))\n",
    "    splits = [(np.where(folds != f)[0], np.where(folds == f)[0]) for f in uniq]\n",
    "\n",
    "    rows = []\n",
    "    for t in target_cols:\n",
    "        y = dat[t].values\n",
    "\n",
    "        # null baseline\n",
    "        null_mae = np.mean([\n",
    "            mean_absolute_error(y[val], np.full(len(val), y[trn].mean()))\n",
    "            for trn, val in splits\n",
    "        ])\n",
    "\n",
    "        # Ridge (grid kecil)\n",
    "        best_mae, best_alpha = 1e9, None\n",
    "        for a in [0.1, 0.3, 1, 3, 10, 30, 100]:\n",
    "            fold_mae = []\n",
    "            for trn, val in splits:\n",
    "                pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                                 (\"ridge\", Ridge(alpha=a))])\n",
    "                pipe.fit(dat.iloc[trn][feat_cols], y[trn])\n",
    "                pred = pipe.predict(dat.iloc[val][feat_cols]).clip(0, 1)\n",
    "                fold_mae.append(mean_absolute_error(y[val], pred))\n",
    "            mae = float(np.mean(fold_mae))\n",
    "            if mae < best_mae:\n",
    "                best_mae, best_alpha = mae, a\n",
    "\n",
    "        rows.append({\"trait\": t, \"mae_model\": best_mae, \"mae_null\": null_mae, \"best_alpha\": best_alpha})\n",
    "\n",
    "    dfres = pd.DataFrame(rows)\n",
    "    summary = {\n",
    "        \"mae_avg\": dfres[\"mae_model\"].mean(),\n",
    "        \"lift_abs\": (dfres[\"mae_null\"] - dfres[\"mae_model\"]).mean(),\n",
    "        \"lift_rel_%\": (100 * ((dfres[\"mae_null\"] - dfres[\"mae_model\"]) / dfres[\"mae_null\"])).mean(),\n",
    "    }\n",
    "    return dfres, summary\n",
    "\n",
    "leader = []\n",
    "for m in pilot_meta:\n",
    "    dfres, summ = eval_ridge_cv(m[\"parquet\"], target_cols)\n",
    "    dfres.to_csv(f\"/kaggle/working/results_step0_{m['backbone']}_pertrait.csv\", index=False)\n",
    "    leader.append({\n",
    "        \"backbone\": m[\"backbone\"],\n",
    "        \"emb_dim\": m[\"emb_dim\"],\n",
    "        \"sec_per_clip\": round(m[\"sec_per_clip\"], 4),\n",
    "        \"mae_avg\": round(summ[\"mae_avg\"], 4),\n",
    "        \"lift_abs\": round(summ[\"lift_abs\"], 4),\n",
    "        \"lift_rel_%\": round(summ[\"lift_rel_%\"], 2),\n",
    "        \"pertrait_csv\": f\"/kaggle/working/results_step0_{m['backbone']}_pertrait.csv\",\n",
    "    })\n",
    "\n",
    "leader_df = pd.DataFrame(leader).sort_values([\"mae_avg\", \"sec_per_clip\"]).reset_index(drop=True)\n",
    "leader_df.to_csv(\"/kaggle/working/results_step0_leaderboard.csv\", index=False)\n",
    "leader_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eea07e",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 13)\n",
    "- Mengevaluasi backbone pilot memakai Ridge CV per fold, menghitung MAE dan lift vs baseline, lalu membuat leaderboard results_step0_leaderboard.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keputusan Step-0: pakai hubert-base sebagai pemenang untuk lanjut full run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1) Ekstrak FULL embedding (hubert-base)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec/clip ~ 0.0978 | emb_dim = 768 | rows = 5425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/emb/ssl_emb_hubert-base_mean_full.parquet',\n",
       " '/kaggle/working/emb/ssl_emb_hubert-base_stats_full.parquet')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Setup meta & kolom ===\n",
    "import os, pandas as pd, numpy as np, time, librosa, torch\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "BASE = \"/kaggle/input/traningv1\"\n",
    "df = pd.read_csv(f\"{BASE}/fi_v2_meta.with_feats.clean.folds.csv\")\n",
    "\n",
    "audio_col = \"wav_path\" if \"wav_path\" in df.columns else \"relpath\"\n",
    "df[\"audio_path\"] = df[audio_col].apply(lambda p: os.path.join(BASE, p.lstrip(\"./\")))\n",
    "if \"person_id\" not in df.columns:\n",
    "    df[\"person_id\"] = df.get(\"group_id\", df[audio_col].apply(lambda p: os.path.basename(p).split(\".\")[0]))\n",
    "df[\"clip_id\"] = df[audio_col].apply(lambda p: os.path.basename(p))\n",
    "\n",
    "target_cols = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "assert all(c in df.columns for c in target_cols)\n",
    "\n",
    "# === Ekstraksi sekali untuk 2 pooling: mean & stats ===\n",
    "SR = 16000\n",
    "def load_16k(p): \n",
    "    y, _ = librosa.load(p, sr=SR, mono=True); return y\n",
    "\n",
    "def extract_full_two_poolings(local_dir, paths, batch_size=8):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    fe = AutoFeatureExtractor.from_pretrained(local_dir, local_files_only=True)\n",
    "    model = AutoModel.from_pretrained(local_dir, local_files_only=True).to(device).eval()\n",
    "\n",
    "    mean_chunks, stats_chunks = [], []\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(paths), batch_size):\n",
    "            waves = [load_16k(p) for p in paths[i:i+batch_size]]\n",
    "            inputs = fe(waves, sampling_rate=SR, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "            hs = model(**inputs).last_hidden_state   # [B,T,D]\n",
    "            mean = hs.mean(dim=1)                    # [B,D]\n",
    "            std  = hs.std(dim=1)                     # [B,D]\n",
    "            mean_chunks.append(mean.cpu().numpy())\n",
    "            stats_chunks.append(torch.cat([mean, std], dim=1).cpu().numpy())\n",
    "    spc = (time.time()-t0)/max(len(paths),1)\n",
    "    mean_mat  = np.vstack(mean_chunks)\n",
    "    stats_mat = np.vstack(stats_chunks)\n",
    "    return mean_mat, stats_mat, spc, mean_mat.shape[1]\n",
    "\n",
    "# Path lokal model pemenang\n",
    "local_dir = LOCAL_DIRS[\"hubert-base\"]  # dari langkah sebelumnya (snapshot_download)\n",
    "\n",
    "mean_mat, stats_mat, sec_per_clip, D = extract_full_two_poolings(local_dir, df[\"audio_path\"].tolist(), batch_size=8)\n",
    "print(\"sec/clip ~\", round(sec_per_clip, 4), \"| emb_dim =\", D, \"| rows =\", len(df))\n",
    "\n",
    "os.makedirs(\"/kaggle/working/emb\", exist_ok=True)\n",
    "\n",
    "# Simpan Parquet: mean pooling\n",
    "cols_mean = [f\"emb_{i}\" for i in range(D)]\n",
    "emb_mean_df = pd.concat([\n",
    "    df[[\"person_id\",\"fold\",\"clip_id\",\"audio_path\"] + target_cols].reset_index(drop=True),\n",
    "    pd.DataFrame(mean_mat, columns=cols_mean)\n",
    "], axis=1)\n",
    "PQ_MEAN = \"/kaggle/working/emb/ssl_emb_hubert-base_mean_full.parquet\"\n",
    "emb_mean_df.to_parquet(PQ_MEAN, index=False)\n",
    "\n",
    "# Simpan Parquet: stats pooling (mean ⨁ std)\n",
    "cols_stats = [f\"emb_{i}\" for i in range(2*D)]\n",
    "emb_stats_df = pd.concat([\n",
    "    df[[\"person_id\",\"fold\",\"clip_id\",\"audio_path\"] + target_cols].reset_index(drop=True),\n",
    "    pd.DataFrame(stats_mat, columns=cols_stats)\n",
    "], axis=1)\n",
    "PQ_STATS = \"/kaggle/working/emb/ssl_emb_hubert-base_stats_full.parquet\"\n",
    "emb_stats_df.to_parquet(PQ_STATS, index=False)\n",
    "\n",
    "PQ_MEAN, PQ_STATS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec33d0d",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 14)\n",
    "- Menjalankan ekstraksi penuh untuk backbone pemenang (hubert-base) dengan pooling mean dan stats; menyimpan Parquet PQ_MEAN & PQ_STATS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluasi klasifikasi (mean vs stats) — macro-F1 & balanced accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PROGRESS EVAL (versi detail) ---\n",
    "import numpy as np, pandas as pd, time, json, os, warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "target_cols = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "\n",
    "def eval_cls_tertile_from_parquet_progress(\n",
    "    parquet_path,\n",
    "    target_cols,\n",
    "    fold_col=\"fold\",\n",
    "    n_classes=3,\n",
    "    save_prefix=\"/kaggle/working/_progress\",\n",
    "    solver=\"saga\",\n",
    "    max_iter=2000,\n",
    "    random_state=42,\n",
    "    verbose=3,  # 0: sunyi, 1: per-trait saja, 2: +running mean, 3: +log per-fold\n",
    "):\n",
    "    os.makedirs(save_prefix, exist_ok=True)\n",
    "    base = os.path.basename(parquet_path)\n",
    "    log_path = f\"{save_prefix}/log_{base}.txt\"\n",
    "\n",
    "    def _log(msg):\n",
    "        if verbose >= 3:\n",
    "            tqdm.write(msg)\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "\n",
    "    dat = pd.read_parquet(parquet_path)\n",
    "    feat_cols = [c for c in dat.columns if c.startswith(\"emb_\")]\n",
    "    X = dat[feat_cols].values\n",
    "    folds = dat[fold_col].values\n",
    "    uniq = sorted(np.unique(folds))\n",
    "\n",
    "    rows, t_global0 = [], time.time()\n",
    "    trait_iter = tqdm(target_cols, desc=f\"Traits ({base})\", disable=(verbose==0))\n",
    "    for t in trait_iter:\n",
    "        # wadah metrik per-fold (disimpan bertahap)\n",
    "        fold_rows = []\n",
    "\n",
    "        y_cont = dat[t].values\n",
    "        f1s, bals, f1s_null, bals_null = [], [], [], []\n",
    "        t0 = time.time()\n",
    "\n",
    "        # progress bar per-fold\n",
    "        fold_iter = tqdm(uniq, leave=False, desc=f\"{t}: folds\", disable=(verbose<2))\n",
    "        for f in fold_iter:\n",
    "            trn = np.where(folds != f)[0]\n",
    "            val = np.where(folds == f)[0]\n",
    "\n",
    "            # ambang anti-leak dari TRAIN\n",
    "            if n_classes == 3:\n",
    "                bins = np.quantile(y_cont[trn], [1/3, 2/3])\n",
    "            else:\n",
    "                bins = [np.quantile(y_cont[trn], 0.5)]\n",
    "            y_trn = np.digitize(y_cont[trn], bins, right=True)\n",
    "            y_val = np.digitize(y_cont[val], bins, right=True)\n",
    "\n",
    "            # model\n",
    "            pipe = Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"logreg\", LogisticRegression(\n",
    "                    max_iter=max_iter, solver=solver,\n",
    "                    multi_class=\"multinomial\", class_weight=\"balanced\",\n",
    "                    n_jobs=(-1 if solver in {\"saga\",\"sag\"} else None),\n",
    "                    random_state=random_state,\n",
    "                ))\n",
    "            ])\n",
    "\n",
    "            t_fit0 = time.time()\n",
    "            pipe.fit(X[trn], y_trn)\n",
    "            y_hat = pipe.predict(X[val])\n",
    "            sec_fit = time.time() - t_fit0\n",
    "\n",
    "            # baseline mayoritas\n",
    "            maj = np.bincount(y_trn).argmax()\n",
    "            y_null = np.full_like(y_val, maj)\n",
    "\n",
    "            # metrik\n",
    "            f1  = f1_score(y_val, y_hat, average=\"macro\")\n",
    "            bal = balanced_accuracy_score(y_val, y_hat)\n",
    "            f1n = f1_score(y_val, y_null, average=\"macro\")\n",
    "            baln= balanced_accuracy_score(y_val, y_null)\n",
    "\n",
    "            f1s.append(f1); bals.append(bal); f1s_null.append(f1n); bals_null.append(baln)\n",
    "\n",
    "            # log detail per-fold\n",
    "            dist = dict(zip(*np.unique(y_trn, return_counts=True)))\n",
    "            _log(\n",
    "                f\"[{t}] fold={f} | trn={len(trn)} val={len(val)} | bins={np.round(bins,4).tolist()} \"\n",
    "                f\"| train_dist={dist} | sec_fit={sec_fit:.2f} \"\n",
    "                f\"| F1={f1:.3f} Bal={bal:.3f} | nullF1={f1n:.3f} nullBal={baln:.3f}\"\n",
    "            )\n",
    "\n",
    "            fold_rows.append({\n",
    "                \"trait\": t, \"fold\": int(f),\n",
    "                \"train_n\": int(len(trn)), \"val_n\": int(len(val)),\n",
    "                \"bin_lo\": float(bins[0]), \"bin_hi\": float(bins[-1]),\n",
    "                \"train_dist_0\": int(dist.get(0,0)),\n",
    "                \"train_dist_1\": int(dist.get(1,0)),\n",
    "                \"train_dist_2\": int(dist.get(2,0)),\n",
    "                \"sec_fit\": round(sec_fit,3),\n",
    "                \"f1\": float(f1), \"bal\": float(bal),\n",
    "                \"f1_null\": float(f1n), \"bal_null\": float(baln),\n",
    "            })\n",
    "\n",
    "            if verbose >= 2:\n",
    "                fold_iter.set_postfix({\n",
    "                    \"F1(run)\": f\"{np.mean(f1s):.3f}\",\n",
    "                    \"Bal(run)\": f\"{np.mean(bals):.3f}\"\n",
    "                })\n",
    "\n",
    "        # simpan metrik per-fold untuk trait ini\n",
    "        pd.DataFrame(fold_rows).to_csv(\n",
    "            f\"{save_prefix}/fold_metrics_{base}_{t}.csv\", index=False\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"trait\": t,\n",
    "            \"f1_macro\": float(np.mean(f1s)),\n",
    "            \"bal_acc\":  float(np.mean(bals)),\n",
    "            \"f1_macro_null\": float(np.mean(f1s_null)),\n",
    "            \"bal_acc_null\":  float(np.mean(bals_null)),\n",
    "            \"time_s\": round(time.time() - t0, 2),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        # partial CSV per-trait (rolling)\n",
    "        pd.DataFrame(rows).to_csv(f\"{save_prefix}/partial_{base}.csv\", index=False)\n",
    "\n",
    "        if verbose >= 1:\n",
    "            trait_iter.set_postfix({\n",
    "                \"last_trait\": t,\n",
    "                \"F1\": f\"{row['f1_macro']:.3f}\",\n",
    "                \"Bal\": f\"{row['bal_acc']:.3f}\"\n",
    "            })\n",
    "\n",
    "    dfres = pd.DataFrame(rows)\n",
    "    summary = {\n",
    "        \"f1_macro_avg\": dfres[\"f1_macro\"].mean(),\n",
    "        \"bal_acc_avg\":  dfres[\"bal_acc\"].mean(),\n",
    "        \"lift_f1_pct\":  100*(dfres[\"f1_macro\"].mean() - dfres[\"f1_macro_null\"].mean()),\n",
    "        \"time_total_s\": round(time.time() - t_global0, 2),\n",
    "        \"parquet\": parquet_path,\n",
    "        \"n_rows\": len(dat),\n",
    "        \"n_feats\": len(feat_cols),\n",
    "        \"solver\": solver,\n",
    "        \"max_iter\": max_iter,\n",
    "        \"log_path\": log_path,\n",
    "        \"partial_csv\": f\"{save_prefix}/partial_{base}.csv\",\n",
    "    }\n",
    "    with open(f\"{save_prefix}/summary_{base}.json\",\"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    return dfres, summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21778d95",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 15)\n",
    "- Mendefinisikan evaluator klasifikasi tertile (logreg SAGA) dengan logging progress/partial per fold agar proses panjang bisa dilanjutkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af68421ace564178a4fff601ff7dcd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Traits (ssl_emb_hubert-base_mean_full.parquet):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "extraversion: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extraversion] fold=0 | trn=4332 val=1093 | bins=[0.4206, 0.5514] | train_dist={0: 1538, 1: 1357, 2: 1437} | sec_fit=151.31 | F1=0.469 Bal=0.471 | nullF1=0.172 nullBal=0.333\n",
      "[extraversion] fold=1 | trn=4408 val=1017 | bins=[0.4206, 0.5514] | train_dist={0: 1556, 1: 1391, 2: 1461} | sec_fit=153.63 | F1=0.494 Bal=0.494 | nullF1=0.175 nullBal=0.333\n",
      "[extraversion] fold=2 | trn=4304 val=1121 | bins=[0.4206, 0.5607] | train_dist={0: 1531, 1: 1437, 2: 1336} | sec_fit=149.89 | F1=0.453 Bal=0.461 | nullF1=0.171 nullBal=0.333\n",
      "[extraversion] fold=3 | trn=4319 val=1106 | bins=[0.4206, 0.5514] | train_dist={0: 1516, 1: 1382, 2: 1421} | sec_fit=150.55 | F1=0.487 Bal=0.486 | nullF1=0.178 nullBal=0.333\n",
      "[extraversion] fold=4 | trn=4337 val=1088 | bins=[0.4206, 0.5514] | train_dist={0: 1531, 1: 1381, 2: 1425} | sec_fit=150.96 | F1=0.459 Bal=0.458 | nullF1=0.175 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "agreeableness: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[agreeableness] fold=0 | trn=4332 val=1093 | bins=[0.5055, 0.6154] | train_dist={0: 1504, 1: 1418, 2: 1410} | sec_fit=152.83 | F1=0.404 Bal=0.407 | nullF1=0.168 nullBal=0.333\n",
      "[agreeableness] fold=1 | trn=4408 val=1017 | bins=[0.5055, 0.6154] | train_dist={0: 1548, 1: 1446, 2: 1414} | sec_fit=155.68 | F1=0.403 Bal=0.404 | nullF1=0.161 nullBal=0.333\n",
      "[agreeableness] fold=2 | trn=4304 val=1121 | bins=[0.5055, 0.6154] | train_dist={0: 1468, 1: 1421, 2: 1415} | sec_fit=149.68 | F1=0.406 Bal=0.407 | nullF1=0.177 nullBal=0.333\n",
      "[agreeableness] fold=3 | trn=4319 val=1106 | bins=[0.5055, 0.6154] | train_dist={0: 1491, 1: 1439, 2: 1389} | sec_fit=150.32 | F1=0.428 Bal=0.431 | nullF1=0.171 nullBal=0.333\n",
      "[agreeableness] fold=4 | trn=4337 val=1088 | bins=[0.5055, 0.6154] | train_dist={0: 1481, 1: 1448, 2: 1408} | sec_fit=150.79 | F1=0.393 Bal=0.393 | nullF1=0.177 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conscientiousness: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[conscientiousness] fold=0 | trn=4332 val=1093 | bins=[0.466, 0.6019] | train_dist={0: 1516, 1: 1442, 2: 1374} | sec_fit=150.94 | F1=0.469 Bal=0.470 | nullF1=0.168 nullBal=0.333\n",
      "[conscientiousness] fold=1 | trn=4408 val=1017 | bins=[0.466, 0.6019] | train_dist={0: 1521, 1: 1454, 2: 1433} | sec_fit=153.29 | F1=0.452 Bal=0.451 | nullF1=0.176 nullBal=0.333\n",
      "[conscientiousness] fold=2 | trn=4304 val=1121 | bins=[0.466, 0.6019] | train_dist={0: 1503, 1: 1385, 2: 1416} | sec_fit=150.60 | F1=0.450 Bal=0.454 | nullF1=0.169 nullBal=0.333\n",
      "[conscientiousness] fold=3 | trn=4319 val=1106 | bins=[0.466, 0.6019] | train_dist={0: 1501, 1: 1409, 2: 1409} | sec_fit=150.88 | F1=0.501 Bal=0.507 | nullF1=0.172 nullBal=0.333\n",
      "[conscientiousness] fold=4 | trn=4337 val=1088 | bins=[0.466, 0.6019] | train_dist={0: 1499, 1: 1414, 2: 1424} | sec_fit=151.64 | F1=0.461 Bal=0.461 | nullF1=0.175 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neuroticism: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neuroticism] fold=0 | trn=4332 val=1093 | bins=[0.4688, 0.6042] | train_dist={0: 1508, 1: 1459, 2: 1365} | sec_fit=151.32 | F1=0.459 Bal=0.465 | nullF1=0.160 nullBal=0.333\n",
      "[neuroticism] fold=1 | trn=4408 val=1017 | bins=[0.4688, 0.6042] | train_dist={0: 1500, 1: 1515, 2: 1393} | sec_fit=154.23 | F1=0.463 Bal=0.463 | nullF1=0.164 nullBal=0.333\n",
      "[neuroticism] fold=2 | trn=4304 val=1121 | bins=[0.4688, 0.6042] | train_dist={0: 1448, 1: 1456, 2: 1400} | sec_fit=150.58 | F1=0.447 Bal=0.450 | nullF1=0.173 nullBal=0.333\n",
      "[neuroticism] fold=3 | trn=4319 val=1106 | bins=[0.4688, 0.6042] | train_dist={0: 1484, 1: 1482, 2: 1353} | sec_fit=151.05 | F1=0.509 Bal=0.513 | nullF1=0.167 nullBal=0.333\n",
      "[neuroticism] fold=4 | trn=4337 val=1088 | bins=[0.4688, 0.6042] | train_dist={0: 1472, 1: 1480, 2: 1385} | sec_fit=151.82 | F1=0.450 Bal=0.450 | nullF1=0.168 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openness: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[openness] fold=0 | trn=4332 val=1093 | bins=[0.5111, 0.6333] | train_dist={0: 1457, 1: 1454, 2: 1421} | sec_fit=151.57 | F1=0.485 Bal=0.489 | nullF1=0.161 nullBal=0.333\n",
      "[openness] fold=1 | trn=4408 val=1017 | bins=[0.5222, 0.6333] | train_dist={0: 1589, 1: 1350, 2: 1469} | sec_fit=154.26 | F1=0.464 Bal=0.464 | nullF1=0.176 nullBal=0.333\n",
      "[openness] fold=2 | trn=4304 val=1121 | bins=[0.5222, 0.6444] | train_dist={0: 1540, 1: 1437, 2: 1327} | sec_fit=149.68 | F1=0.472 Bal=0.475 | nullF1=0.180 nullBal=0.333\n",
      "[openness] fold=3 | trn=4319 val=1106 | bins=[0.5111, 0.6333] | train_dist={0: 1445, 1: 1448, 2: 1426} | sec_fit=150.57 | F1=0.497 Bal=0.497 | nullF1=0.166 nullBal=0.333\n",
      "[openness] fold=4 | trn=4337 val=1088 | bins=[0.5222, 0.6444] | train_dist={0: 1555, 1: 1439, 2: 1343} | sec_fit=150.99 | F1=0.451 Bal=0.453 | nullF1=0.179 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1028e7c876b64836a208216598b07f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Traits (ssl_emb_hubert-base_stats_full.parquet):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "extraversion: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extraversion] fold=0 | trn=4332 val=1093 | bins=[0.4206, 0.5514] | train_dist={0: 1538, 1: 1357, 2: 1437} | sec_fit=298.76 | F1=0.464 Bal=0.465 | nullF1=0.172 nullBal=0.333\n",
      "[extraversion] fold=1 | trn=4408 val=1017 | bins=[0.4206, 0.5514] | train_dist={0: 1556, 1: 1391, 2: 1461} | sec_fit=304.13 | F1=0.420 Bal=0.421 | nullF1=0.175 nullBal=0.333\n",
      "[extraversion] fold=2 | trn=4304 val=1121 | bins=[0.4206, 0.5607] | train_dist={0: 1531, 1: 1437, 2: 1336} | sec_fit=301.45 | F1=0.458 Bal=0.460 | nullF1=0.171 nullBal=0.333\n",
      "[extraversion] fold=3 | trn=4319 val=1106 | bins=[0.4206, 0.5514] | train_dist={0: 1516, 1: 1382, 2: 1421} | sec_fit=298.39 | F1=0.475 Bal=0.474 | nullF1=0.178 nullBal=0.333\n",
      "[extraversion] fold=4 | trn=4337 val=1088 | bins=[0.4206, 0.5514] | train_dist={0: 1531, 1: 1381, 2: 1425} | sec_fit=299.58 | F1=0.438 Bal=0.437 | nullF1=0.175 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "agreeableness: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[agreeableness] fold=0 | trn=4332 val=1093 | bins=[0.5055, 0.6154] | train_dist={0: 1504, 1: 1418, 2: 1410} | sec_fit=299.40 | F1=0.421 Bal=0.424 | nullF1=0.168 nullBal=0.333\n",
      "[agreeableness] fold=1 | trn=4408 val=1017 | bins=[0.5055, 0.6154] | train_dist={0: 1548, 1: 1446, 2: 1414} | sec_fit=304.55 | F1=0.393 Bal=0.393 | nullF1=0.161 nullBal=0.333\n",
      "[agreeableness] fold=2 | trn=4304 val=1121 | bins=[0.5055, 0.6154] | train_dist={0: 1468, 1: 1421, 2: 1415} | sec_fit=297.27 | F1=0.417 Bal=0.417 | nullF1=0.177 nullBal=0.333\n",
      "[agreeableness] fold=3 | trn=4319 val=1106 | bins=[0.5055, 0.6154] | train_dist={0: 1491, 1: 1439, 2: 1389} | sec_fit=298.41 | F1=0.433 Bal=0.432 | nullF1=0.171 nullBal=0.333\n",
      "[agreeableness] fold=4 | trn=4337 val=1088 | bins=[0.5055, 0.6154] | train_dist={0: 1481, 1: 1448, 2: 1408} | sec_fit=299.80 | F1=0.391 Bal=0.391 | nullF1=0.177 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conscientiousness: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[conscientiousness] fold=0 | trn=4332 val=1093 | bins=[0.466, 0.6019] | train_dist={0: 1516, 1: 1442, 2: 1374} | sec_fit=299.42 | F1=0.432 Bal=0.432 | nullF1=0.168 nullBal=0.333\n",
      "[conscientiousness] fold=1 | trn=4408 val=1017 | bins=[0.466, 0.6019] | train_dist={0: 1521, 1: 1454, 2: 1433} | sec_fit=304.46 | F1=0.415 Bal=0.414 | nullF1=0.176 nullBal=0.333\n",
      "[conscientiousness] fold=2 | trn=4304 val=1121 | bins=[0.466, 0.6019] | train_dist={0: 1503, 1: 1385, 2: 1416} | sec_fit=297.27 | F1=0.432 Bal=0.433 | nullF1=0.169 nullBal=0.333\n",
      "[conscientiousness] fold=3 | trn=4319 val=1106 | bins=[0.466, 0.6019] | train_dist={0: 1501, 1: 1409, 2: 1409} | sec_fit=298.57 | F1=0.449 Bal=0.450 | nullF1=0.172 nullBal=0.333\n",
      "[conscientiousness] fold=4 | trn=4337 val=1088 | bins=[0.466, 0.6019] | train_dist={0: 1499, 1: 1414, 2: 1424} | sec_fit=301.49 | F1=0.440 Bal=0.441 | nullF1=0.175 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neuroticism: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neuroticism] fold=0 | trn=4332 val=1093 | bins=[0.4688, 0.6042] | train_dist={0: 1508, 1: 1459, 2: 1365} | sec_fit=301.16 | F1=0.449 Bal=0.451 | nullF1=0.160 nullBal=0.333\n",
      "[neuroticism] fold=1 | trn=4408 val=1017 | bins=[0.4688, 0.6042] | train_dist={0: 1500, 1: 1515, 2: 1393} | sec_fit=306.52 | F1=0.432 Bal=0.431 | nullF1=0.164 nullBal=0.333\n",
      "[neuroticism] fold=2 | trn=4304 val=1121 | bins=[0.4688, 0.6042] | train_dist={0: 1448, 1: 1456, 2: 1400} | sec_fit=299.06 | F1=0.420 Bal=0.420 | nullF1=0.173 nullBal=0.333\n",
      "[neuroticism] fold=3 | trn=4319 val=1106 | bins=[0.4688, 0.6042] | train_dist={0: 1484, 1: 1482, 2: 1353} | sec_fit=300.09 | F1=0.469 Bal=0.470 | nullF1=0.167 nullBal=0.333\n",
      "[neuroticism] fold=4 | trn=4337 val=1088 | bins=[0.4688, 0.6042] | train_dist={0: 1472, 1: 1480, 2: 1385} | sec_fit=301.37 | F1=0.441 Bal=0.441 | nullF1=0.168 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openness: folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[openness] fold=0 | trn=4332 val=1093 | bins=[0.5111, 0.6333] | train_dist={0: 1457, 1: 1454, 2: 1421} | sec_fit=300.96 | F1=0.435 Bal=0.435 | nullF1=0.161 nullBal=0.333\n",
      "[openness] fold=1 | trn=4408 val=1017 | bins=[0.5222, 0.6333] | train_dist={0: 1589, 1: 1350, 2: 1469} | sec_fit=306.02 | F1=0.425 Bal=0.425 | nullF1=0.176 nullBal=0.333\n",
      "[openness] fold=2 | trn=4304 val=1121 | bins=[0.5222, 0.6444] | train_dist={0: 1540, 1: 1437, 2: 1327} | sec_fit=298.91 | F1=0.455 Bal=0.456 | nullF1=0.180 nullBal=0.333\n",
      "[openness] fold=3 | trn=4319 val=1106 | bins=[0.5111, 0.6333] | train_dist={0: 1445, 1: 1448, 2: 1426} | sec_fit=299.97 | F1=0.446 Bal=0.446 | nullF1=0.166 nullBal=0.333\n",
      "[openness] fold=4 | trn=4337 val=1088 | bins=[0.5222, 0.6444] | train_dist={0: 1555, 1: 1439, 2: 1343} | sec_fit=298.80 | F1=0.415 Bal=0.420 | nullF1=0.179 nullBal=0.333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pooling</th>\n",
       "      <th>f1_macro_avg</th>\n",
       "      <th>bal_acc_avg</th>\n",
       "      <th>lift_f1_pct</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>parquet</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_feats</th>\n",
       "      <th>solver</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>log_path</th>\n",
       "      <th>partial_csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.457030</td>\n",
       "      <td>0.458808</td>\n",
       "      <td>28.583792</td>\n",
       "      <td>3789.33</td>\n",
       "      <td>/kaggle/working/emb/ssl_emb_hubert-base_mean_f...</td>\n",
       "      <td>5425</td>\n",
       "      <td>768</td>\n",
       "      <td>saga</td>\n",
       "      <td>2000</td>\n",
       "      <td>/kaggle/working/_progress_mean/log_ssl_emb_hub...</td>\n",
       "      <td>/kaggle/working/_progress_mean/partial_ssl_emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stats</td>\n",
       "      <td>0.434651</td>\n",
       "      <td>0.435139</td>\n",
       "      <td>26.345903</td>\n",
       "      <td>7516.10</td>\n",
       "      <td>/kaggle/working/emb/ssl_emb_hubert-base_stats_...</td>\n",
       "      <td>5425</td>\n",
       "      <td>1536</td>\n",
       "      <td>saga</td>\n",
       "      <td>2000</td>\n",
       "      <td>/kaggle/working/_progress_stats/log_ssl_emb_hu...</td>\n",
       "      <td>/kaggle/working/_progress_stats/partial_ssl_em...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pooling  f1_macro_avg  bal_acc_avg  lift_f1_pct  time_total_s  \\\n",
       "0    mean      0.457030     0.458808    28.583792       3789.33   \n",
       "1   stats      0.434651     0.435139    26.345903       7516.10   \n",
       "\n",
       "                                             parquet  n_rows  n_feats solver  \\\n",
       "0  /kaggle/working/emb/ssl_emb_hubert-base_mean_f...    5425      768   saga   \n",
       "1  /kaggle/working/emb/ssl_emb_hubert-base_stats_...    5425     1536   saga   \n",
       "\n",
       "   max_iter                                           log_path  \\\n",
       "0      2000  /kaggle/working/_progress_mean/log_ssl_emb_hub...   \n",
       "1      2000  /kaggle/working/_progress_stats/log_ssl_emb_hu...   \n",
       "\n",
       "                                         partial_csv  \n",
       "0  /kaggle/working/_progress_mean/partial_ssl_emb...  \n",
       "1  /kaggle/working/_progress_stats/partial_ssl_em...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# jalankan evaluasi dengan progress\n",
    "# evaluasi dengan log super-detail\n",
    "res_mean, sum_mean = eval_cls_tertile_from_parquet_progress(\n",
    "    PQ_MEAN, target_cols,\n",
    "    save_prefix=\"/kaggle/working/_progress_mean\",\n",
    "    verbose=3  # tampilkan log per-fold\n",
    ")\n",
    "res_stats, sum_stats = eval_cls_tertile_from_parquet_progress(\n",
    "    PQ_STATS, target_cols,\n",
    "    save_prefix=\"/kaggle/working/_progress_stats\",\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "pool_leader = pd.DataFrame([\n",
    "    {\"pooling\":\"mean\",  **sum_mean},\n",
    "    {\"pooling\":\"stats\", **sum_stats},\n",
    "]).sort_values([\"f1_macro_avg\",\"bal_acc_avg\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "display(pool_leader)\n",
    "res_mean.to_csv(\"/kaggle/working/results_cls_hubert_mean_pertrait.csv\", index=False)\n",
    "res_stats.to_csv(\"/kaggle/working/results_cls_hubert_stats_pertrait.csv\", index=False)\n",
    "pool_leader.to_csv(\"/kaggle/working/results_cls_hubert_pool_leader.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd7759",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 16)\n",
    "- Menjalankan evaluasi mean vs stats embedding, menyimpan hasil per trait dan leaderboard pooling; keluaran dipakai memilih pooling terbaik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== /kaggle/working/_progress_mean\n",
      " partial: partial_ssl_emb_hubert-base_mean_full.parquet.csv | summary: summary_ssl_emb_hubert-base_mean_full.parquet.json | fold_files: 5\n",
      " traits_done: ['extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness'] | n: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>f1_macro_null</th>\n",
       "      <th>bal_acc_null</th>\n",
       "      <th>time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extraversion</td>\n",
       "      <td>0.472535</td>\n",
       "      <td>0.473862</td>\n",
       "      <td>0.174142</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>756.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agreeableness</td>\n",
       "      <td>0.406927</td>\n",
       "      <td>0.408189</td>\n",
       "      <td>0.170874</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>759.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conscientiousness</td>\n",
       "      <td>0.466526</td>\n",
       "      <td>0.468481</td>\n",
       "      <td>0.171963</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>757.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neuroticism</td>\n",
       "      <td>0.465642</td>\n",
       "      <td>0.468174</td>\n",
       "      <td>0.166478</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>759.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openness</td>\n",
       "      <td>0.473522</td>\n",
       "      <td>0.475336</td>\n",
       "      <td>0.172504</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>757.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               trait  f1_macro   bal_acc  f1_macro_null  bal_acc_null  time_s\n",
       "0       extraversion  0.472535  0.473862       0.174142      0.333333  756.38\n",
       "1      agreeableness  0.406927  0.408189       0.170874      0.333333  759.34\n",
       "2  conscientiousness  0.466526  0.468481       0.171963      0.333333  757.39\n",
       "3        neuroticism  0.465642  0.468174       0.166478      0.333333  759.06\n",
       "4           openness  0.473522  0.475336       0.172504      0.333333  757.12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== /kaggle/working/_progress_stats\n",
      " partial: partial_ssl_emb_hubert-base_stats_full.parquet.csv | summary: summary_ssl_emb_hubert-base_stats_full.parquet.json | fold_files: 5\n",
      " traits_done: ['extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness'] | n: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>f1_macro_null</th>\n",
       "      <th>bal_acc_null</th>\n",
       "      <th>time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extraversion</td>\n",
       "      <td>0.451267</td>\n",
       "      <td>0.451263</td>\n",
       "      <td>0.174142</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1502.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agreeableness</td>\n",
       "      <td>0.410876</td>\n",
       "      <td>0.411497</td>\n",
       "      <td>0.170874</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1499.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conscientiousness</td>\n",
       "      <td>0.433627</td>\n",
       "      <td>0.433968</td>\n",
       "      <td>0.171963</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1501.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neuroticism</td>\n",
       "      <td>0.442015</td>\n",
       "      <td>0.442565</td>\n",
       "      <td>0.166478</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1508.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openness</td>\n",
       "      <td>0.435472</td>\n",
       "      <td>0.436402</td>\n",
       "      <td>0.172504</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1504.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               trait  f1_macro   bal_acc  f1_macro_null  bal_acc_null   time_s\n",
       "0       extraversion  0.451267  0.451263       0.174142      0.333333  1502.37\n",
       "1      agreeableness  0.410876  0.411497       0.170874      0.333333  1499.48\n",
       "2  conscientiousness  0.433627  0.433968       0.171963      0.333333  1501.26\n",
       "3        neuroticism  0.442015  0.442565       0.166478      0.333333  1508.25\n",
       "4           openness  0.435472  0.436402       0.172504      0.333333  1504.71"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cek status savepoint (apa saja trait yang sudah selesai)\n",
    "\n",
    "import os, glob, pandas as pd, json\n",
    "\n",
    "def check_progress(prefix):\n",
    "    part = glob.glob(f\"{prefix}/partial_*.csv\")\n",
    "    summ = glob.glob(f\"{prefix}/summary_*.json\")\n",
    "    fold = sorted(glob.glob(f\"{prefix}/fold_metrics_*_*.csv\"))\n",
    "    print(\"==\", prefix)\n",
    "    print(\" partial:\", os.path.basename(part[0]) if part else \"-\",\n",
    "          \"| summary:\", os.path.basename(summ[0]) if summ else \"-\",\n",
    "          \"| fold_files:\", len(fold))\n",
    "    if part:\n",
    "        dfp = pd.read_csv(part[0])\n",
    "        print(\" traits_done:\", dfp[\"trait\"].tolist(), \"| n:\", len(dfp))\n",
    "        display(dfp)\n",
    "\n",
    "check_progress(\"/kaggle/working/_progress_mean\")\n",
    "check_progress(\"/kaggle/working/_progress_stats\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cdc6a",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 17)\n",
    "- Mengecek file progress/summary yang sudah tersimpan untuk memastikan tidak ada trait yang tertinggal sebelum melanjutkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tetapkan pemenang:\n",
    "\n",
    "POOLING_CHOSEN = \"mean\"\n",
    "PARQUET = PQ_MEAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dab521",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 18)\n",
    "- Mengunci pilihan pooling (mean) dan menyimpan path Parquet yang akan dipakai seluruh tahap berikutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pooling</th>\n",
       "      <th>f1_macro_avg</th>\n",
       "      <th>bal_acc_avg</th>\n",
       "      <th>lift_f1_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.457030</td>\n",
       "      <td>0.458808</td>\n",
       "      <td>28.583792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stats</td>\n",
       "      <td>0.434651</td>\n",
       "      <td>0.435139</td>\n",
       "      <td>26.345903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pooling  f1_macro_avg  bal_acc_avg  lift_f1_pct\n",
       "0    mean      0.457030     0.458808    28.583792\n",
       "1   stats      0.434651     0.435139    26.345903"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tulis CSV “resmi” + leaderboard dari partial (aman bila file akhir belum tertulis):\n",
    "\n",
    "import glob, pandas as pd, numpy as np\n",
    "\n",
    "pm = glob.glob(\"/kaggle/working/_progress_mean/partial_*.csv\")[0]\n",
    "ps = glob.glob(\"/kaggle/working/_progress_stats/partial_*.csv\")[0]\n",
    "res_mean  = pd.read_csv(pm); res_mean.to_csv(\"/kaggle/working/results_cls_hubert_mean_pertrait.csv\", index=False)\n",
    "res_stats = pd.read_csv(ps); res_stats.to_csv(\"/kaggle/working/results_cls_hubert_stats_pertrait.csv\", index=False)\n",
    "\n",
    "pool_leader = pd.DataFrame([\n",
    "    {\"pooling\":\"mean\",  \"f1_macro_avg\": res_mean[\"f1_macro\"].mean(),  \"bal_acc_avg\": res_mean[\"bal_acc\"].mean(),\n",
    "     \"lift_f1_pct\": 100*(res_mean[\"f1_macro\"].mean() - res_mean[\"f1_macro_null\"].mean())},\n",
    "    {\"pooling\":\"stats\", \"f1_macro_avg\": res_stats[\"f1_macro\"].mean(), \"bal_acc_avg\": res_stats[\"bal_acc\"].mean(),\n",
    "     \"lift_f1_pct\": 100*(res_stats[\"f1_macro\"].mean() - res_stats[\"f1_macro_null\"].mean())},\n",
    "]).sort_values([\"f1_macro_avg\",\"bal_acc_avg\"], ascending=[False, False])\n",
    "pool_leader.to_csv(\"/kaggle/working/results_cls_hubert_pool_leader.csv\", index=False)\n",
    "pool_leader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a04af",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 19)\n",
    "- Menulis ulang hasil resmi (mean/stats) dan leaderboard ke CSV, menjaga artefak tetap ada walau proses sebelumnya berhenti tengah jalan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yang sudah dilakukan sebelumnya\n",
    "\n",
    "* **Step-0 Env & Backbone check**: beresin versi lib, load model HF via snapshot lokal.\n",
    "* **Bake-off pilot**: bandingkan huBERT / WavLM / W2V → pemenang **huBERT-base**.\n",
    "* **Full embedding**: ekstrak **mean** & **stats** untuk semua klip.\n",
    "* **Evaluasi klasifikasi (3 kelas tertile, speaker-wise CV)** dengan progress & savepoints → **mean pooling unggul**.\n",
    "\n",
    "## Selanjutnya (yang dikerjakan sekarang)\n",
    "\n",
    "1. **Tetapkan pooling pemenang** (mean) dan **train final** per trait di **seluruh data**.\n",
    "2. Simpan artefak: model `.joblib`, `thresholds.json`, `config.json`.\n",
    "3. (Opsional) buat confusion matrix & demo inference 1 audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL DONE: []\n",
      "MODEL TODO: ['extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness']\n",
      "report_partial_mean.csv → False\n",
      "report_final_mean.csv → False\n",
      "config.json → False\n",
      "thresholds.json → False\n",
      "ssl_emb_hubert-base_mean_full.parquet → True\n",
      "results_cls_hubert_mean_pertrait.csv → True\n",
      "results_cls_hubert_stats_pertrait.csv → True\n",
      "results_cls_hubert_pool_leader.csv → True\n"
     ]
    }
   ],
   "source": [
    "# cek save point, Cek apa yang sudah ke-save\n",
    "import os, pandas as pd\n",
    "\n",
    "target_cols = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "done = [t for t in target_cols if os.path.exists(f\"/kaggle/working/models/{t}_mean_logreg.joblib\")]\n",
    "todo = [t for t in target_cols if t not in done]\n",
    "\n",
    "print(\"MODEL DONE:\", done)\n",
    "print(\"MODEL TODO:\", todo)\n",
    "for p in [\n",
    "    \"/kaggle/working/models/report_partial_mean.csv\",\n",
    "    \"/kaggle/working/models/report_final_mean.csv\",\n",
    "    \"/kaggle/working/models/config.json\",\n",
    "    \"/kaggle/working/models/thresholds.json\",\n",
    "    \"/kaggle/working/emb/ssl_emb_hubert-base_mean_full.parquet\",\n",
    "    \"/kaggle/working/results_cls_hubert_mean_pertrait.csv\",\n",
    "    \"/kaggle/working/results_cls_hubert_stats_pertrait.csv\",\n",
    "    \"/kaggle/working/results_cls_hubert_pool_leader.csv\",\n",
    "]:\n",
    "    print(os.path.basename(p), \"→\", os.path.exists(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40552b8",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 20)\n",
    "- Mengecek model yang sudah terlatih, daftar file kritikal, dan status keberadaan embedding/hasil evaluasi sebelum training final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='pip freeze > /kaggle/working/requirements.txt', returncode=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dokumentasi env (opsional tapi bagus buat reproducibility)\n",
    "import subprocess\n",
    "subprocess.run(\"pip freeze > /kaggle/working/requirements.txt\", shell=True, check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fe3d3",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 21)\n",
    "- Membekukan versi paket saat runtime ke requirements.txt sebagai dokumentasi reproduktibilitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK FILES:\n",
      "FILE /kaggle/working/emb/ssl_emb_hubert-base_mean_full.parquet → True\n",
      "FILE /kaggle/working/models → False\n",
      "DIR  /kaggle/working/_progress_mean → True\n",
      "DIR  /kaggle/working/_progress_stats → True\n",
      "FILE /kaggle/working/results_cls_hubert_mean_pertrait.csv → True\n",
      "FILE /kaggle/working/results_cls_hubert_stats_pertrait.csv → True\n",
      "FILE /kaggle/working/results_cls_hubert_pool_leader.csv → True\n",
      "FILE /kaggle/working/requirements.txt → True\n",
      "ZIPPED → /kaggle/working/TA_S1_checkpoint.zip\n",
      "\n",
      "Sekarang klik **Save Version** agar semua isi /kaggle/working tersimpan sebagai Output Files.\n"
     ]
    }
   ],
   "source": [
    "# --- Pack & Save Now ---\n",
    "import os, zipfile, subprocess, shutil, glob, json, pandas as pd\n",
    "\n",
    "# 1) dokumentasi env\n",
    "subprocess.run(\"pip freeze > /kaggle/working/requirements.txt\", shell=True, check=False)\n",
    "\n",
    "# 2) list ringkas isi penting\n",
    "must_have = [\n",
    "    \"/kaggle/working/emb/ssl_emb_hubert-base_mean_full.parquet\",\n",
    "    \"/kaggle/working/models\",\n",
    "    \"/kaggle/working/_progress_mean\",\n",
    "    \"/kaggle/working/_progress_stats\",\n",
    "    \"/kaggle/working/results_cls_hubert_mean_pertrait.csv\",\n",
    "    \"/kaggle/working/results_cls_hubert_stats_pertrait.csv\",\n",
    "    \"/kaggle/working/results_cls_hubert_pool_leader.csv\",\n",
    "    \"/kaggle/working/requirements.txt\",\n",
    "]\n",
    "print(\"CHECK FILES:\")\n",
    "for p in must_have:\n",
    "    print((\"DIR \" if os.path.isdir(p) else \"FILE\"), p, \"→\", os.path.exists(p))\n",
    "\n",
    "# 3) opsional: arsip ringkas supaya rapi\n",
    "zip_path = \"/kaggle/working/TA_S1_checkpoint.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in must_have:\n",
    "        if os.path.isdir(p):\n",
    "            for root,_,files in os.walk(p):\n",
    "                for f in files:\n",
    "                    full = os.path.join(root,f)\n",
    "                    z.write(full, full.replace(\"/kaggle/working/\",\"\"))\n",
    "        elif os.path.exists(p):\n",
    "            z.write(p, p.replace(\"/kaggle/working/\",\"\"))\n",
    "print(\"ZIPPED →\", zip_path)\n",
    "print(\"\\nSekarang klik **Save Version** agar semua isi /kaggle/working tersimpan sebagai Output Files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e124308",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 22)\n",
    "- Mem-packing artefak penting (embedding, progress, model, hasil) ke ZIP working_artifacts.zip untuk disimpan sebagai output Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LANJUTIN CHECKPOINT 1 (dataset trainingv1-checkpoint1)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:52:10.217450Z",
     "iopub.status.busy": "2025-11-13T06:52:10.216901Z",
     "iopub.status.idle": "2025-11-13T06:52:40.053394Z",
     "shell.execute_reply": "2025-11-13T06:52:40.052764Z",
     "shell.execute_reply.started": "2025-11-13T06:52:10.217430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL DONE: []\n",
      "MODEL TODO: ['extraversion', 'agreeableness', 'conscientiousness', 'neuroticism', 'openness']\n",
      "ssl_emb_hubert-base_mean_full.parquet → True\n",
      "results_cls_hubert_mean_pertrait.csv → True\n",
      "results_cls_hubert_stats_pertrait.csv → True\n",
      "results_cls_hubert_pool_leader.csv → True\n"
     ]
    }
   ],
   "source": [
    "# === Rehydrate dari dataset output versi sebelumnya ===\n",
    "import os, shutil, glob, pandas as pd\n",
    "\n",
    "BASE_IN = \"/kaggle/input/trainingv1-checkpoint1\"  # <-- kalau slug beda, ganti ini\n",
    "\n",
    "# copy folder penting ke working (supaya bisa nulis/overwrite)\n",
    "for d in [\"models\", \"_progress_mean\", \"_progress_stats\", \"emb\", \"hf_cache\"]:\n",
    "    src = os.path.join(BASE_IN, d)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copytree(src, f\"/kaggle/working/{d}\", dirs_exist_ok=True)\n",
    "\n",
    "# path embedding (pakai langsung dari INPUT biar hemat disk)\n",
    "PQ_MEAN  = f\"{BASE_IN}/emb/ssl_emb_hubert-base_mean_full.parquet\"\n",
    "\n",
    "# Cek model yang sudah ada\n",
    "TARGET_COLS = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "done = [t for t in TARGET_COLS if os.path.exists(f\"/kaggle/working/models/{t}_mean_logreg.joblib\")]\n",
    "todo = [t for t in TARGET_COLS if t not in done]\n",
    "print(\"MODEL DONE:\", done)\n",
    "print(\"MODEL TODO:\", todo)\n",
    "\n",
    "# sanity: file yang wajib ada\n",
    "for p in [PQ_MEAN,\n",
    "          f\"{BASE_IN}/results_cls_hubert_mean_pertrait.csv\",\n",
    "          f\"{BASE_IN}/results_cls_hubert_stats_pertrait.csv\",\n",
    "          f\"{BASE_IN}/results_cls_hubert_pool_leader.csv\"]:\n",
    "    print(os.path.basename(p), \"→\", os.path.exists(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a60d54",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 23)\n",
    "- Memuat ulang artefak dari dataset checkpoint (models, progress, embedding) sehingga percobaan bisa dilanjutkan tanpa re-run penuh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T06:54:19.925069Z",
     "iopub.status.busy": "2025-11-13T06:54:19.924519Z",
     "iopub.status.idle": "2025-11-13T07:10:31.804244Z",
     "shell.execute_reply": "2025-11-13T07:10:31.803645Z",
     "shell.execute_reply.started": "2025-11-13T06:54:19.925046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f72813cfb94f09bea873ce1583acdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Final train (mean):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>epochs_used</th>\n",
       "      <th>time_s</th>\n",
       "      <th>train_f1_macro</th>\n",
       "      <th>train_bal_acc</th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extraversion</td>\n",
       "      <td>2000</td>\n",
       "      <td>193.84</td>\n",
       "      <td>0.661482</td>\n",
       "      <td>0.662232</td>\n",
       "      <td>/kaggle/working/models/extraversion_mean_logre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agreeableness</td>\n",
       "      <td>2000</td>\n",
       "      <td>194.63</td>\n",
       "      <td>0.611461</td>\n",
       "      <td>0.612339</td>\n",
       "      <td>/kaggle/working/models/agreeableness_mean_logr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conscientiousness</td>\n",
       "      <td>2000</td>\n",
       "      <td>193.93</td>\n",
       "      <td>0.667658</td>\n",
       "      <td>0.669445</td>\n",
       "      <td>/kaggle/working/models/conscientiousness_mean_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neuroticism</td>\n",
       "      <td>2000</td>\n",
       "      <td>194.75</td>\n",
       "      <td>0.658126</td>\n",
       "      <td>0.661284</td>\n",
       "      <td>/kaggle/working/models/neuroticism_mean_logreg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openness</td>\n",
       "      <td>2000</td>\n",
       "      <td>194.36</td>\n",
       "      <td>0.645608</td>\n",
       "      <td>0.646382</td>\n",
       "      <td>/kaggle/working/models/openness_mean_logreg.jo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               trait  epochs_used  time_s  train_f1_macro  train_bal_acc  \\\n",
       "0       extraversion         2000  193.84        0.661482       0.662232   \n",
       "1      agreeableness         2000  194.63        0.611461       0.612339   \n",
       "2  conscientiousness         2000  193.93        0.667658       0.669445   \n",
       "3        neuroticism         2000  194.75        0.658126       0.661284   \n",
       "4           openness         2000  194.36        0.645608       0.646382   \n",
       "\n",
       "                                          model_path  \n",
       "0  /kaggle/working/models/extraversion_mean_logre...  \n",
       "1  /kaggle/working/models/agreeableness_mean_logr...  \n",
       "2  /kaggle/working/models/conscientiousness_mean_...  \n",
       "3  /kaggle/working/models/neuroticism_mean_logreg...  \n",
       "4  /kaggle/working/models/openness_mean_logreg.jo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Final train dgn SAGA + progress bar EPOCH ===\n",
    "import os, json, time, joblib, numpy as np, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "# ---- config ----\n",
    "POOLING_CHOSEN = \"mean\"\n",
    "PARQUET = PQ_MEAN                     # pastikan sudah didefinisikan\n",
    "SAVE_DIR = \"/kaggle/working/models\"\n",
    "TARGET_COLS = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "\n",
    "SOLVER = \"saga\"                       # pakai saga agar ada iterasi\n",
    "PENALTY = \"l2\"                        # ganti: \"l1\" / \"elasticnet\" (set L1_RATIO)\n",
    "C_VAL   = 1.0\n",
    "L1_RATIO = None                       # contoh elasticnet: 0.5\n",
    "MAX_ITER = 2000                       # total epoch target\n",
    "EPOCH_STEP = 50                       # langkah progress bar (iter per chunk)\n",
    "CLASS_WEIGHT = \"balanced\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ---- load embedding + thresholds (tertile global) ----\n",
    "dat = pd.read_parquet(PARQUET)\n",
    "feat_cols = [c for c in dat.columns if c.startswith(\"emb_\")]\n",
    "X = dat[feat_cols].values\n",
    "thresholds = {t: [float(b) for b in np.quantile(dat[t].values, [1/3,2/3])] for t in TARGET_COLS}\n",
    "\n",
    "# tulis config/thresholds di awal (safe)\n",
    "with open(f\"{SAVE_DIR}/config.json\",\"w\") as f:\n",
    "    json.dump({\"backbone\":\"hubert-base\",\"pooling\":POOLING_CHOSEN,\"embedding_parquet\":PARQUET,\n",
    "               \"targets\":TARGET_COLS,\"n_rows\":int(len(dat)),\"n_feats\":int(len(feat_cols)),\n",
    "               \"solver\":SOLVER,\"penalty\":PENALTY,\"C\":C_VAL,\"l1_ratio\":L1_RATIO,\n",
    "               \"max_iter\":MAX_ITER,\"class_weight\":CLASS_WEIGHT}, f, indent=2)\n",
    "with open(f\"{SAVE_DIR}/thresholds.json\",\"w\") as f:\n",
    "    json.dump(thresholds, f, indent=2)\n",
    "\n",
    "def fit_with_epoch_bar(X, y, *, total_iter=2000, step=50):\n",
    "    \"\"\"Fit LogisticRegression incrementally; update tqdm bar tiap 'step' iter.\"\"\"\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logreg\", LogisticRegression(\n",
    "            solver=SOLVER, penalty=PENALTY, C=C_VAL, l1_ratio=L1_RATIO,\n",
    "            multi_class=\"multinomial\", class_weight=CLASS_WEIGHT,\n",
    "            warm_start=True, max_iter=step, n_jobs=-1, verbose=0, random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    bar = tqdm(total=total_iter, desc=\"epochs\", leave=False)\n",
    "    used = 0\n",
    "    last_small = 0\n",
    "    t0 = time.time()\n",
    "    while used < total_iter:\n",
    "        # set iter target untuk chunk berikutnya\n",
    "        remain = total_iter - used\n",
    "        chunk = min(step, remain)\n",
    "        pipe.set_params(logreg__max_iter=chunk)\n",
    "        pipe.fit(X, y)\n",
    "\n",
    "        # n_iter_ yg dipakai solver (bisa < chunk jika konvergen)\n",
    "        it = int(np.max(np.atleast_1d(pipe.named_steps[\"logreg\"].n_iter_)))\n",
    "        used += it\n",
    "        bar.update(it)\n",
    "\n",
    "        # metrik training (sekilas) untuk postfix bar\n",
    "        y_hat = pipe.predict(X)\n",
    "        f1_tr = f1_score(y, y_hat, average=\"macro\")\n",
    "        bal_tr = balanced_accuracy_score(y, y_hat)\n",
    "        bar.set_postfix({\"f1_tr\": f\"{f1_tr:.3f}\", \"bal_tr\": f\"{bal_tr:.3f}\"})\n",
    "\n",
    "        # early stop if sudah konvergen 2 kali berturut2 (iter yang dipakai kecil)\n",
    "        if it < step//2:\n",
    "            last_small += 1\n",
    "        else:\n",
    "            last_small = 0\n",
    "        if last_small >= 2:\n",
    "            break\n",
    "    bar.close()\n",
    "    elapsed = time.time() - t0\n",
    "    return pipe, used, elapsed\n",
    "\n",
    "rows = []\n",
    "todo = [t for t in TARGET_COLS if not os.path.exists(f\"{SAVE_DIR}/{t}_{POOLING_CHOSEN}_logreg.joblib\")]\n",
    "outer = tqdm(todo, desc=f\"Final train ({POOLING_CHOSEN})\")\n",
    "for t in outer:\n",
    "    y = np.digitize(dat[t].values, thresholds[t], right=True)\n",
    "\n",
    "    # progress bar epoch\n",
    "    pipe, used_iter, sec = fit_with_epoch_bar(X, y, total_iter=MAX_ITER, step=EPOCH_STEP)\n",
    "\n",
    "    # save model + report partial\n",
    "    out_path = f\"{SAVE_DIR}/{t}_{POOLING_CHOSEN}_logreg.joblib\"\n",
    "    joblib.dump(pipe, out_path)\n",
    "    y_hat = pipe.predict(X)\n",
    "    row = {\n",
    "        \"trait\": t,\n",
    "        \"epochs_used\": used_iter,\n",
    "        \"time_s\": round(sec,2),\n",
    "        \"train_f1_macro\": float(f1_score(y, y_hat, average=\"macro\")),\n",
    "        \"train_bal_acc\": float(balanced_accuracy_score(y, y_hat)),\n",
    "        \"model_path\": out_path\n",
    "    }\n",
    "    rows.append(row)\n",
    "    pd.DataFrame(rows).to_csv(f\"{SAVE_DIR}/report_partial_{POOLING_CHOSEN}.csv\", index=False)\n",
    "\n",
    "# simpan report final\n",
    "if rows:\n",
    "    pd.DataFrame(rows).to_csv(f\"{SAVE_DIR}/report_final_{POOLING_CHOSEN}.csv\", index=False)\n",
    "    display(pd.DataFrame(rows))\n",
    "else:\n",
    "    print(\"Tidak ada trait yang perlu dilatih (semua model sudah ada).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cddc72",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 24)\n",
    "- Melatih final classifier SAGA per trait dengan progress chunked; menyimpan model, threshold, laporan partial/final, dan resume otomatis jika sudah ada model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:42:09.838301Z",
     "iopub.status.busy": "2025-11-13T07:42:09.837793Z",
     "iopub.status.idle": "2025-11-13T08:45:32.967587Z",
     "shell.execute_reply": "2025-11-13T08:45:32.966847Z",
     "shell.execute_reply.started": "2025-11-13T07:42:09.838274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>bal_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openness</td>\n",
       "      <td>0.473522</td>\n",
       "      <td>0.475336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>extraversion</td>\n",
       "      <td>0.472535</td>\n",
       "      <td>0.473862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conscientiousness</td>\n",
       "      <td>0.466526</td>\n",
       "      <td>0.468481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neuroticism</td>\n",
       "      <td>0.465803</td>\n",
       "      <td>0.468354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agreeableness</td>\n",
       "      <td>0.406927</td>\n",
       "      <td>0.408189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               trait  f1_macro   bal_acc\n",
       "0           openness  0.473522  0.475336\n",
       "1       extraversion  0.472535  0.473862\n",
       "2  conscientiousness  0.466526  0.468481\n",
       "3        neuroticism  0.465803  0.468354\n",
       "4      agreeableness  0.406927  0.408189"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# == Final CV report (sesuai setup klasifikasi tertile) ==\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "PARQUET = PQ_MEAN\n",
    "dat = pd.read_parquet(PARQUET)\n",
    "X = dat[[c for c in dat.columns if c.startswith(\"emb_\")]].values\n",
    "folds = dat[\"fold\"].values\n",
    "traits = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "\n",
    "rows, cms = [], {}\n",
    "for t in traits:\n",
    "    y = dat[t].values\n",
    "    f1s, bals = [], []\n",
    "    cm_sum = np.zeros((3,3), dtype=int)\n",
    "    for f in sorted(np.unique(folds)):\n",
    "        trn, val = np.where(folds!=f)[0], np.where(folds==f)[0]\n",
    "        bins = np.quantile(y[trn], [1/3, 2/3])\n",
    "        y_trn = np.digitize(y[trn], bins, right=True)\n",
    "        y_val = np.digitize(y[val], bins, right=True)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                solver=\"saga\", penalty=\"l2\", C=1.0, multi_class=\"multinomial\",\n",
    "                class_weight=\"balanced\", max_iter=2000, n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "        pipe.fit(X[trn], y_trn)\n",
    "        y_hat = pipe.predict(X[val])\n",
    "\n",
    "        f1s.append(f1_score(y_val, y_hat, average=\"macro\"))\n",
    "        bals.append(balanced_accuracy_score(y_val, y_hat))\n",
    "        cm_sum += confusion_matrix(y_val, y_hat, labels=[0,1,2])\n",
    "\n",
    "    rows.append({\"trait\":t, \"f1_macro\":np.mean(f1s), \"bal_acc\":np.mean(bals)})\n",
    "    cms[t] = cm_sum\n",
    "\n",
    "df_cv = pd.DataFrame(rows).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "display(df_cv)\n",
    "df_cv.to_csv(\"/kaggle/working/final_cv_metrics.csv\", index=False)\n",
    "\n",
    "# simpan confusion matrix tiap trait\n",
    "for t, cm in cms.items():\n",
    "    pd.DataFrame(cm, index=[\"true_L\",\"true_M\",\"true_H\"], columns=[\"pred_L\",\"pred_M\",\"pred_H\"])\\\n",
    "      .to_csv(f\"/kaggle/working/confmat_{t}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6ffa8",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 25)\n",
    "- Menghitung laporan CV akhir (F1/balanced accuracy) dan confusion matrix per trait, lalu menyimpan ke file untuk audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:07:18.669990Z",
     "iopub.status.busy": "2025-11-13T09:07:18.669675Z",
     "iopub.status.idle": "2025-11-13T09:19:57.764175Z",
     "shell.execute_reply": "2025-11-13T09:19:57.763539Z",
     "shell.execute_reply.started": "2025-11-13T09:07:18.669967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>f1_L</th>\n",
       "      <th>f1_M</th>\n",
       "      <th>f1_H</th>\n",
       "      <th>f1_macro_baseline</th>\n",
       "      <th>bal_acc_baseline</th>\n",
       "      <th>lift_f1_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extraversion</td>\n",
       "      <td>0.476674</td>\n",
       "      <td>0.478437</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.340491</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.177928</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>29.874623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conscientiousness</td>\n",
       "      <td>0.472121</td>\n",
       "      <td>0.475106</td>\n",
       "      <td>0.563776</td>\n",
       "      <td>0.310078</td>\n",
       "      <td>0.542510</td>\n",
       "      <td>0.178587</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>29.353356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neuroticism</td>\n",
       "      <td>0.460403</td>\n",
       "      <td>0.462131</td>\n",
       "      <td>0.519894</td>\n",
       "      <td>0.336634</td>\n",
       "      <td>0.524683</td>\n",
       "      <td>0.167472</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>29.293159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openness</td>\n",
       "      <td>0.454332</td>\n",
       "      <td>0.455783</td>\n",
       "      <td>0.525510</td>\n",
       "      <td>0.345781</td>\n",
       "      <td>0.491704</td>\n",
       "      <td>0.176935</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>27.739679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agreeableness</td>\n",
       "      <td>0.416912</td>\n",
       "      <td>0.417316</td>\n",
       "      <td>0.451007</td>\n",
       "      <td>0.378531</td>\n",
       "      <td>0.421199</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>24.230923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               trait  f1_macro   bal_acc      f1_L      f1_M      f1_H  \\\n",
       "0       extraversion  0.476674  0.478437  0.560606  0.340491  0.528926   \n",
       "1  conscientiousness  0.472121  0.475106  0.563776  0.310078  0.542510   \n",
       "2        neuroticism  0.460403  0.462131  0.519894  0.336634  0.524683   \n",
       "3           openness  0.454332  0.455783  0.525510  0.345781  0.491704   \n",
       "4      agreeableness  0.416912  0.417316  0.451007  0.378531  0.421199   \n",
       "\n",
       "   f1_macro_baseline  bal_acc_baseline  lift_f1_pct  \n",
       "0           0.177928          0.333333    29.874623  \n",
       "1           0.178587          0.333333    29.353356  \n",
       "2           0.167472          0.333333    29.293159  \n",
       "3           0.176935          0.333333    27.739679  \n",
       "4           0.174603          0.333333    24.230923  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_macro_avg</th>\n",
       "      <th>bal_acc_avg</th>\n",
       "      <th>lift_f1_pct_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.456089</td>\n",
       "      <td>0.457755</td>\n",
       "      <td>28.098348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1_macro_avg  bal_acc_avg  lift_f1_pct_avg\n",
       "0      0.456089     0.457755        28.098348"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === HELD-OUT TEST (by person) & evaluation ===\n",
    "import numpy as np, pandas as pd, os, json\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "PARQUET = PQ_MEAN  # dari langkah sebelumnya (mean pooling)\n",
    "dat = pd.read_parquet(PARQUET)\n",
    "\n",
    "assert \"person_id\" in dat.columns, \"person_id diperlukan untuk split by person\"\n",
    "X = dat[[c for c in dat.columns if c.startswith(\"emb_\")]].values\n",
    "groups = dat[\"person_id\"].values\n",
    "traits = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "\n",
    "# 1) split TEST sekali (reproducible)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "trn_idx, tst_idx = next(gss.split(X, groups=groups))\n",
    "Xtr, Xte = X[trn_idx], X[tst_idx]\n",
    "dat_tr, dat_te = dat.iloc[trn_idx].reset_index(drop=True), dat.iloc[tst_idx].reset_index(drop=True)\n",
    "\n",
    "# simpan siapa saja yg jadi test (reproducibility)\n",
    "os.makedirs(\"/kaggle/working/splits\", exist_ok=True)\n",
    "pd.Series(sorted(dat_te[\"person_id\"].unique())).to_csv(\"/kaggle/working/splits/test_persons.csv\", index=False, header=[\"person_id\"])\n",
    "\n",
    "# 2) evaluasi per trait (train->fit, test->score)\n",
    "rows = []\n",
    "for t in traits:\n",
    "    y = dat[t].values\n",
    "    # ambang dari TRAIN saja (anti-leak)\n",
    "    bins = np.quantile(y[trn_idx], [1/3, 2/3])\n",
    "    ytr = np.digitize(y[trn_idx], bins, right=True)\n",
    "    yte = np.digitize(y[tst_idx],  bins, right=True)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            solver=\"saga\", penalty=\"l2\", C=1.0,\n",
    "            multi_class=\"multinomial\", class_weight=\"balanced\",\n",
    "            max_iter=2000, n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    yhat = pipe.predict(Xte)\n",
    "\n",
    "    f1m  = f1_score(yte, yhat, average=\"macro\")\n",
    "    balc = balanced_accuracy_score(yte, yhat)\n",
    "    rpt  = classification_report(yte, yhat, labels=[0,1,2], output_dict=True, zero_division=0)\n",
    "\n",
    "    # per-class F1\n",
    "    f1_L = rpt[\"0\"][\"f1-score\"]; f1_M = rpt[\"1\"][\"f1-score\"]; f1_H = rpt[\"2\"][\"f1-score\"]\n",
    "\n",
    "    # baseline majority (train)\n",
    "    maj  = np.bincount(ytr).argmax()\n",
    "    y0   = np.full_like(yte, maj)\n",
    "    f1m0 = f1_score(yte, y0, average=\"macro\")\n",
    "    balc0= balanced_accuracy_score(yte, y0)\n",
    "\n",
    "    rows.append({\n",
    "        \"trait\": t,\n",
    "        \"f1_macro\": f1m, \"bal_acc\": balc,\n",
    "        \"f1_L\": f1_L, \"f1_M\": f1_M, \"f1_H\": f1_H,\n",
    "        \"f1_macro_baseline\": f1m0, \"bal_acc_baseline\": balc0,\n",
    "        \"lift_f1_pct\": 100*(f1m - f1m0)\n",
    "    })\n",
    "\n",
    "    # simpan confusion matrix\n",
    "    cm = confusion_matrix(yte, yhat, labels=[0,1,2])\n",
    "    pd.DataFrame(cm, index=[\"true_L\",\"true_M\",\"true_H\"], columns=[\"pred_L\",\"pred_M\",\"pred_H\"])\\\n",
    "      .to_csv(f\"/kaggle/working/test_confmat_{t}.csv\")\n",
    "\n",
    "df_test = pd.DataFrame(rows).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "df_test.to_csv(\"/kaggle/working/test_metrics.csv\", index=False)\n",
    "display(df_test)\n",
    "\n",
    "# agregat 5 trait\n",
    "agg = {\n",
    "    \"f1_macro_avg\": df_test[\"f1_macro\"].mean(),\n",
    "    \"bal_acc_avg\":  df_test[\"bal_acc\"].mean(),\n",
    "    \"lift_f1_pct_avg\": df_test[\"lift_f1_pct\"].mean()\n",
    "}\n",
    "pd.DataFrame([agg]).to_csv(\"/kaggle/working/test_metrics_aggregate.csv\", index=False)\n",
    "display(pd.DataFrame([agg]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032fcdef",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 26)\n",
    "- Melakukan held-out test split by person, mengevaluasi tiap trait, menyimpan metrik per fold/aggregate, dan membuat laporan ringkas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:23:36.147668Z",
     "iopub.status.busy": "2025-11-13T09:23:36.147401Z",
     "iopub.status.idle": "2025-11-13T09:23:36.153392Z",
     "shell.execute_reply": "2025-11-13T09:23:36.152598Z",
     "shell.execute_reply.started": "2025-11-13T09:23:36.147648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_cv_metrics.csv → True\n",
      "test_metrics.csv → True\n",
      "test_metrics_aggregate.csv → True\n",
      "test_persons.csv → True\n",
      "config.json → True\n",
      "thresholds.json → True\n",
      "\n",
      "Kalau semua True, klik **Save Version** supaya jadi Output Files.\n"
     ]
    }
   ],
   "source": [
    "# Ringkas file penting dan sarankan commit\n",
    "for p in [\n",
    "    \"/kaggle/working/final_cv_metrics.csv\",\n",
    "    \"/kaggle/working/test_metrics.csv\",\n",
    "    \"/kaggle/working/test_metrics_aggregate.csv\",\n",
    "    \"/kaggle/working/splits/test_persons.csv\",\n",
    "    \"/kaggle/working/models/config.json\",\n",
    "    \"/kaggle/working/models/thresholds.json\",\n",
    "]:\n",
    "    print(os.path.basename(p), \"→\", os.path.exists(p))\n",
    "print(\"\\nKalau semua True, klik **Save Version** supaya jadi Output Files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61735388",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 27)\n",
    "- Mencetak keberadaan file kunci (metrics, splits, config) sebagai checklist sebelum menyimpan versi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T10:06:33.496714Z",
     "iopub.status.busy": "2025-11-13T10:06:33.496142Z",
     "iopub.status.idle": "2025-11-13T10:07:25.159501Z",
     "shell.execute_reply": "2025-11-13T10:07:25.158706Z",
     "shell.execute_reply.started": "2025-11-13T10:06:33.496690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrices to /kaggle/working/confmats/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_L</th>\n",
       "      <th>pred_M</th>\n",
       "      <th>pred_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_L</th>\n",
       "      <td>219</td>\n",
       "      <td>108</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_M</th>\n",
       "      <td>127</td>\n",
       "      <td>118</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_H</th>\n",
       "      <td>58</td>\n",
       "      <td>90</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred_L  pred_M  pred_H\n",
       "true_L     219     108      68\n",
       "true_M     127     118     109\n",
       "true_H      58      90     188"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Confusion Matrix (VAL & TEST) untuk 3-way split by person ===\n",
    "import os, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "PARQUET = PQ_MEAN\n",
    "dat = pd.read_parquet(PARQUET)\n",
    "assert \"person_id\" in dat.columns, \"person_id dibutuhkan.\"\n",
    "\n",
    "X = dat[[c for c in dat.columns if c.startswith(\"emb_\")]].values\n",
    "persons = dat[\"person_id\"].astype(str).values\n",
    "traits = [\"extraversion\",\"agreeableness\",\"conscientiousness\",\"neuroticism\",\"openness\"]\n",
    "\n",
    "# -- Ambil split dari file kalau ada; kalau tidak, regenerate (seed sama) --\n",
    "split_dir = \"/kaggle/working/splits\"\n",
    "if all(os.path.exists(f\"{split_dir}/{n}_persons.csv\") for n in [\"train\",\"val\",\"test\"]):\n",
    "    train_p = set(pd.read_csv(f\"{split_dir}/train_persons.csv\")[\"person_id\"].astype(str))\n",
    "    val_p   = set(pd.read_csv(f\"{split_dir}/val_persons.csv\")[\"person_id\"].astype(str))\n",
    "    test_p  = set(pd.read_csv(f\"{split_dir}/test_persons.csv\")[\"person_id\"].astype(str))\n",
    "    train_idx = np.where(pd.Series(persons).isin(train_p))[0]\n",
    "    val_idx   = np.where(pd.Series(persons).isin(val_p))[0]\n",
    "    test_idx  = np.where(pd.Series(persons).isin(test_p))[0]\n",
    "else:\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "    trval_idx, test_idx = next(gss1.split(X, groups=persons))\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=0.20/0.80, random_state=43)\n",
    "    tr_idx_sub, val_idx_sub = next(gss2.split(X[trval_idx], groups=persons[trval_idx]))\n",
    "    train_idx = trval_idx[tr_idx_sub]\n",
    "    val_idx   = trval_idx[val_idx_sub]\n",
    "\n",
    "Xtr, Xva, Xte = X[train_idx], X[val_idx], X[test_idx]\n",
    "\n",
    "os.makedirs(\"/kaggle/working/confmats\", exist_ok=True)\n",
    "\n",
    "def disc_from_train(y_all, tr_idx):\n",
    "    bins = np.quantile(y_all[tr_idx], [1/3, 2/3])\n",
    "    return np.digitize(y_all, bins, right=True), bins\n",
    "\n",
    "for t in traits:\n",
    "    y_all = dat[t].values\n",
    "    y_disc, bins = disc_from_train(y_all, train_idx)\n",
    "    ytr, yva, yte = y_disc[train_idx], y_disc[val_idx], y_disc[test_idx]\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            solver=\"lbfgs\", penalty=\"l2\", multi_class=\"multinomial\",\n",
    "            class_weight=\"balanced\", max_iter=2000\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Train -> VAL CM\n",
    "    clf.fit(Xtr, ytr)\n",
    "    yva_hat = clf.predict(Xva)\n",
    "    cm_val = confusion_matrix(yva, yva_hat, labels=[0,1,2])\n",
    "\n",
    "    # Refit (TRAIN+VAL) -> TEST CM\n",
    "    Xtrva = np.vstack([Xtr, Xva]); ytrva = np.concatenate([ytr, yva])\n",
    "    clf.fit(Xtrva, ytrva)\n",
    "    yte_hat = clf.predict(Xte)\n",
    "    cm_test = confusion_matrix(yte, yte_hat, labels=[0,1,2])\n",
    "\n",
    "    # Save kedua CM\n",
    "    pd.DataFrame(cm_val,  index=[\"true_L\",\"true_M\",\"true_H\"], columns=[\"pred_L\",\"pred_M\",\"pred_H\"])\\\n",
    "      .to_csv(f\"/kaggle/working/confmats/confmat_val_{t}.csv\")\n",
    "    pd.DataFrame(cm_test, index=[\"true_L\",\"true_M\",\"true_H\"], columns=[\"pred_L\",\"pred_M\",\"pred_H\"])\\\n",
    "      .to_csv(f\"/kaggle/working/confmats/confmat_test_{t}.csv\")\n",
    "\n",
    "print(\"Saved confusion matrices to /kaggle/working/confmats/\")\n",
    "# Tampilkan contoh CM TEST untuk 1 trait biar cepat cek:\n",
    "sample_t = traits[0]\n",
    "display(pd.read_csv(f\"/kaggle/working/confmats/confmat_test_{sample_t}.csv\", index_col=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff337d5",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 28)\n",
    "- Menghasilkan confusion matrix untuk val/test berbasis split person, menyimpannya per trait, dan menampilkan contoh untuk verifikasi cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T10:54:29.642361Z",
     "iopub.status.busy": "2025-11-13T10:54:29.641677Z",
     "iopub.status.idle": "2025-11-13T10:54:42.968916Z",
     "shell.execute_reply": "2025-11-13T10:54:42.968275Z",
     "shell.execute_reply.started": "2025-11-13T10:54:29.642338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUBERT_DIR: /kaggle/working/hf_cache/models--facebook--hubert-base-ls960/snapshots/dba3bb02fda4248b6e082697eee756de8fe8aa8a\n",
      "Loaded backbone from local snapshot.\n",
      "SAMPLE WAV: /kaggle/input/traningv1/wav/J4GQm9j0JZ0.003.wav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pred_idx</th>\n",
       "      <th>probs_LMH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>extraversion</th>\n",
       "      <td>Mid</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.28299999237060547, 0.5070000290870667, 0.20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agreeableness</th>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.42399999499320984, 0.19300000369548798, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conscientiousness</th>\n",
       "      <td>Mid</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4480000138282776, 0.49300000071525574, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuroticism</th>\n",
       "      <td>Mid</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2669999897480011, 0.414000004529953, 0.3190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openness</th>\n",
       "      <td>Mid</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.16500000655651093, 0.7110000252723694, 0.12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  label pred_idx  \\\n",
       "extraversion        Mid        1   \n",
       "agreeableness       Low        0   \n",
       "conscientiousness   Mid        1   \n",
       "neuroticism         Mid        1   \n",
       "openness            Mid        1   \n",
       "\n",
       "                                                           probs_LMH  \n",
       "extraversion       [0.28299999237060547, 0.5070000290870667, 0.20...  \n",
       "agreeableness      [0.42399999499320984, 0.19300000369548798, 0.3...  \n",
       "conscientiousness  [0.4480000138282776, 0.49300000071525574, 0.05...  \n",
       "neuroticism        [0.2669999897480011, 0.414000004529953, 0.3190...  \n",
       "openness           [0.16500000655651093, 0.7110000252723694, 0.12...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== DEMO INFERENCE OFFLINE: 1 WAV -> 5 trait (L/M/H) ====\n",
    "import os, glob, json, joblib, numpy as np, pandas as pd, librosa, torch\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "SR = 16000\n",
    "MODELDIR = \"/kaggle/working/models\"\n",
    "\n",
    "# 1) cari snapshot lokal huBERT di hf_cache (working atau dataset checkpoint)\n",
    "candidates = []\n",
    "for base in [\n",
    "    \"/kaggle/working/hf_cache\",\n",
    "    \"/kaggle/input/trainingv1-checkpoint1/hf_cache\",  # kalau pakai dataset checkpoint\n",
    "]:\n",
    "    hub_root = os.path.join(base, \"models--facebook--hubert-base-ls960\")\n",
    "    snap_glob = os.path.join(hub_root, \"snapshots\", \"*\")\n",
    "    candidates += glob.glob(snap_glob)\n",
    "\n",
    "assert candidates, \"Snapshot lokal hubert-base tidak ditemukan di hf_cache\"\n",
    "HUBERT_DIR = candidates[0]\n",
    "print(\"HUBERT_DIR:\", HUBERT_DIR)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 2) load feature extractor + backbone dari folder lokal (NO internet)\n",
    "fe = AutoFeatureExtractor.from_pretrained(HUBERT_DIR, local_files_only=True)\n",
    "backbone = AutoModel.from_pretrained(HUBERT_DIR, local_files_only=True).to(device).eval()\n",
    "\n",
    "print(\"Loaded backbone from local snapshot.\")\n",
    "\n",
    "# 3) load thresholds + model klasifikasi (logreg) yang sudah kamu train\n",
    "with open(os.path.join(MODELDIR, \"thresholds.json\")) as f:\n",
    "    thresholds = json.load(f)\n",
    "traits = list(thresholds.keys())\n",
    "models = {t: joblib.load(os.path.join(MODELDIR, f\"{t}_mean_logreg.joblib\")) for t in traits}\n",
    "\n",
    "def embed_mean(wav_path: str):\n",
    "    y, _ = librosa.load(wav_path, sr=SR, mono=True)\n",
    "    with torch.no_grad():\n",
    "        inputs = fe([y], sampling_rate=SR, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        hs = backbone(**inputs).last_hidden_state  # [1,T,D]\n",
    "        emb = hs.mean(dim=1).cpu().numpy()        # [1,D]\n",
    "    return emb\n",
    "\n",
    "def predict_personality(wav_path: str):\n",
    "    emb = embed_mean(wav_path)\n",
    "    out = {}\n",
    "    for t in traits:\n",
    "        clf = models[t]\n",
    "        probs = clf.predict_proba(emb)[0]\n",
    "        pred_idx = int(probs.argmax())\n",
    "        label = [\"Low\", \"Mid\", \"High\"][pred_idx]\n",
    "        out[t] = {\n",
    "            \"label\": label,\n",
    "            \"pred_idx\": pred_idx,\n",
    "            \"probs_LMH\": probs.round(3).tolist(),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# 4) contoh: pakai 1 klip dari dataset asli\n",
    "BASE_TR = \"/kaggle/input/traningv1\"\n",
    "df_meta = pd.read_csv(f\"{BASE_TR}/fi_v2_meta.with_feats.clean.folds.csv\")\n",
    "wav_col = \"wav_path\" if \"wav_path\" in df_meta.columns else \"relpath\"\n",
    "sample_path = os.path.join(BASE_TR, df_meta[wav_col].iloc[0].lstrip(\"./\"))\n",
    "print(\"SAMPLE WAV:\", sample_path)\n",
    "\n",
    "pred = predict_personality(sample_path)\n",
    "pd.DataFrame(pred).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fbd5a1",
   "metadata": {},
   "source": [
    "#### Penjelasan (Sel 29)\n",
    "- Demo inferensi offline: memuat backbone dan model tersimpan, memproses satu WAV, dan menampilkan prediksi L/M/H untuk lima trait."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9d018",
   "metadata": {},
   "source": [
    "## Kesimpulan\n",
    "- Dataset dan fold sudah tervalidasi tanpa kebocoran, backbone dievaluasi, dan huBERT-base dengan pooling mean dipilih.\n",
    "- Embedding penuh diekstrak, evaluasi klasifikasi tersimpan, model final dilatih (beserta laporan CV/test dan confusion matrix).\n",
    "- Artefak utama (embedding, model, metrics, paket env) sudah dipaketkan sehingga eksperimen bisa direpro dan dijalankan ulang dari checkpoint atau untuk inferensi."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8612601,
     "sourceId": 13559138,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
